<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Multivariate methods | Quantitative Methods for Geographers</title>
  <meta name="description" content="This is the script of the course ‘Quantitative Methods for Geographers’ run at the Geography Department of Humboldt-Universität zu Berlin." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Multivariate methods | Quantitative Methods for Geographers" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the script of the course ‘Quantitative Methods for Geographers’ run at the Geography Department of Humboldt-Universität zu Berlin." />
  <meta name="github-repo" content="krueger-t/qm4g" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Multivariate methods | Quantitative Methods for Geographers" />
  
  <meta name="twitter:description" content="This is the script of the course ‘Quantitative Methods for Geographers’ run at the Geography Department of Humboldt-Universität zu Berlin." />
  

<meta name="author" content="Tobias Krueger" />


<meta name="date" content="2020-12-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="glms.html"/>
<link rel="next" href="solutions-to-exercises.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Quantitative Methods for Geographers</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="orga.html"><a href="orga.html"><i class="fa fa-check"></i><b>1</b> Organisational matters</a><ul>
<li class="chapter" data-level="1.1" data-path="orga.html"><a href="orga.html#motivating-example"><i class="fa fa-check"></i><b>1.1</b> Motivating example</a></li>
<li class="chapter" data-level="1.2" data-path="orga.html"><a href="orga.html#topics"><i class="fa fa-check"></i><b>1.2</b> Topics</a></li>
<li class="chapter" data-level="1.3" data-path="orga.html"><a href="orga.html#format"><i class="fa fa-check"></i><b>1.3</b> Format</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="math.html"><a href="math.html"><i class="fa fa-check"></i><b>2</b> Mathematical preliminaries</a><ul>
<li class="chapter" data-level="2.1" data-path="math.html"><a href="math.html#logarithm-and-exponentiation"><i class="fa fa-check"></i><b>2.1</b> Logarithm and exponentiation</a></li>
<li class="chapter" data-level="2.2" data-path="math.html"><a href="math.html#centring-and-standardisation"><i class="fa fa-check"></i><b>2.2</b> Centring and standardisation</a></li>
<li class="chapter" data-level="2.3" data-path="math.html"><a href="math.html#derivatives"><i class="fa fa-check"></i><b>2.3</b> Derivatives</a></li>
<li class="chapter" data-level="2.4" data-path="math.html"><a href="math.html#matrix-algebra"><i class="fa fa-check"></i><b>2.4</b> Matrix algebra</a><ul>
<li class="chapter" data-level="2.4.1" data-path="math.html"><a href="math.html#simple-matrix-operations"><i class="fa fa-check"></i><b>2.4.1</b> Simple matrix operations</a></li>
<li class="chapter" data-level="2.4.2" data-path="math.html"><a href="math.html#matrix-multiplication"><i class="fa fa-check"></i><b>2.4.2</b> Matrix multiplication</a></li>
<li class="chapter" data-level="2.4.3" data-path="math.html"><a href="math.html#matrix-division-inverse-of-a-matrix-identity-matrix"><i class="fa fa-check"></i><b>2.4.3</b> Matrix division, inverse of a matrix, identity matrix</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="math.html"><a href="math.html#exercises"><i class="fa fa-check"></i><b>2.5</b> Exercises</a><ul>
<li class="chapter" data-level="" data-path="math.html"><a href="math.html#exercise-1"><i class="fa fa-check"></i>Exercise 1</a></li>
<li class="chapter" data-level="" data-path="math.html"><a href="math.html#exercise-2"><i class="fa fa-check"></i>Exercise 2</a></li>
<li class="chapter" data-level="" data-path="math.html"><a href="math.html#exercise-3"><i class="fa fa-check"></i>Exercise 3</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linreg.html"><a href="linreg.html"><i class="fa fa-check"></i><b>3</b> Linear regression</a><ul>
<li class="chapter" data-level="3.1" data-path="linreg.html"><a href="linreg.html#motivation"><i class="fa fa-check"></i><b>3.1</b> Motivation</a></li>
<li class="chapter" data-level="3.2" data-path="linreg.html"><a href="linreg.html#the-linear-model"><i class="fa fa-check"></i><b>3.2</b> The linear model</a></li>
<li class="chapter" data-level="3.3" data-path="linreg.html"><a href="linreg.html#description-versus-prediction"><i class="fa fa-check"></i><b>3.3</b> Description versus prediction</a></li>
<li class="chapter" data-level="3.4" data-path="linreg.html"><a href="linreg.html#linear-regression"><i class="fa fa-check"></i><b>3.4</b> Linear Regression</a></li>
<li class="chapter" data-level="3.5" data-path="linreg.html"><a href="linreg.html#significance-of-regression"><i class="fa fa-check"></i><b>3.5</b> Significance of regression</a></li>
<li class="chapter" data-level="3.6" data-path="linreg.html"><a href="linreg.html#confidence-in-parameter-estimates"><i class="fa fa-check"></i><b>3.6</b> Confidence in parameter estimates</a></li>
<li class="chapter" data-level="3.7" data-path="linreg.html"><a href="linreg.html#goodness-of-fit"><i class="fa fa-check"></i><b>3.7</b> Goodness of fit</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="categoricalvars.html"><a href="categoricalvars.html"><i class="fa fa-check"></i><b>4</b> Categorical predictors</a></li>
<li class="chapter" data-level="5" data-path="multiplelinreg.html"><a href="multiplelinreg.html"><i class="fa fa-check"></i><b>5</b> Multiple linear regression</a><ul>
<li class="chapter" data-level="5.1" data-path="multiplelinreg.html"><a href="multiplelinreg.html#model-selection"><i class="fa fa-check"></i><b>5.1</b> Model selection</a></li>
<li class="chapter" data-level="5.2" data-path="multiplelinreg.html"><a href="multiplelinreg.html#collinearity"><i class="fa fa-check"></i><b>5.2</b> Collinearity</a></li>
<li class="chapter" data-level="5.3" data-path="multiplelinreg.html"><a href="multiplelinreg.html#overfitting"><i class="fa fa-check"></i><b>5.3</b> Overfitting</a></li>
<li class="chapter" data-level="5.4" data-path="multiplelinreg.html"><a href="multiplelinreg.html#aic"><i class="fa fa-check"></i><b>5.4</b> Information criteria</a></li>
<li class="chapter" data-level="5.5" data-path="multiplelinreg.html"><a href="multiplelinreg.html#ancova"><i class="fa fa-check"></i><b>5.5</b> Mixed continuous-categorical predictors</a></li>
<li class="chapter" data-level="5.6" data-path="multiplelinreg.html"><a href="multiplelinreg.html#general-advise"><i class="fa fa-check"></i><b>5.6</b> General advise</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="mlbayes.html"><a href="mlbayes.html"><i class="fa fa-check"></i><b>6</b> Probabilistic underpinnings</a><ul>
<li class="chapter" data-level="6.1" data-path="mlbayes.html"><a href="mlbayes.html#ml"><i class="fa fa-check"></i><b>6.1</b> Inference via Maximum Likelihood</a></li>
<li class="chapter" data-level="6.2" data-path="mlbayes.html"><a href="mlbayes.html#bayes"><i class="fa fa-check"></i><b>6.2</b> Outlook: Bayesian inference</a><ul>
<li class="chapter" data-level="6.2.1" data-path="mlbayes.html"><a href="mlbayes.html#frequentist-sampling-distributions"><i class="fa fa-check"></i><b>6.2.1</b> Frequentist sampling distributions</a></li>
<li class="chapter" data-level="6.2.2" data-path="mlbayes.html"><a href="mlbayes.html#bayesian-posterior-distributions"><i class="fa fa-check"></i><b>6.2.2</b> Bayesian posterior distributions</a></li>
<li class="chapter" data-level="6.2.3" data-path="mlbayes.html"><a href="mlbayes.html#a-bayesian-analysis-of-the-yield-dataset"><i class="fa fa-check"></i><b>6.2.3</b> A Bayesian analysis of the yield dataset</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="glms.html"><a href="glms.html"><i class="fa fa-check"></i><b>7</b> Generalised Linear Models (GLMs)</a><ul>
<li class="chapter" data-level="7.1" data-path="glms.html"><a href="glms.html#generalising-the-normal-distribution-through-the-exponential-class-of-probability-functions"><i class="fa fa-check"></i><b>7.1</b> Generalising the normal distribution through the exponential class of probability functions</a></li>
<li class="chapter" data-level="7.2" data-path="glms.html"><a href="glms.html#the-link-function-between-mean-response-and-predictor-variables"><i class="fa fa-check"></i><b>7.2</b> The link function between mean response and predictor variables</a></li>
<li class="chapter" data-level="7.3" data-path="glms.html"><a href="glms.html#log-linear-regression"><i class="fa fa-check"></i><b>7.3</b> Log-linear regression</a></li>
<li class="chapter" data-level="7.4" data-path="glms.html"><a href="glms.html#logistic-regression"><i class="fa fa-check"></i><b>7.4</b> Logistic regression</a></li>
<li class="chapter" data-level="7.5" data-path="glms.html"><a href="glms.html#goodness-of-fit-of-glms"><i class="fa fa-check"></i><b>7.5</b> Goodness of fit of GLMs</a></li>
<li class="chapter" data-level="7.6" data-path="glms.html"><a href="glms.html#overdisp"><i class="fa fa-check"></i><b>7.6</b> Over-dispersion</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="multivariate.html"><a href="multivariate.html"><i class="fa fa-check"></i><b>8</b> Multivariate methods</a><ul>
<li class="chapter" data-level="8.1" data-path="multivariate.html"><a href="multivariate.html#clusteranalysis"><i class="fa fa-check"></i><b>8.1</b> Cluster analysis</a><ul>
<li class="chapter" data-level="8.1.1" data-path="multivariate.html"><a href="multivariate.html#the-kmeans-algorithm"><i class="fa fa-check"></i><b>8.1.1</b> The <em>kmeans</em> algorithm</a></li>
<li class="chapter" data-level="8.1.2" data-path="multivariate.html"><a href="multivariate.html#hierarchical-methods"><i class="fa fa-check"></i><b>8.1.2</b> Hierarchical methods</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="multivariate.html"><a href="multivariate.html#pca"><i class="fa fa-check"></i><b>8.2</b> Principal Component Analysis (PCA)</a><ul>
<li class="chapter" data-level="8.2.1" data-path="multivariate.html"><a href="multivariate.html#from-univariate-normal-to-multivariate-normal"><i class="fa fa-check"></i><b>8.2.1</b> From univariate normal to multivariate normal</a></li>
<li class="chapter" data-level="8.2.2" data-path="multivariate.html"><a href="multivariate.html#linear-combination-of-variables"><i class="fa fa-check"></i><b>8.2.2</b> Linear combination of variables</a></li>
<li class="chapter" data-level="8.2.3" data-path="multivariate.html"><a href="multivariate.html#eigenanalysis"><i class="fa fa-check"></i><b>8.2.3</b> Eigenanalysis</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="multivariate.html"><a href="multivariate.html#manova"><i class="fa fa-check"></i><b>8.3</b> Multivariate ANOVA (MANOVA)</a><ul>
<li class="chapter" data-level="8.3.1" data-path="multivariate.html"><a href="multivariate.html#steps-of-manova"><i class="fa fa-check"></i><b>8.3.1</b> Steps of MANOVA</a></li>
<li class="chapter" data-level="8.3.2" data-path="multivariate.html"><a href="multivariate.html#null-hypothesis-test-of-manova"><i class="fa fa-check"></i><b>8.3.2</b> Null hypothesis test of MANOVA</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="multivariate.html"><a href="multivariate.html#dfa"><i class="fa fa-check"></i><b>8.4</b> Discriminant Function Analysis (DFA)</a><ul>
<li class="chapter" data-level="8.4.1" data-path="multivariate.html"><a href="multivariate.html#classification"><i class="fa fa-check"></i><b>8.4.1</b> Classification</a></li>
<li class="chapter" data-level="8.4.2" data-path="multivariate.html"><a href="multivariate.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>8.4.2</b> Leave-one-out cross-validation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="solutions-to-exercises.html"><a href="solutions-to-exercises.html"><i class="fa fa-check"></i>Solutions to exercises</a><ul>
<li class="chapter" data-level="" data-path="solutions-to-exercises.html"><a href="solutions-to-exercises.html#answer-to-q1"><i class="fa fa-check"></i>Answer to Q1</a></li>
<li class="chapter" data-level="" data-path="solutions-to-exercises.html"><a href="solutions-to-exercises.html#answer-to-q2"><i class="fa fa-check"></i>Answer to Q2</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Quantitative Methods for Geographers</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multivariate" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Multivariate methods</h1>
<p><strong>Multivariate regression</strong> extends the case of multiple regression (one response variable/ multiple predictor variables) to the case of <em>multiple response variables</em>/ multiple predictor variables. <em>Predictors are continuous variables</em>, though there is the same correspondence as between linear regression and ANOVA.</p>
<p><strong>Multivariate analysis of variance (MANOVA)</strong> (Chapter <a href="multivariate.html#manova">8.3</a>) extends the case of ANOVA (one response variable/ multiple predictor variables) to the case of <em>multiple response variables</em>/ multiple predictor variables. <em>Predictors are categorical variables</em>, though MANOVA relates to multivariate regression just like ANOVA does to linear regression.</p>
<p><strong>Discriminant Function Analysis (DFA)</strong> (Chapter <a href="multivariate.html#dfa">8.4</a>) is a <em>classification</em> method that tests how well multi-response observations discriminate between pre-determined groups, and can also be used to classify new observations into one of the groups.</p>
<p><strong>Principle Component Analysis (PCA)</strong> (Chapter <a href="multivariate.html#pca">8.2</a>), <strong>Factor Analysis (FA)</strong> and related methods aim at finding <em>structure</em> in a multivariate dataset, not deciding on response/predictor variables just yet. They extract a reduced set of components that explain much of the <em>variability</em> or <em>correlation</em> among the original variables. PCA and FA are typically employed to pre-structure and <em>simplify</em> a problem by reducing its data dimensions, e.g. to reduce collinearity (compare Chapter <a href="multiplelinreg.html#multiplelinreg">5</a>).</p>
<p><strong>Cluster Analysis</strong> (Chapter <a href="multivariate.html#clusteranalysis">8.1</a>) looks for groups in a multivariate dataset. Data points belonging to the same group “resemble” each other - we will see what this means. Data points belonging to different groups are “dissimilar”.</p>
<div id="clusteranalysis" class="section level2">
<h2><span class="header-section-number">8.1</span> Cluster analysis</h2>
<p>This section is based on material by Cornelius Senf.<a href="#fn23" class="footnote-ref" id="fnref23"><sup>23</sup></a> Cluster analysis looks for groups (clusters) in a multivariate dataset. Objects (data points) belonging to the same group “resemble” each other. Objects belonging to different groups are “dissimilar”. There are three groups of methods:</p>
<ol style="list-style-type: decimal">
<li><strong>Partitioning</strong> the dataset into a number of clusters specified by the user, e.g. the <em>kmeans</em> algorithm</li>
<li><strong>Hierarchical</strong>, starting with each object (data point) as a separate cluster and then aggregating these step by step, ending up with a single cluster</li>
<li><strong>Divisive</strong>, starting with a single cluster of all objects (data points) and then splitting this step by step until all objects are in different clusters</li>
</ol>
<p>Let’s illustrate the principles of these methods with the Iris dataset that is available from <em>R</em>, on which cluster analysis can be used to separate taxonomic groups. The dataset consists of a sample of Iris flowers for which the lengths and widths of their sepals and petals were measured. Sepals and petals are two different kinds of leaves in the flower. The question is: <em>Can we separate clusters of flowers that are sufficiently different with respect to these four features?</em> This then could form the basis of deriving taxonomic groups; indeed this is a typical approach in botany. To get a sense of the dataset let’s first plot a scatterplot matrix:</p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="multivariate.html#cb148-1"></a><span class="co"># load Iris dataset</span></span>
<span id="cb148-2"><a href="multivariate.html#cb148-2"></a><span class="kw">data</span>(iris)</span>
<span id="cb148-3"><a href="multivariate.html#cb148-3"></a><span class="co"># scatterplot matrix</span></span>
<span id="cb148-4"><a href="multivariate.html#cb148-4"></a><span class="kw">plot</span>(iris[,<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>])</span></code></pre></div>
<p><img src="qm4g_files/figure-html/unnamed-chunk-37-1.png" width="80%" /></p>
<p>We can already see at least two clusters. In some dimensions (petal length and width) they are more apart than in others (sepal length and width). Let’s formalise this analysis using the <em>kmeans</em> algorithm and afterwards look briefly at what hierarchical methods do.</p>
<div id="the-kmeans-algorithm" class="section level3">
<h3><span class="header-section-number">8.1.1</span> The <em>kmeans</em> algorithm</h3>
<p>The purpose of <em>kmeans</em> is to build clusters such that the distance of cluster objects (data points) to cluster <em>centroids</em> (vectors of means) is minimised. The algorithm proceeds though the following steps:</p>
<ol style="list-style-type: decimal">
<li>Choose <span class="math inline">\(k\)</span> random cluster centroids in the multivariate space</li>
<li>Allocate each object to a cluster so that <em>total intra-cluster sum of squares</em> (Equation <a href="multivariate.html#eq:intraclusterss">(8.1)</a>) is minimised</li>
</ol>
<p><span class="math display" id="eq:intraclusterss">\[\begin{equation}
\sum_{j=1}^{k}\sum_{i=1}^{n_j}\lVert \mathbf{y}_{ij}-\boldsymbol{\mu}_j\rVert^2
\tag{8.1}
\end{equation}\]</span></p>
<p><span class="math inline">\(\boldsymbol{\mu}_j\)</span> is the centroid of cluster <span class="math inline">\(j=1,2,\ldots,k\)</span>, i.e. the vector of means across the data dimensions (here four). <span class="math inline">\(\mathbf{y}_{ij}\)</span> is data point <span class="math inline">\(i=1,2,\ldots,n_j\)</span> of cluster <span class="math inline">\(j\)</span>, i.e. a multivariate vector too. <span class="math inline">\(\lVert\cdot\rVert\)</span> symbolises the <em>Euclidean distance</em>.</p>
<ol start="3" style="list-style-type: decimal">
<li>Re-calculate cluster centroids</li>
<li>Repeat steps 2-3 until cluster centroids are not changing much anymore (by some chosen criterion)</li>
</ol>
<p>Often the Euclidean distance is used as a measure of (dis)similarity but others can be specified as well.</p>
<p>We have to tell the algorithm how many clusters we want. Let’s use two to begin with (because that was our intuition earlier):</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="multivariate.html#cb149-1"></a><span class="co"># run kmeans algorithm on Iris data asking for 2 clusters</span></span>
<span id="cb149-2"><a href="multivariate.html#cb149-2"></a>iris_fit2 &lt;-<span class="st"> </span><span class="kw">kmeans</span>(iris[,<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>], <span class="dt">centers=</span><span class="dv">2</span>)</span>
<span id="cb149-3"><a href="multivariate.html#cb149-3"></a><span class="co"># scatterplot matrix</span></span>
<span id="cb149-4"><a href="multivariate.html#cb149-4"></a><span class="kw">plot</span>(iris[,<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>], <span class="dt">col=</span>iris_fit2<span class="op">$</span>cluster)</span></code></pre></div>
<p><img src="qm4g_files/figure-html/unnamed-chunk-38-1.png" width="80%" /></p>
<p>Two clusters didn’t seem enough to reproduce the separation we see visually. Let’s increase the number of clusters to three:</p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="multivariate.html#cb150-1"></a><span class="co"># run kmeans algorithm on Iris data asking for 3 clusters</span></span>
<span id="cb150-2"><a href="multivariate.html#cb150-2"></a>iris_fit3 &lt;-<span class="st"> </span><span class="kw">kmeans</span>(iris[,<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>], <span class="dt">centers=</span><span class="dv">3</span>)</span>
<span id="cb150-3"><a href="multivariate.html#cb150-3"></a><span class="co"># scatterplot matrix</span></span>
<span id="cb150-4"><a href="multivariate.html#cb150-4"></a><span class="kw">plot</span>(iris[,<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>], <span class="dt">col=</span>iris_fit3<span class="op">$</span>cluster)</span></code></pre></div>
<p><img src="qm4g_files/figure-html/unnamed-chunk-39-1.png" width="80%" /></p>
<p>I think we would be happy with this visually. But are there perhaps even more clusters? When to stop? A useful stopping criterion is to look at the inflexion point where the total intra-cluster sum of squares (Equation <a href="multivariate.html#eq:intraclusterss">(8.1)</a>) does not change much anymore with increasing <span class="math inline">\(k\)</span> in a so called <strong>screeplot</strong><a href="#fn24" class="footnote-ref" id="fnref24"><sup>24</sup></a>:</p>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb151-1"><a href="multivariate.html#cb151-1"></a><span class="co"># specify vector of clusters</span></span>
<span id="cb151-2"><a href="multivariate.html#cb151-2"></a>clusters &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">10</span></span>
<span id="cb151-3"><a href="multivariate.html#cb151-3"></a><span class="co"># initialise corresponding vector of total intra-cluster sum of squares</span></span>
<span id="cb151-4"><a href="multivariate.html#cb151-4"></a>ticss &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, <span class="dv">10</span>)</span>
<span id="cb151-5"><a href="multivariate.html#cb151-5"></a><span class="co"># loop through vector of clusters</span></span>
<span id="cb151-6"><a href="multivariate.html#cb151-6"></a><span class="cf">for</span>(i <span class="cf">in</span> clusters){</span>
<span id="cb151-7"><a href="multivariate.html#cb151-7"></a>  <span class="co"># run kmeans</span></span>
<span id="cb151-8"><a href="multivariate.html#cb151-8"></a>  iris_fit &lt;-<span class="st"> </span><span class="kw">kmeans</span>(iris[,<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>], <span class="dt">centers=</span>i)</span>
<span id="cb151-9"><a href="multivariate.html#cb151-9"></a>  <span class="co"># collect total intra-cluster sum of squares</span></span>
<span id="cb151-10"><a href="multivariate.html#cb151-10"></a>  ticss[i] &lt;-<span class="st"> </span>iris_fit<span class="op">$</span>tot.withinss</span>
<span id="cb151-11"><a href="multivariate.html#cb151-11"></a>}</span>
<span id="cb151-12"><a href="multivariate.html#cb151-12"></a><span class="co"># plot total intra-cluster sum of squares against clusters</span></span>
<span id="cb151-13"><a href="multivariate.html#cb151-13"></a><span class="kw">plot</span>(clusters, ticss, <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;b&#39;</span>, <span class="dt">xlab =</span> <span class="st">&quot;Number of clusters&quot;</span>,</span>
<span id="cb151-14"><a href="multivariate.html#cb151-14"></a>     <span class="dt">ylab =</span> <span class="st">&quot;Total intra-cluster sum of squares&quot;</span>)</span></code></pre></div>
<p><img src="qm4g_files/figure-html/unnamed-chunk-40-1.png" width="80%" /></p>
<p>We can see that beyond three clusters the improvement in separation is minimal, so we would leave it at three. This, it turns out, matches almost perfectly the official taxonomic separation of the Iris dataset:</p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="multivariate.html#cb152-1"></a><span class="co"># contingency table of cluster:species matches</span></span>
<span id="cb152-2"><a href="multivariate.html#cb152-2"></a><span class="kw">table</span>(iris_fit3<span class="op">$</span>cluster, iris<span class="op">$</span>Species)</span></code></pre></div>
<pre><code>##    
##     setosa versicolor virginica
##   1      0         48        14
##   2     50          0         0
##   3      0          2        36</code></pre>
<p>The separation of Iris versicolor and Iris virginica, however, is not perfect.</p>
</div>
<div id="hierarchical-methods" class="section level3">
<h3><span class="header-section-number">8.1.2</span> Hierarchical methods</h3>
<p>The principle of hierarchical methods is to start with each object as a separate cluster and then aggregate these step by step, ending up with a single cluster. Note, devisive methods are not covered here, but they work exactly the other way round. An example algorithm works as follows:</p>
<ol style="list-style-type: decimal">
<li>Build <em>dissimilarity matrix</em>, i.e. a matrix of the distances of every object to every other object in the multivariate space, e.g. by Euclidean distance <span class="math inline">\(\lVert\mathbf{y}_i-\mathbf{y}_{i^*}\rVert\)</span></li>
<li>Start with each object as a separate cluster</li>
<li>Join the two most similar clusters, e.g. those that lead to minimum increase in total intra-cluster sum of squares after merging (Equation <a href="multivariate.html#eq:intraclusterss">(8.1)</a>); this is called <em>Ward’s method</em></li>
<li>Repeat step 3 until a single cluster is build</li>
</ol>
<p>The result is a <strong>dendrogram</strong>, whose “height” is the distance between clusters, e.g. in Ward’s method the increase in the intra-cluster sum of squares of the clusters being merged:</p>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="multivariate.html#cb154-1"></a><span class="co"># construct dissimilarity matrix for Iris data</span></span>
<span id="cb154-2"><a href="multivariate.html#cb154-2"></a>iris_dist &lt;-<span class="st"> </span><span class="kw">dist</span>(iris[,<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>])</span>
<span id="cb154-3"><a href="multivariate.html#cb154-3"></a><span class="co"># run hierarchical clustering with Ward&#39;s method</span></span>
<span id="cb154-4"><a href="multivariate.html#cb154-4"></a>iris_fith &lt;-<span class="st"> </span><span class="kw">hclust</span>(iris_dist, <span class="dt">method=</span><span class="st">&quot;ward.D2&quot;</span>)</span>
<span id="cb154-5"><a href="multivariate.html#cb154-5"></a><span class="co"># dendrogram</span></span>
<span id="cb154-6"><a href="multivariate.html#cb154-6"></a><span class="kw">plot</span>(iris_fith)</span></code></pre></div>
<p><img src="qm4g_files/figure-html/unnamed-chunk-42-1.png" width="80%" /></p>
<p>Again we can see the three clearly separate clusters, beyond which any further separation is ambiguous.</p>
</div>
</div>
<div id="pca" class="section level2">
<h2><span class="header-section-number">8.2</span> Principal Component Analysis (PCA)</h2>
<p>To illustrate PCA I will use a dataset from <span class="citation">Lovett, Weathers, and Sobczak (<a href="#ref-lovett2000" role="doc-biblioref">2000</a>)</span>, cited in <span class="citation">Quinn and Keough (<a href="#ref-quinn2002" role="doc-biblioref">2002</a>)</span>, that consists of stream chemistry measurements from 38 forested catchments in the Catskill Mountains, New York:</p>
<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb155-1"><a href="multivariate.html#cb155-1"></a><span class="co"># load stream chemistry data</span></span>
<span id="cb155-2"><a href="multivariate.html#cb155-2"></a>streams &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="st">&quot;data/streams.txt&quot;</span>,<span class="dt">header=</span>T)</span>
<span id="cb155-3"><a href="multivariate.html#cb155-3"></a><span class="kw">head</span>(streams)</span></code></pre></div>
<pre><code>##    catchm_name max_elev sample_elev stream_length catchm_area  NO3 TON   TN NH4
## 1   Santa_Cruz     1006         680          1680          23 24.2 5.6 29.9 0.8
## 2      Colgate     1216         628          3912         462 25.4 4.9 30.3 1.4
## 3       Halsey     1204         625          4032         297 29.7 4.4 33.0 0.8
## 4 Batavia_Hill     1213         663          3072         399 22.1 6.1 28.3 1.4
## 5 Windham_Ridg     1074         616          2520         207 13.1 5.7 17.6 0.6
## 6 Silver_Sprin     1113         451          3120         348 27.5 3.0 30.8 1.1
##     DOC  SO4   CL   CA   MG    H
## 1 180.4 50.6 15.5 54.7 14.4 0.48
## 2 108.8 55.4 16.4 58.4 17.0 0.24
## 3 104.7 56.5 17.1 65.9 19.6 0.47
## 4  84.5 57.5 16.8 59.5 19.5 0.23
## 5  82.4 58.3 18.3 54.6 21.9 0.37
## 6  86.6 63.0 15.7 68.5 22.4 0.17</code></pre>
<p>The dataset first lists catchment characteristic: catchment name, maximum elevation in catchment, elevation where stream water sample was taken, stream length in catchment, catchment area. The stream chemistry variables are concentrations of: nitrate, total organic nitrogen, total nitrogen, ammonium, dissolved organic carbon, sulfate, chloride, calcium, magnesium, hydrogen.</p>
<div id="from-univariate-normal-to-multivariate-normal" class="section level3">
<h3><span class="header-section-number">8.2.1</span> From univariate normal to multivariate normal</h3>
<p>Like univariate methods, multivariate methods, too, rely on the normality assumption; the univariate normal distribution (Figure <a href="multivariate.html#fig:camg">8.1</a>, left) is generalised to the <strong>multivariate normal distribution</strong> (Figure <a href="multivariate.html#fig:camg">8.1</a>, right). Taking calcium concentration in the stream chemistry dataset as the example, the univariate normal model would be <span class="math inline">\(N(\mu,\sigma)\)</span> with mean <span class="math inline">\(\mu=65.13\)</span> and variance <span class="math inline">\(\sigma^2=194.74\)</span> (Figure <a href="multivariate.html#fig:camg">8.1</a>, left). Looking at calcium and magnesium concentration together, the multivariate normal model would be <span class="math inline">\(MVN(\boldsymbol\mu,\boldsymbol\Sigma)\)</span> with <strong>centroid</strong> (vector of means) <span class="math inline">\(\boldsymbol\mu=\begin{pmatrix}65.13&amp;22.86\end{pmatrix}^T\)</span> and <strong>variance-covariance matrix</strong> <span class="math inline">\(\boldsymbol\Sigma=\begin{pmatrix}194.74&amp;27.03\\27.03&amp;194.74\end{pmatrix}\)</span> (Figure <a href="multivariate.html#fig:camg">8.1</a>, right).</p>
<div class="figure" style="text-align: center"><span id="fig:camg"></span>
<img src="figs/ca_pdf.jpg" alt="Left: Histogram of calcium concentration measurements from stream chemistry dataset, with fitted normal distribution. The vertical line marks the mean. Right: Scatterplot of calcium against magnesium concentration measurements, with coloured countours of fitted multivariate normal distribution. The vertical and horizontal lines are the individual means, which intersect at the centroid. Data from: @lovett2000, cited in @quinn2002." width="50%" /><img src="figs/ca_mg_jointpdf.jpg" alt="Left: Histogram of calcium concentration measurements from stream chemistry dataset, with fitted normal distribution. The vertical line marks the mean. Right: Scatterplot of calcium against magnesium concentration measurements, with coloured countours of fitted multivariate normal distribution. The vertical and horizontal lines are the individual means, which intersect at the centroid. Data from: @lovett2000, cited in @quinn2002." width="50%" />
<p class="caption">
Figure 8.1: Left: Histogram of calcium concentration measurements from stream chemistry dataset, with fitted normal distribution. The vertical line marks the mean. Right: Scatterplot of calcium against magnesium concentration measurements, with coloured countours of fitted multivariate normal distribution. The vertical and horizontal lines are the individual means, which intersect at the centroid. Data from: <span class="citation">Lovett, Weathers, and Sobczak (<a href="#ref-lovett2000" role="doc-biblioref">2000</a>)</span>, cited in <span class="citation">Quinn and Keough (<a href="#ref-quinn2002" role="doc-biblioref">2002</a>)</span>.
</p>
</div>
<p>The <strong>variance-covariance matrix</strong> (or just covariance matrix) is a matrix of associations between variables. On its diagonal are the variances of the individual variables,<a href="#fn25" class="footnote-ref" id="fnref25"><sup>25</sup></a> on the off-diagonals are the covariances between two variables:</p>
<p><span class="math display" id="eq:covmatrix">\[\begin{equation}
\mathbf{C}=\begin{pmatrix}
\frac{\sum_{i=1}^{n}\left(y_{i1}-\bar y_1\right)^2}{n-1} &amp; \cdots &amp; \frac{\sum_{i=1}^{n}\left(y_{ip}-\bar y_p\right)\cdot\left(y_{i1}-\bar y_1\right)}{n-1} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{\sum_{i=1}^{n}\left(y_{i1}-\bar y_1\right)\cdot\left(y_{ip}-\bar y_p\right)}{n-1} &amp; \cdots &amp; \frac{\sum_{i=1}^{n}\left(y_{ip}-\bar y_p\right)^2}{n-1}
\end{pmatrix}
\tag{8.2}
\end{equation}\]</span></p>
<p>Normalising the variance-covariance terms in Equation <a href="multivariate.html#eq:covmatrix">(8.2)</a> by the variances of the respective variables, e.g. <span class="math inline">\(corr\left(y_1,y_2\right)=\frac{cov\left(y_1,y_2\right)}{\sqrt{\sigma_{y_1}^2\cdot\sigma_{y_2}^2}}=\frac{cov\left(y_1,y_2\right)}{\sigma_{y_1}\cdot\sigma_{y_2}}\)</span>, yields the <strong>correlation matrix</strong>:</p>
<p><span class="math display" id="eq:corrmatrix">\[\begin{equation}
\mathbf{R}=\begin{pmatrix}
\frac{\sum_{i=1}^{n}\left(y_{i1}-\bar y_1\right)^2}{\sqrt{\sum_{i=1}^{n}\left(y_{i1}-\bar y_1\right)^2\cdot\sum_{i=1}^{n}\left(y_{i1}-\bar y_1\right)^2}} &amp; \cdots &amp; \frac{\sum_{i=1}^{n}\left(y_{ip}-\bar y_p\right)\cdot\left(y_{i1}-\bar y_1\right)}{\sqrt{\sum_{i=1}^{n}\left(y_{ip}-\bar y_p\right)^2\cdot\sum_{i=1}^{n}\left(y_{i1}-\bar y_1\right)^2}} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{\sum_{i=1}^{n}\left(y_{i1}-\bar y_1\right)\cdot\left(y_{ip}-\bar y_p\right)}{\sqrt{\sum_{i=1}^{n}\left(y_{i1}-\bar y_1\right)^2\cdot\sum_{i=1}^{n}\left(y_{ip}-\bar y_p\right)^2}} &amp; \cdots &amp; \frac{\sum_{i=1}^{n}\left(y_{ip}-\bar y_p\right)^2}{\sqrt{\sum_{i=1}^{n}\left(y_{ip}-\bar y_p\right)^2\cdot\sum_{i=1}^{n}\left(y_{ip}-\bar y_p\right)^2}}
\end{pmatrix}
\tag{8.3}
\end{equation}\]</span></p>
<p>Let’s look at the associations between the variables in the stream chemistry dataset:</p>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb157-1"><a href="multivariate.html#cb157-1"></a><span class="kw">pairs</span>(streams[,<span class="dv">6</span><span class="op">:</span><span class="dv">15</span>], <span class="dt">diag.panel =</span> panel.hist)</span></code></pre></div>
<p><img src="qm4g_files/figure-html/unnamed-chunk-45-1.png" width="672" /></p>
<p>First of all we notice the non-normality of DOC, Cl and H (at least; in principle this diagnosis is ambiguous). Hence we log-transform these three variables to make them adhere better to normality:</p>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb158-1"><a href="multivariate.html#cb158-1"></a>streams_log &lt;-<span class="st"> </span>streams</span>
<span id="cb158-2"><a href="multivariate.html#cb158-2"></a>streams_log<span class="op">$</span>DOC &lt;-<span class="st"> </span><span class="kw">log</span>(streams<span class="op">$</span>DOC)</span>
<span id="cb158-3"><a href="multivariate.html#cb158-3"></a>streams_log<span class="op">$</span>CL &lt;-<span class="st"> </span><span class="kw">log</span>(streams<span class="op">$</span>CL)</span>
<span id="cb158-4"><a href="multivariate.html#cb158-4"></a>streams_log<span class="op">$</span>H &lt;-<span class="st"> </span><span class="kw">log</span>(streams<span class="op">$</span>H)</span>
<span id="cb158-5"><a href="multivariate.html#cb158-5"></a><span class="kw">pairs</span>(streams_log[,<span class="dv">6</span><span class="op">:</span><span class="dv">15</span>], <span class="dt">diag.panel =</span> panel.hist)</span></code></pre></div>
<p><img src="qm4g_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
<p>We then notice a positive correlation between NO3 and TN; most of the total nitrogen in streams actually comes in the form of nitrate. We notice a positive correlation between Ca and Mg; both are characteristic of alkaline waters and often appear together. We notice a positive correlation between Mg (and Ca) and SO4; magnesium (and calcium) often appear as Mg(Ca)-sulphate. And we notice a negative correlation between Ca and H; calcium is characteristic of alkaline waters, i.e. waters that have a high pH, hence a low hydrogen concentration. These associations are reflected in the covariance matrix:</p>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb159-1"><a href="multivariate.html#cb159-1"></a><span class="kw">cov</span>(streams_log[,<span class="dv">6</span><span class="op">:</span><span class="dv">15</span>])</span></code></pre></div>
<pre><code>##             NO3          TON         TN         NH4         DOC        SO4
## NO3  74.1885064 -4.367083926 68.5690754 -1.21607397  0.26399923 -1.8439972
## TON  -4.3670839  1.636706970 -2.5259104  0.14816501  0.07963509  0.3585633
## TN   68.5690754 -2.525910384 65.6039900 -0.97096728  0.30096702  0.9395590
## NH4  -1.2160740  0.148165007 -0.9709673  0.53607397 -0.09426689  0.3710242
## DOC   0.2639992  0.079635093  0.3009670 -0.09426689  0.11367492 -0.6113830
## SO4  -1.8439972  0.358563300  0.9395590  0.37102418 -0.61138295 27.2433286
## CL   -1.2917322  0.060402549 -1.1991558  0.07313198 -0.01935573  0.4772130
## CA   29.1343670 -2.879523471 29.0208179  0.57644381 -1.25321565 35.0845235
## MG  -14.7774538  2.627396871 -9.8611522  0.77961593 -0.60795805 19.8258464
## H     0.3516809 -0.008264002  0.1717175 -0.10160741  0.09357589 -1.5999792
##              CL          CA          MG            H
## NO3 -1.29173215  29.1343670 -14.7774538  0.351680867
## TON  0.06040255  -2.8795235   2.6273969 -0.008264002
## TN  -1.19915582  29.0208179  -9.8611522  0.171717531
## NH4  0.07313198   0.5764438   0.7796159 -0.101607407
## DOC -0.01935573  -1.2532156  -0.6079580  0.093575891
## SO4  0.47721304  35.0845235  19.8258464 -1.599979158
## CL   0.12765675   0.8214436   1.0110406 -0.080456005
## CA   0.82144357 194.7417710  27.0328307 -7.639475486
## MG   1.01104057  27.0328307  26.2495306 -1.883364151
## H   -0.08045601  -7.6394755  -1.8833642  0.454192635</code></pre>
<p>In particular, <span class="math inline">\(cov(NO3,TN)=68.57\)</span>, <span class="math inline">\(cov(SO4,MG)=19.83\)</span> and <span class="math inline">\(cov(CA,H)=-7.64\)</span>. However, at the scale of covariances we cannot judge the strengths of the associations, because the magnitude of the covariance is determined by the diagonal variance terms of the respective variables. We need to normalise by those variances, i.e. look at the correlation matrix:</p>
<div class="sourceCode" id="cb161"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb161-1"><a href="multivariate.html#cb161-1"></a><span class="kw">cor</span>(streams_log[,<span class="dv">6</span><span class="op">:</span><span class="dv">15</span>])</span></code></pre></div>
<pre><code>##             NO3          TON          TN         NH4         DOC         SO4
## NO3  1.00000000 -0.396312489  0.98286722 -0.19283209  0.09090798 -0.04101681
## TON -0.39631249  1.000000000 -0.24376275  0.15817863  0.18462338  0.05369703
## TN   0.98286722 -0.243762753  1.00000000 -0.16372957  0.11021011  0.02222434
## NH4 -0.19283209  0.158178634 -0.16372957  1.00000000 -0.38186908  0.09708671
## DOC  0.09090798  0.184623379  0.11021011 -0.38186908  1.00000000 -0.34741688
## SO4 -0.04101681  0.053697033  0.02222434  0.09708671 -0.34741688  1.00000000
## CL  -0.41974184  0.132144140 -0.41437028  0.27955874 -0.16067757  0.25589413
## CA   0.24238610 -0.161289316  0.25675267  0.05641766 -0.26635688  0.48167703
## MG  -0.33486559  0.400847742 -0.23763027  0.20782968 -0.35194963  0.74138003
## H    0.06058434 -0.009584841  0.03145788 -0.20591736  0.41182395 -0.45484581
##             CL          CA         MG            H
## NO3 -0.4197418  0.24238610 -0.3348656  0.060584340
## TON  0.1321441 -0.16128932  0.4008477 -0.009584841
## TN  -0.4143703  0.25675267 -0.2376303  0.031457881
## NH4  0.2795587  0.05641766  0.2078297 -0.205917361
## DOC -0.1606776 -0.26635688 -0.3519496  0.411823946
## SO4  0.2558941  0.48167703  0.7413800 -0.454845809
## CL   1.0000000  0.16475033  0.5523138 -0.334130816
## CA   0.1647503  1.00000000  0.3780952 -0.812295299
## MG   0.5523138  0.37809523  1.0000000 -0.545448165
## H   -0.3341308 -0.81229530 -0.5454482  1.000000000</code></pre>
<p>Now the magnitude of the association is easier to see: <span class="math inline">\(corr(NO3,TN)=0.98\)</span>, <span class="math inline">\(corr(SO4,MG)=0.74\)</span> and <span class="math inline">\(corr(CA,H)=-0.81\)</span>.</p>
</div>
<div id="linear-combination-of-variables" class="section level3">
<h3><span class="header-section-number">8.2.2</span> Linear combination of variables</h3>
<p>A first step in multivariate analyses (PCA, DFA, FA etc.) is usually to centre the variables to zero mean: <span class="math inline">\(y^*=y-\mu\)</span> so that <span class="math inline">\(\mu^*=0\)</span> and <span class="math inline">\(\sigma^*=\sigma\)</span>. Compare Chapter <a href="math.html#math">2</a>.</p>
<p>Then, the fundamental concept underlying multivariate analyses is to derive new linear combinations of the variables that summarise the variation in the original data set:</p>
<p><span class="math display" id="eq:lincomb">\[\begin{equation}
z_{ik}=u_{1k}\cdot y_{i1}+u_{2k}\cdot y_{i2}+\ldots+u_{pk}\cdot y_{ip}
\tag{8.4}
\end{equation}\]</span></p>
<p><span class="math inline">\(z_{ik}\)</span> is the value of the <em>new</em> variable <span class="math inline">\(k\)</span> for object <span class="math inline">\(i\)</span>. The object can be a timestep or a spatial location or else. <span class="math inline">\(u_{1k},\ldots,u_{pk}\)</span> are the <em>coefficients</em> indicating how much each original variable contributes to the linear combination. This is the <em>Eigenvector</em> of the covariance matrix (more on this later). <span class="math inline">\(y_{i1},\ldots,y_{ip}\)</span> are the values of <em>original</em> variables <span class="math inline">\(1,\ldots,p\)</span> for object <span class="math inline">\(i\)</span>.</p>
<p>The new variables are called (depending on the method) <strong>principal components</strong>, factors or discriminant functions. They are extracted so that the first explains most of the variance in the original variables, the second explains most of the remaining variance after the first has been extracted but is <em>independent</em> of (uncorrelated with) the first … and so on. The number of new variables <span class="math inline">\(k\)</span> is the same as the number of original variables <span class="math inline">\(p\)</span>, although the variance is usually consolidated in the first few new variables. The unknown coefficients <span class="math inline">\(u_{1k},\ldots,u_{pk}\)</span> are determined via so called <strong>Eigenanalysis</strong> (see below).</p>
<p>A graphical explanation of what PCA does is <strong>axis rotation</strong>. We illustrate this with an example in 2D from <span class="citation">Green (<a href="#ref-green1997" role="doc-biblioref">1997</a>)</span>, cited in <span class="citation">Quinn and Keough (<a href="#ref-quinn2002" role="doc-biblioref">2002</a>)</span>; total biomass of red land crabs against number of burrows at ten forested sites on Christmas Island (Figure <a href="multivariate.html#fig:crabs1">8.2</a>). When the data are centred then the origin is where the individual means intersect (the centroid); the original axes are the black lines in the plots. Through PCA, the original axes are now rotated such that the 1st new one (PC1 in red) goes in the direction of the greatest spread of points and the 2nd new axis (PC2 in red), still perpendicular, covers the secondary variation. This is also the principle with more dimensions, though we cannot visualise it anymore.</p>
<div class="figure" style="text-align: center"><span id="fig:crabs1"></span>
<img src="qm4g_files/figure-html/crabs1-1.png" alt="Total biomass of red land crabs against number of burrows at ten forested sites on Christmas Island. When the data are centred then the origin is where the individual means intersect (the centroid); the original axes are the black lines in the plots. Through PCA, the original axes are now rotated such that the 1st new one (PC1 in red) goes in the direction of the greatest spread of points and the 2nd new axis (PC2 in red), still perpendicular, covers the secondary variation. For one data point, marked by the cross, the new coordinates in PC1 and PC2 direction are marked in blue and green, respectively. After: @quinn2002, based on data by @green1997." width="672" />
<p class="caption">
Figure 8.2: Total biomass of red land crabs against number of burrows at ten forested sites on Christmas Island. When the data are centred then the origin is where the individual means intersect (the centroid); the original axes are the black lines in the plots. Through PCA, the original axes are now rotated such that the 1st new one (PC1 in red) goes in the direction of the greatest spread of points and the 2nd new axis (PC2 in red), still perpendicular, covers the secondary variation. For one data point, marked by the cross, the new coordinates in PC1 and PC2 direction are marked in blue and green, respectively. After: <span class="citation">Quinn and Keough (<a href="#ref-quinn2002" role="doc-biblioref">2002</a>)</span>, based on data by <span class="citation">Green (<a href="#ref-green1997" role="doc-biblioref">1997</a>)</span>.
</p>
</div>
<p>Let’s look at one data point, marked by the cross in Figure <a href="multivariate.html#fig:crabs1">8.2</a>. The new coordinate in PC1 direction, marked in blue in the plot, is calculated after Equation <a href="multivariate.html#eq:lincomb">(8.4)</a> as:
<span class="math display" id="eq:zi1">\[\begin{equation}
\color{blue}{z_{i1}=u_{11}\cdot y_{i1}+u_{21}\cdot y_{i2}}
\tag{8.5}
\end{equation}\]</span></p>
<p>The new coordinate in PC2 direction, marked in green in the plot, is calculated after Equation <a href="multivariate.html#eq:lincomb">(8.4)</a> as:
<span class="math display" id="eq:zi2">\[\begin{equation}
\color{green}{z_{i2}=u_{12}\cdot y_{i1}+u_{22}\cdot y_{i2}}
\tag{8.6}
\end{equation}\]</span></p>
<p>To see how this is a rotation consider Figure <a href="multivariate.html#fig:rotation">8.3</a>: After rotating the <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> axes by angle <span class="math inline">\(\alpha\)</span>, the new coordinates <span class="math inline">\(x&#39;\)</span> and <span class="math inline">\(y&#39;\)</span> of point <span class="math inline">\(P\)</span> are:
<span class="math display" id="eq:xtick">\[\begin{equation}
x&#39;=\cos\alpha\cdot x+\sin\alpha\cdot y
\tag{8.7}
\end{equation}\]</span>
<span class="math display" id="eq:ytick">\[\begin{equation}
y&#39;=-\sin\alpha\cdot x+\cos\alpha\cdot y
\tag{8.8}
\end{equation}\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:rotation"></span>
<img src="figs/Coordinate_system_rotation_svg.svg" alt="Geometric derivation of the new coordinates of a point $P$ after rotating the $x$ and $y$ axes by an angle $\alpha$. Note that $\sin\alpha=\frac{opposite}{hypotenuse}$ and $\cos\alpha=\frac{adjacent}{hypotenuse}$. &lt;br&gt;&lt;small&gt;By Jochen Burghardt - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=71157895.&lt;/small&gt;" width="80%" />
<p class="caption">
Figure 8.3: Geometric derivation of the new coordinates of a point <span class="math inline">\(P\)</span> after rotating the <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> axes by an angle <span class="math inline">\(\alpha\)</span>. Note that <span class="math inline">\(\sin\alpha=\frac{opposite}{hypotenuse}\)</span> and <span class="math inline">\(\cos\alpha=\frac{adjacent}{hypotenuse}\)</span>. <br><small>By Jochen Burghardt - Own work, CC BY-SA 4.0, <a href="https://commons.wikimedia.org/w/index.php?curid=71157895" class="uri">https://commons.wikimedia.org/w/index.php?curid=71157895</a>.</small>
</p>
</div>
</div>
<div id="eigenanalysis" class="section level3">
<h3><span class="header-section-number">8.2.3</span> Eigenanalysis</h3>
<p>The vector of coefficients <span class="math inline">\(\begin{pmatrix}u_{1k}&amp;\ldots&amp;u_{pk}\end{pmatrix}^T\)</span> in Equation <a href="multivariate.html#eq:lincomb">(8.4)</a> is called the <span class="math inline">\(k\)</span>th <strong>Eigenvector</strong>. The Eigenvectors span the new coordinate system after rotation as illustrated in Figure <a href="multivariate.html#fig:crabs2">8.4</a>: The first Eigenvector is <span class="math inline">\(\color{blue}{\begin{pmatrix}u_{11}&amp;u_{21}\end{pmatrix}^T}\)</span> and the second Eigenvector is <span class="math inline">\(\color{green}{\begin{pmatrix}u_{12}&amp;u_{22}\end{pmatrix}^T}\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:crabs2"></span>
<img src="qm4g_files/figure-html/crabs2-1.png" alt="PCA of the crabs dataset as axis rotation. The two **Eigenvectors** spanning the new coordinate system are marked in blue and green. After: @quinn2002, based on data by @green1997." width="672" />
<p class="caption">
Figure 8.4: PCA of the crabs dataset as axis rotation. The two <strong>Eigenvectors</strong> spanning the new coordinate system are marked in blue and green. After: <span class="citation">Quinn and Keough (<a href="#ref-quinn2002" role="doc-biblioref">2002</a>)</span>, based on data by <span class="citation">Green (<a href="#ref-green1997" role="doc-biblioref">1997</a>)</span>.
</p>
</div>
<p>The Eigenvectors are found so that the following equation holds:<a href="#fn26" class="footnote-ref" id="fnref26"><sup>26</sup></a></p>
<p><span class="math display" id="eq:eigen">\[\begin{equation}
\mathbf{C}\cdot\begin{pmatrix}
u_{1k}\\u_{2k}\\\vdots\\u_{pk}
\end{pmatrix}=\lambda_k\cdot\begin{pmatrix}
u_{1k}\\u_{2k}\\\vdots\\u_{pk}
\end{pmatrix}
\tag{8.9}
\end{equation}\]</span></p>
<p><span class="math inline">\(\mathbf{C}\)</span> is the covariance matrix and <span class="math inline">\(\lambda_1,\ldots,\lambda_k\)</span> are the so called <strong>Eigenvalues</strong>, which equal the amount of variance explained by each new variable. The sum of variances (Eigenvalues) of the new variables equals the sum of variances of the original variables.</p>
<p>The Eigenanalysis can be carried out, i.e. the Eigenvectors and Eigenvalues determined, on the covariance matrix <span class="math inline">\(\mathbf{C}\)</span> or the correlation matrix <span class="math inline">\(\mathbf{R}\)</span> using <em>Spectral Decomposition (Eigendecomposition)</em> or on the data matrix (raw, centred or standardised) using <em>Singular Value Decomposition</em>, which is the more general method. We don’t go into the details of these techniques here.</p>
<p>If the Eigenanalysis is carried out on the covariance matrix <span class="math inline">\(\mathbf{C}\)</span> then <span class="math inline">\(\sum_{j=1}^{k}\lambda_j=Tr(\mathbf{C})\)</span>, i.e. the sum of the Eigenvalues is the trace of <span class="math inline">\(\mathbf{C}\)</span>. The <strong>trace</strong> is defined as the sum of the diagonal elements of a matrix, i.e. here the sum of the variances of the original <em>centred</em> variables. This isappropriate when the variables are measured in comparable units and differences in variance make an important contribution to interpretation.</p>
<p>If the Eigenanalysis is carried out on the correlation matrix <span class="math inline">\(\mathbf{R}\)</span> then <span class="math inline">\(\sum_{j=1}^{k}\lambda_j=Tr(\mathbf{R})\)</span>, i.e. the sum of the variances of the original <em>standardised</em> variables: <span class="math inline">\(y^*=\frac{y-\mu}{\sigma}\)</span> so that <span class="math inline">\(\mu^*=0\)</span> and <span class="math inline">\(\sigma^*=1\)</span>. Compare Chapter <a href="math.html#math">2</a>. This is necessary when variables are measured in very different units or scales, otherwise variables with large values/variances may dominate the results.</p>
<p>In the stream chemistry example we best work with standardised data because variances are orders of magnitude different.<a href="#fn27" class="footnote-ref" id="fnref27"><sup>27</sup></a> We use the <code>prcomp()</code> function on the data matrix, and tell it to standardise the variables by <code>scale=TRUE</code>. To only centre the variables we’d use <code>center=TRUE</code> and <code>scale=FALSE</code> (which is the default).</p>
<div class="sourceCode" id="cb163"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb163-1"><a href="multivariate.html#cb163-1"></a>streams_pca &lt;-<span class="st"> </span><span class="kw">prcomp</span>(streams_log[,<span class="dv">6</span><span class="op">:</span><span class="dv">15</span>], <span class="dt">scale=</span><span class="ot">TRUE</span>)</span>
<span id="cb163-2"><a href="multivariate.html#cb163-2"></a>streams_pca</span></code></pre></div>
<pre><code>## Standard deviations (1, .., p=10):
##  [1] 1.85038489 1.57254556 1.08217040 0.96516016 0.86381333 0.80396686
##  [7] 0.61930355 0.36160718 0.30237094 0.04783651
## 
## Rotation (n x k) = (10 x 10):
##            PC1         PC2          PC3         PC4         PC5        PC6
## NO3 -0.2608012  0.51882614  0.048948664  0.22991773 -0.02957754 -0.2391299
## TON  0.1470633 -0.29927267  0.515134430  0.54375942 -0.11855680  0.3029571
## TN  -0.2284551  0.51019467  0.153554316  0.34323284 -0.02311043 -0.1993705
## NH4  0.2279503 -0.07531476 -0.486775784  0.65372357 -0.26639758 -0.1033473
## DOC -0.2882751 -0.14662430  0.562051058 -0.09028595 -0.43309048 -0.2021881
## SO4  0.3684647  0.22497988  0.241732702  0.01627193  0.53819564 -0.1550440
## CL   0.3578631 -0.15795736 -0.017738392 -0.16532689 -0.32510731 -0.7380443
## CA   0.2811469  0.44581191  0.080590317 -0.18761722 -0.36044030  0.2312748
## MG   0.4716303  0.01538667  0.300889286  0.10730721  0.23553548 -0.1420917
## H   -0.3972262 -0.28148573  0.005572391  0.15438867  0.38112577 -0.3424885
##             PC7         PC8         PC9         PC10
## NO3 -0.16234834  0.05636563 -0.03009849 -0.720448525
## TON -0.29797877 -0.28358340 -0.20552744 -0.102368378
## TN  -0.18049740  0.05424407 -0.04787331  0.684124042
## NH4  0.43198173  0.06398237  0.07470189 -0.011423521
## DOC  0.53976801  0.22174766  0.04692825 -0.013216360
## SO4  0.48378154 -0.15731293 -0.42645336 -0.023891086
## CL  -0.29877601 -0.19877842 -0.19925916  0.021153361
## CA   0.15884541 -0.57822152  0.37080339  0.006383739
## MG  -0.13127371  0.41741127  0.63806761 -0.031885844
## H    0.09240295 -0.53606219  0.42495406  0.008249647</code></pre>
<p>In the output, what’s called “standard deviations” are the square roots of the Eigenvalues of the principal components (PCs). So squaring them yields the Eigenvalues:</p>
<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb165-1"><a href="multivariate.html#cb165-1"></a>ev &lt;-<span class="st"> </span>streams_pca<span class="op">$</span>sdev<span class="op">^</span><span class="dv">2</span></span>
<span id="cb165-2"><a href="multivariate.html#cb165-2"></a>ev</span></code></pre></div>
<pre><code>##  [1] 3.423924226 2.472899532 1.171092767 0.931534143 0.746173464 0.646362713
##  [7] 0.383536884 0.130759752 0.091428188 0.002288332</code></pre>
<p>We can verify that in this example the Eigenvalues sum to 10, which is the trace of the correlation matrix:</p>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="multivariate.html#cb167-1"></a><span class="kw">sum</span>(ev)</span></code></pre></div>
<pre><code>## [1] 10</code></pre>
<p>By normalising the Eigenvalues by 10 we get the fraction of original variance explained by each PC:</p>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb169-1"><a href="multivariate.html#cb169-1"></a>ev<span class="op">/</span><span class="kw">sum</span>(ev)<span class="op">*</span><span class="dv">100</span></span></code></pre></div>
<pre><code>##  [1] 34.23924226 24.72899532 11.71092767  9.31534143  7.46173464  6.46362713
##  [7]  3.83536884  1.30759752  0.91428188  0.02288332</code></pre>
<p>Plotting the Eigenvalues in a <strong>screeplot</strong> shows that the bulk of the original variance is consolidated in the first three PCs; together they explain 71% of the original variance:</p>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb171-1"><a href="multivariate.html#cb171-1"></a><span class="kw">plot</span>(<span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">10</span>,<span class="dv">1</span>), ev, <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&#39;b&#39;</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">4</span>),</span>
<span id="cb171-2"><a href="multivariate.html#cb171-2"></a>     <span class="dt">xlab=</span><span class="st">&quot;Principal Component&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Eigenvalue&quot;</span>)</span></code></pre></div>
<p><img src="qm4g_files/figure-html/unnamed-chunk-53-1.png" width="80%" /></p>
<p>What’s called “rotation” in the output are the Eigenvectors, measuring how much much each original variable contributes to the PCs. We look at the Eigenvectors of the first three PCs only here:</p>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb172-1"><a href="multivariate.html#cb172-1"></a>streams_pca<span class="op">$</span>rotation[,<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]</span></code></pre></div>
<pre><code>##            PC1         PC2          PC3
## NO3 -0.2608012  0.51882614  0.048948664
## TON  0.1470633 -0.29927267  0.515134430
## TN  -0.2284551  0.51019467  0.153554316
## NH4  0.2279503 -0.07531476 -0.486775784
## DOC -0.2882751 -0.14662430  0.562051058
## SO4  0.3684647  0.22497988  0.241732702
## CL   0.3578631 -0.15795736 -0.017738392
## CA   0.2811469  0.44581191  0.080590317
## MG   0.4716303  0.01538667  0.300889286
## H   -0.3972262 -0.28148573  0.005572391</code></pre>
<p>We see that SO4, log(Cl) and Mg load strongly positively on PC1 and log(H) loads strongly negatively. It seems PC1 distinguishes alkaline from acidic sites. NO3, total nitrogen and Ca load strongly positively on PC2. The correlation between total nitrogen and NO3 is again apparent. My interpretation is that PC2 distinguishes subsurface (high Ca, high nitrogen) from surface water sites. Total organic nitrogen and log(dissolved organic carbon) load strongly positively on PC3 and NH4 loads strongly negatively. PC3 might distinguish organic from mineralised (surface) water sites. We can also visualise this in so called <strong>biplots</strong>:</p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb174-1"><a href="multivariate.html#cb174-1"></a><span class="kw">biplot</span>(streams_pca, <span class="dt">choices=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb174-2"><a href="multivariate.html#cb174-2"></a><span class="kw">biplot</span>(streams_pca, <span class="dt">choices=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))</span>
<span id="cb174-3"><a href="multivariate.html#cb174-3"></a><span class="kw">biplot</span>(streams_pca, <span class="dt">choices=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>))</span></code></pre></div>
<p><img src="qm4g_files/figure-html/unnamed-chunk-55-1.png" width="33%" /><img src="qm4g_files/figure-html/unnamed-chunk-55-2.png" width="33%" /><img src="qm4g_files/figure-html/unnamed-chunk-55-3.png" width="33%" /></p>
<p>In these plots we see the original data points (black numbers) in PC space. The red vectors show the contributions of the original variables to the PCs. Vectors close together in all three dimensions indicate correlation; we can see this best here for NO3 and total nitrogen.</p>
<p>Lastly, in order to shows what we can do with the PCs, we extract the values of the first three PCs and plot them against the other catchment characteristics in the dataset:</p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb175-1"><a href="multivariate.html#cb175-1"></a><span class="kw">library</span>(latex2exp)</span>
<span id="cb175-2"><a href="multivariate.html#cb175-2"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>){</span>
<span id="cb175-3"><a href="multivariate.html#cb175-3"></a>  <span class="kw">plot</span>(streams_pca<span class="op">$</span>x[,i],streams_log<span class="op">$</span>max_elev, <span class="dt">xlab=</span><span class="kw">paste</span>(<span class="st">&quot;PC&quot;</span>,i), <span class="dt">ylab=</span><span class="st">&quot;Max. elevation&quot;</span>,</span>
<span id="cb175-4"><a href="multivariate.html#cb175-4"></a>       <span class="dt">main=</span><span class="kw">TeX</span>(<span class="kw">paste</span>(<span class="st">&quot;$</span><span class="ch">\\</span><span class="st">rho$=&quot;</span>,<span class="kw">round</span>(<span class="kw">cor</span>(streams_pca<span class="op">$</span>x[,i],streams_log<span class="op">$</span>sample_elev),<span class="dt">digits=</span><span class="dv">3</span>))))</span>
<span id="cb175-5"><a href="multivariate.html#cb175-5"></a>  <span class="kw">plot</span>(streams_pca<span class="op">$</span>x[,i],streams_log<span class="op">$</span>sample_elev, <span class="dt">xlab=</span><span class="kw">paste</span>(<span class="st">&quot;PC&quot;</span>,i), <span class="dt">ylab=</span><span class="st">&quot;Sample elevation&quot;</span>,</span>
<span id="cb175-6"><a href="multivariate.html#cb175-6"></a>       <span class="dt">main=</span><span class="kw">TeX</span>(<span class="kw">paste</span>(<span class="st">&quot;$</span><span class="ch">\\</span><span class="st">rho$=&quot;</span>,<span class="kw">round</span>(<span class="kw">cor</span>(streams_pca<span class="op">$</span>x[,i],streams_log<span class="op">$</span>stream_length),<span class="dt">digits=</span><span class="dv">3</span>))))</span>
<span id="cb175-7"><a href="multivariate.html#cb175-7"></a>  <span class="kw">plot</span>(streams_pca<span class="op">$</span>x[,i],streams_log<span class="op">$</span>stream_length, <span class="dt">xlab=</span><span class="kw">paste</span>(<span class="st">&quot;PC&quot;</span>,i), <span class="dt">ylab=</span><span class="st">&quot;Stream length&quot;</span>,</span>
<span id="cb175-8"><a href="multivariate.html#cb175-8"></a>       <span class="dt">main=</span><span class="kw">TeX</span>(<span class="kw">paste</span>(<span class="st">&quot;$</span><span class="ch">\\</span><span class="st">rho$=&quot;</span>,<span class="kw">round</span>(<span class="kw">cor</span>(streams_pca<span class="op">$</span>x[,i],streams_log<span class="op">$</span>catchm_area),<span class="dt">digits=</span><span class="dv">3</span>))))</span>
<span id="cb175-9"><a href="multivariate.html#cb175-9"></a>  <span class="kw">plot</span>(streams_pca<span class="op">$</span>x[,i],streams_log<span class="op">$</span>catchm_area, <span class="dt">xlab=</span><span class="kw">paste</span>(<span class="st">&quot;PC&quot;</span>,i), <span class="dt">ylab=</span><span class="st">&quot;Catchment area&quot;</span>,</span>
<span id="cb175-10"><a href="multivariate.html#cb175-10"></a>       <span class="dt">main=</span><span class="kw">TeX</span>(<span class="kw">paste</span>(<span class="st">&quot;$</span><span class="ch">\\</span><span class="st">rho$=&quot;</span>,<span class="kw">round</span>(<span class="kw">cor</span>(streams_pca<span class="op">$</span>x[,i],streams_log<span class="op">$</span>max_elev),<span class="dt">digits=</span><span class="dv">3</span>))))</span>
<span id="cb175-11"><a href="multivariate.html#cb175-11"></a>}</span></code></pre></div>
<p><img src="qm4g_files/figure-html/unnamed-chunk-56-1.png" width="25%" /><img src="qm4g_files/figure-html/unnamed-chunk-56-2.png" width="25%" /><img src="qm4g_files/figure-html/unnamed-chunk-56-3.png" width="25%" /><img src="qm4g_files/figure-html/unnamed-chunk-56-4.png" width="25%" /><img src="qm4g_files/figure-html/unnamed-chunk-56-5.png" width="25%" /><img src="qm4g_files/figure-html/unnamed-chunk-56-6.png" width="25%" /><img src="qm4g_files/figure-html/unnamed-chunk-56-7.png" width="25%" /><img src="qm4g_files/figure-html/unnamed-chunk-56-8.png" width="25%" /><img src="qm4g_files/figure-html/unnamed-chunk-56-9.png" width="25%" /><img src="qm4g_files/figure-html/unnamed-chunk-56-10.png" width="25%" /><img src="qm4g_files/figure-html/unnamed-chunk-56-11.png" width="25%" /><img src="qm4g_files/figure-html/unnamed-chunk-56-12.png" width="25%" /></p>
<p>On top of these plots are the linear correlation coefficients <span class="math inline">\(\rho\)</span> as a first indication of associations between PCs and other catchment characteristics. The strongest correlation is that between PC1 and max. elevation. Given our interpretation of PC1 from above we can conclude that waters tend to get more acidic (less alkaline) with elevation, perhaps because these sites are less influenced by human activity that tends to introduce alkalinity through agricultural or urban runoff and discharges.</p>
<p>We could also use the PCs as predictors in a regression model. This is especially useful for predictors that are highly correlated; these we can consolidate in a few PCs that then serve as independent predictors. Interpretation, however - as the stream chemistry example demonstrates - may be challenging.</p>
</div>
</div>
<div id="manova" class="section level2">
<h2><span class="header-section-number">8.3</span> Multivariate ANOVA (MANOVA)</h2>
<p>MANOVA extends ANOVA to the case of multiple response variables. Let’s remind ourselves (compare Chapter <a href="categoricalvars.html#categoricalvars">4</a>) that the categorical predictor variables are called <strong>factors</strong>, the categories of each factor are called <strong>levels</strong>, <strong>groups</strong> or <strong>treatments</strong>. And the parameters are called <strong>effects</strong>. Where ANOVA asks whether <em>means</em> across factor levels (groups) are significantly different (compare the yields example from Chapter <a href="categoricalvars.html#categoricalvars">4</a>), MANOVA asks whether <em>centroids</em> across factor levels (groups) are significantly different, taking all response variables together.</p>
<p>We illustrate <strong>single-factor MANOVA</strong> with a marine sediment dataset from <span class="citation">Haynes et al. (<a href="#ref-haynes1995" role="doc-biblioref">1995</a>)</span>, cited in <span class="citation">Quinn and Keough (<a href="#ref-quinn2002" role="doc-biblioref">2002</a>)</span>. We use this dataset to test for differences between sites in trace metal concentrations in marine sediments off the Victorian coast in southern Australia:</p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="multivariate.html#cb176-1"></a><span class="co"># load sediments data</span></span>
<span id="cb176-2"><a href="multivariate.html#cb176-2"></a>sediments &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="st">&quot;data/sediments.txt&quot;</span>,<span class="dt">header=</span>T)</span>
<span id="cb176-3"><a href="multivariate.html#cb176-3"></a>sediments</span></code></pre></div>
<pre><code>##        site    CU    PB    NI    MN
## 1    Delray  7.15 12.00  7.40 137.0
## 2    Delray  5.85  6.35  7.05 106.5
## 3    Delray  5.45  4.95 10.40 102.5
## 4    Delray  8.50  7.05  6.40 125.0
## 5  Seaspray  9.45 10.50 12.15 162.5
## 6  Seaspray  6.30 18.35 11.85 129.5
## 7  Seaspray  7.80 18.10 10.70 224.5
## 8  Seaspray  9.15 16.55 12.20 247.0
## 9  Woodside 10.80 28.90 18.30 403.5
## 10 Woodside 21.20 11.60 18.20 377.0
## 11 Woodside 18.20 13.85 15.30 256.0
## 12 Woodside  7.60  9.35  9.25 440.0</code></pre>
<p>“Delray Beach” is a site of a proposed wastewater outfall, and “Seaspray” and “Woodside” are two possible control sites. At each site, two sediment cores were taken at four randomly chosen locations and analysed for trace metals (CU=copper, PB=lead, NI=nickel, MN=manganese). The results from the two cores were averaged. So we have <span class="math inline">\(n=12\)</span> objects here, the rows, and <span class="math inline">\(p=4\)</span> response variables, the columns. MANOVA tests for the effects of site on the response variables <em>taken together</em>. A first thing to do - as always - is to look at the scatterplot matrix, with datapoints coloured by site:</p>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb178-1"><a href="multivariate.html#cb178-1"></a><span class="kw">pairs</span>(sediments[,<span class="dv">2</span><span class="op">:</span><span class="dv">5</span>], <span class="dt">diag.panel =</span> panel.hist, <span class="dt">lower.panel =</span> panel.cor,</span>
<span id="cb178-2"><a href="multivariate.html#cb178-2"></a>      <span class="dt">pch=</span><span class="dv">21</span>, <span class="dt">bg=</span><span class="kw">c</span>(<span class="st">&quot;black&quot;</span>,<span class="st">&quot;red&quot;</span>,<span class="st">&quot;blue&quot;</span>)[<span class="kw">as.factor</span>(sediments<span class="op">$</span>site)])</span></code></pre></div>
<p><img src="qm4g_files/figure-html/unnamed-chunk-59-1.png" width="80%" /></p>
<p>We notice strong correlations between variables. And log-transformation of all variables looks like a good idea to make the residuals later on adhere better to the normality assumption:</p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb179-1"><a href="multivariate.html#cb179-1"></a>sediments_log &lt;-<span class="st"> </span>sediments</span>
<span id="cb179-2"><a href="multivariate.html#cb179-2"></a>sediments_log[,<span class="dv">2</span><span class="op">:</span><span class="dv">5</span>] &lt;-<span class="st"> </span><span class="kw">log</span>(sediments[,<span class="dv">2</span><span class="op">:</span><span class="dv">5</span>])</span>
<span id="cb179-3"><a href="multivariate.html#cb179-3"></a><span class="kw">pairs</span>(sediments_log[,<span class="dv">2</span><span class="op">:</span><span class="dv">5</span>], <span class="dt">diag.panel =</span> panel.hist, <span class="dt">lower.panel =</span> panel.cor,</span>
<span id="cb179-4"><a href="multivariate.html#cb179-4"></a>      <span class="dt">pch=</span><span class="dv">21</span>, <span class="dt">bg=</span><span class="kw">c</span>(<span class="st">&quot;black&quot;</span>,<span class="st">&quot;red&quot;</span>,<span class="st">&quot;blue&quot;</span>)[<span class="kw">as.factor</span>(sediments_log<span class="op">$</span>site)])</span></code></pre></div>
<p><img src="qm4g_files/figure-html/unnamed-chunk-60-1.png" width="80%" /></p>
<p>The best single discriminator of the groups seems to be manganese; this will be confirmed by DFA in Chapter <a href="multivariate.html#dfa">8.4</a>. The covariance and correlation matrices look as follows:</p>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb180-1"><a href="multivariate.html#cb180-1"></a><span class="kw">cov</span>(sediments_log[,<span class="dv">2</span><span class="op">:</span><span class="dv">5</span>])</span></code></pre></div>
<pre><code>##            CU         PB        NI        MN
## CU 0.17590985 0.07360668 0.1010596 0.1421417
## PB 0.07360668 0.25382115 0.1074090 0.1484811
## NI 0.10105956 0.10740900 0.1209024 0.1190148
## MN 0.14214166 0.14848105 0.1190148 0.2794815</code></pre>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb182-1"><a href="multivariate.html#cb182-1"></a><span class="kw">cor</span>(sediments_log[,<span class="dv">2</span><span class="op">:</span><span class="dv">5</span>])</span></code></pre></div>
<pre><code>##           CU        PB        NI        MN
## CU 1.0000000 0.3483439 0.6929708 0.6410617
## PB 0.3483439 1.0000000 0.6131396 0.5574817
## NI 0.6929708 0.6131396 1.0000000 0.6474510
## MN 0.6410617 0.5574817 0.6474510 1.0000000</code></pre>
<p>Like PCA, MANOVA begins by centring the variables and deriving new linear combinations through Equation <a href="multivariate.html#eq:lincomb">(8.4)</a>. For the sediments data, the equation is:
<span class="math display" id="eq:lincombsed">\[\begin{equation}
z_{ik}=u_{1k}\cdot(\log Cu)_i+u_{2k}\cdot(\log Pb)_i+u_{3k}\cdot(\log Ni)_i+u_{4k}\cdot(\log Mn)_i
\tag{8.10}
\end{equation}\]</span></p>
<p>MANOVA finds the linear combination <span class="math inline">\(z_k\)</span> of response variables which <em>maximises the between-group variance:within-group variance ratio</em> of the data. This is called the <strong>1st discriminant function</strong>; similar, but not identical, to the 1st principal component in PCA.<a href="#fn28" class="footnote-ref" id="fnref28"><sup>28</sup></a> The Null hypothesis of MANOVA is that there are no differences between factor level centroids. Before testing this hypothesis, MANOVA proceeds through the following steps.</p>
<div id="steps-of-manova" class="section level3">
<h3><span class="header-section-number">8.3.1</span> Steps of MANOVA</h3>
<p>The first step is to calculate sums-of-squares-and-cross-products (SSCP) matrices:</p>
<ol style="list-style-type: decimal">
<li>The <strong>total matrix</strong> <span class="math inline">\(\mathbf T\)</span>, replacing SSY of ANOVA (compare Figure <a href="categoricalvars.html#fig:anovassysse">4.2</a>,left)<a href="#fn29" class="footnote-ref" id="fnref29"><sup>29</sup></a></li>
</ol>
<p><span class="math display" id="eq:totalmatrix">\[\begin{equation}
\mathbf T=\begin{pmatrix}
\sum_{i=1}^{n}\left(y_{i1}-\bar y_1\right)^2 &amp; \cdots &amp; \sum_{i=1}^{n}\left(y_{ip}-\bar y_p\right)\cdot\left(y_{i1}-\bar y_1\right) \\
\vdots &amp; \ddots &amp; \vdots \\
\sum_{i=1}^{n}\left(y_{i1}-\bar y_1\right)\cdot\left(y_{ip}-\bar y_p\right) &amp; \cdots &amp; \sum_{i=1}^{n}\left(y_{ip}-\bar y_p\right)^2
\end{pmatrix}
\tag{8.11}
\end{equation}\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>The <strong>error (or residual) matrix</strong> <span class="math inline">\(\mathbf E\)</span> (“within-group SSCP”), replacing SSE of ANOVA (compare Figure <a href="categoricalvars.html#fig:anovassysse">4.2</a>,right)<a href="#fn30" class="footnote-ref" id="fnref30"><sup>30</sup></a></li>
</ol>
<p><span class="math display" id="eq:errormatrix">\[\begin{equation}
\mathbf E=\begin{pmatrix}
\sum_{j=1}^{k}\sum_{i=1}^{n_j}\left(y_{ji1}-\bar y_{j1}\right)^2 &amp; \cdots &amp; \sum_{j=1}^{k}\sum_{i=1}^{n_j}\left(y_{jip}-\bar y_{jp}\right)\cdot\left(y_{ji1}-\bar y_{j1}\right) \\
\vdots &amp; \ddots &amp; \vdots \\
\sum_{j=1}^{k}\sum_{i=1}^{n_j}\left(y_{ji1}-\bar y_{j1}\right)\cdot\left(y_{jip}-\bar y_{jp}\right) &amp; \cdots &amp; \sum_{j=1}^{k}\sum_{i=1}^{n_j}\left(y_{jip}-\bar y_{jp}\right)^2
\end{pmatrix}
\tag{8.12}
\end{equation}\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>The <strong>hypothesis (or effect) matrix</strong> <span class="math inline">\(\mathbf H=\mathbf T-\mathbf E\)</span> (“between-group SSCP”), replacing SSA of ANOVA</li>
</ol>
<p>The second step is to calculate the <strong>between-group SSCP:within-group SSCP ratio</strong>:
<span class="math display" id="eq:heinv">\[\begin{equation}
\mathbf H\cdot\mathbf E^{-1}
\tag{8.13}
\end{equation}\]</span></p>
<p>This replaces the F statistic <span class="math inline">\(\frac{\frac{SSA}{k-1}}{\frac{SSE}{n-k}}\)</span> of ANOVA.</p>
<p>The third step is to perform an Eigenanalysis of the resulting matrix <span class="math inline">\(\mathbf H\cdot\mathbf E^{-1}\)</span>, like in PCA.<a href="#fn31" class="footnote-ref" id="fnref31"><sup>31</sup></a> The resulting Eigenvectors contain the coefficients <span class="math inline">\(u\)</span> of Equation <a href="multivariate.html#eq:lincombsed">(8.10)</a>. Their Eigenvalues measure how much of the between-group SSCP:within-group SSCP ratio is explained by each Eigenvector. As with PCA, the sum of the Eigenvalues is the trace of the matrix <span class="math inline">\(\mathbf H\cdot\mathbf E^{-1}\)</span>: <span class="math inline">\(Tr\left(\mathbf H\cdot\mathbf E^{-1}\right)=\sum_{i=1}^{p}diag\left(\mathbf H\cdot\mathbf E^{-1}\right)\)</span>, i.e. the sum of the univariate between-group SSCP:within-group SSCP ratios.</p>
<p>Fourth, the linear combination producing the largest eigenvalue maximises the between-group variance:within-group variance ratio, with the Eigenvector containing the coefficients <span class="math inline">\(u\)</span>.</p>
<p>Let’s perform the MANOVA for the marine sediment example:</p>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb184-1"><a href="multivariate.html#cb184-1"></a><span class="co"># perform MANOVA</span></span>
<span id="cb184-2"><a href="multivariate.html#cb184-2"></a>sediments_fit &lt;-<span class="st"> </span><span class="kw">manova</span>(<span class="kw">cbind</span>(CU,PB,NI,MN) <span class="op">~</span><span class="st"> </span>site, <span class="dt">data=</span>sediments_log)</span></code></pre></div>
<p>The SSCP matrices are:</p>
<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb185-1"><a href="multivariate.html#cb185-1"></a><span class="co"># extract SSCP matrices</span></span>
<span id="cb185-2"><a href="multivariate.html#cb185-2"></a><span class="kw">summary</span>(sediments_fit)<span class="op">$</span>SS</span></code></pre></div>
<pre><code>## $site
##           CU        PB        NI       MN
## CU 1.0380915 0.8039161 0.8716555 1.621477
## PB 0.8039161 1.4459688 1.0182672 1.457377
## NI 0.8716555 1.0182672 0.8749872 1.445578
## MN 1.6214775 1.4573771 1.4455784 2.582111
## 
## $Residuals
##              CU          PB         NI          MN
## CU  0.896916821 0.005757348  0.2399997 -0.05791919
## PB  0.005757348 1.346063896  0.1632318  0.17591445
## NI  0.239999748 0.163231764  0.4549387 -0.13641609
## MN -0.057919186 0.175914446 -0.1364161  0.49218487</code></pre>
<p>“Site” is the effect matrix in our case and “Residuals” is the residual matrix. Hence <span class="math inline">\(\mathbf H\cdot\mathbf E^{-1}\)</span> is:</p>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb187-1"><a href="multivariate.html#cb187-1"></a><span class="co"># calculate H E^-1</span></span>
<span id="cb187-2"><a href="multivariate.html#cb187-2"></a>H &lt;-<span class="st"> </span><span class="kw">summary</span>(sediments_fit)<span class="op">$</span>SS<span class="op">$</span>site</span>
<span id="cb187-3"><a href="multivariate.html#cb187-3"></a>E &lt;-<span class="st"> </span><span class="kw">summary</span>(sediments_fit)<span class="op">$</span>SS<span class="op">$</span>Residuals</span>
<span id="cb187-4"><a href="multivariate.html#cb187-4"></a>H_E &lt;-<span class="st"> </span>H <span class="op">%*%</span><span class="st"> </span><span class="kw">solve</span>(E)</span></code></pre></div>
<p>The Eigenvalues of <span class="math inline">\(\mathbf H\cdot\mathbf E^{-1}\)</span> are:</p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb188-1"><a href="multivariate.html#cb188-1"></a><span class="co"># extract Eigenvalues</span></span>
<span id="cb188-2"><a href="multivariate.html#cb188-2"></a><span class="kw">summary</span>(sediments_fit)<span class="op">$</span>Eigenvalues</span></code></pre></div>
<pre><code>##          [,1]      [,2]          [,3]          [,4]
## site 9.979034 0.5702563 -3.809231e-16 -3.809231e-16</code></pre>
<p>We see that the 1st discriminant function has the largest Eigenvalue and thus maximises the between group variance:within group variance ratio.</p>
</div>
<div id="null-hypothesis-test-of-manova" class="section level3">
<h3><span class="header-section-number">8.3.2</span> Null hypothesis test of MANOVA</h3>
<p>The null hypothesis that there are no differences between factor level centroids can be tested using a function of the largest Eigenvalue of <span class="math inline">\(\mathbf H\cdot\mathbf E^{-1}\)</span>, i.e. the Eigenvalue of the 1st discriminant function, as the test statistic. This is also called <strong>Roy’s largest root</strong>. The sampling distribution of this statistic is not well understood, however, so it is usually treated as an <em>approximate</em> F statistic, following an F-distribution (analogous to ANOVA). Other choices of the test statistic are possible.</p>
<p>Among these is a function of <strong>Wilk’s lambda</strong>:
<span class="math display" id="eq:wilkslambda">\[\begin{equation}
\frac{|\mathbf E|}{|\mathbf T|}
\tag{8.14}
\end{equation}\]</span></p>
<p>Wilk’s lambda is a measure of how much variance remains <em>unexplained</em> by the model, with smaller values indicating greater group differences. <span class="math inline">\(|M|\)</span> is the so called <strong>determinant</strong> of a matrix <span class="math inline">\(M\)</span>, the generalised variance of that matrix, e.g.:
<span class="math display" id="eq:determinant">\[\begin{equation}
\begin{vmatrix}
a &amp; b &amp; c \\
d &amp; e &amp; f \\
g &amp; h &amp; i \\
\end{vmatrix}=(a\cdot e\cdot i+b\cdot f\cdot g+c\cdot d\cdot h)-(c\cdot e\cdot g+b\cdot d\cdot i+a\cdot f\cdot h)
\tag{8.15}
\end{equation}\]</span></p>
<p>The determinant fulfills the same role as the trace, it just uses more information from the matrix.</p>
<p>Another choice is a function of the <strong>Pillai trace</strong>:
<span class="math display" id="eq:pillaitrace">\[\begin{equation}
Tr\left(\mathbf H\cdot\mathbf T^{-1}\right)
\tag{8.16}
\end{equation}\]</span></p>
<p>This measures of how much variance is <em>explained</em> by the model, with greater values indicating greater group differences.</p>
<p>A fourth choice is a function of the <strong>Hotelling-Lawley trace</strong>:
<span class="math display" id="eq:hltrace">\[\begin{equation}
\frac{|\mathbf H|}{|\mathbf E|}=Tr\left(\mathbf H\cdot\mathbf E^{-1}\right)
\tag{8.17}
\end{equation}\]</span></p>
<p>This measures the ratio of <em>explained to unexplained</em> variance, with greater values indicating greater group differences.</p>
<p>Since the sampling distributions of these statistics are not well understood either, they are again treated as <em>approximate</em> F statistics, following an F-distribution. In practice it is advised to look at all of these variants of the test statistic to see if the results are consistent.</p>
<p>In the marine sediments example, the four test statistics give comparable results, though the test statistic based on the Pillai trace wouldn’t meet the conventional significance level of 0.01:</p>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb190-1"><a href="multivariate.html#cb190-1"></a><span class="kw">summary</span>(sediments_fit, <span class="dt">test =</span><span class="st">&quot;Roy&quot;</span>)</span></code></pre></div>
<pre><code>##           Df   Roy approx F num Df den Df    Pr(&gt;F)    
## site       2 9.979   17.463      4      7 0.0009535 ***
## Residuals  9                                           
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb192-1"><a href="multivariate.html#cb192-1"></a><span class="kw">summary</span>(sediments_fit, <span class="dt">test =</span><span class="st">&quot;Wilks&quot;</span>)</span></code></pre></div>
<pre><code>##           Df    Wilks approx F num Df den Df   Pr(&gt;F)   
## site       2 0.058005   4.7281      8     12 0.008228 **
## Residuals  9                                            
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb194-1"><a href="multivariate.html#cb194-1"></a><span class="kw">summary</span>(sediments_fit, <span class="dt">test =</span><span class="st">&quot;Pillai&quot;</span>)</span></code></pre></div>
<pre><code>##           Df Pillai approx F num Df den Df  Pr(&gt;F)  
## site       2 1.2721   3.0582      8     14 0.03248 *
## Residuals  9                                        
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb196-1"><a href="multivariate.html#cb196-1"></a><span class="kw">summary</span>(sediments_fit, <span class="dt">test =</span><span class="st">&quot;Hotelling-Lawley&quot;</span>)</span></code></pre></div>
<pre><code>##           Df Hotelling-Lawley approx F num Df den Df   Pr(&gt;F)   
## site       2           10.549   6.5933      8     10 0.003762 **
## Residuals  9                                                    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We may still conclude that the trace metal signatures at the sites (summarised by the centroids) are significantly different.</p>
</div>
</div>
<div id="dfa" class="section level2">
<h2><span class="header-section-number">8.4</span> Discriminant Function Analysis (DFA)</h2>
<p>DFA and MANOVA are mathematically identical, though DFA emphasises <em>classification</em> and <em>prediction</em>, and MANOVA tests hypotheses about group differences. MANOVA tests whether centroids across factor levels (groups) are significantly different, using the 1st discriminant function, a linear combination of all response variables. DFA uses all discriminant functions to calculate the <em>probability of correctly assigning observations to their pre-determined groups</em>, e.g. sites in the marine sediment example.</p>
<p>MANOVA is usually the first step in DFA because if the difference between group centroids is not significant then the discriminant functions will not be very useful for separating groups and therefore classifying observations. DFA is also used to <em>classify new observations into one of the groups</em>, calculating some measure of success of classification. DFA goes further than MANOVA in that it uses not only the 1st discriminant function but also additional discriminant functions to classify observations into groups.</p>
<p>In our marine sediments example, we already saw in the last section that the 1st discriminant function has the largest Eigenvalue and thus maximises the between group variance:within group variance ratio, hence separates the three sites best. We can now plot the discriminant functions using DFA:</p>
<div class="sourceCode" id="cb198"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb198-1"><a href="multivariate.html#cb198-1"></a><span class="co"># use linear discriminant function analysis which is available from the MASS package</span></span>
<span id="cb198-2"><a href="multivariate.html#cb198-2"></a><span class="kw">library</span>(MASS)</span>
<span id="cb198-3"><a href="multivariate.html#cb198-3"></a>sediments_dfa &lt;-<span class="st"> </span><span class="kw">lda</span>(sediments_log[,<span class="dv">2</span><span class="op">:</span><span class="dv">5</span>], sediments_log<span class="op">$</span>site)</span>
<span id="cb198-4"><a href="multivariate.html#cb198-4"></a><span class="co"># plot discriminant functions</span></span>
<span id="cb198-5"><a href="multivariate.html#cb198-5"></a><span class="kw">plot</span>(sediments_dfa, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>), <span class="dt">xlab=</span><span class="st">&quot;DF1&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;DF2&quot;</span>)</span></code></pre></div>
<p><img src="qm4g_files/figure-html/unnamed-chunk-67-1.png" width="80%" /></p>
<p>We see that DF1 separates the three sites pretty well, better than the original variables in the scatterplot matrix in the previous section. DF2 adds little to this separation, as could already be seen in its comparably small Eigenvalue. The Eigenvalues of DF3 and DF4 are so small that they are considered zero and not displayed here.<a href="#fn32" class="footnote-ref" id="fnref32"><sup>32</sup></a></p>
<p>Looking at the Eigenvectors, the 4th original variable (Mn) contributes most to the 1st discriminant function - it is the best single discriminator - as could be seen already in the scatterplot matrix in the previous section:</p>
<div class="sourceCode" id="cb199"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb199-1"><a href="multivariate.html#cb199-1"></a><span class="co"># extract Eigenvectors</span></span>
<span id="cb199-2"><a href="multivariate.html#cb199-2"></a>sediments_dfa<span class="op">$</span>scaling</span></code></pre></div>
<pre><code>##           LD1       LD2
## CU -0.5440295  1.316031
## PB  0.2146654 -2.189776
## NI -2.9056055 -1.357409
## MN -4.0423138  1.243619</code></pre>
<div id="classification" class="section level3">
<h3><span class="header-section-number">8.4.1</span> Classification</h3>
<p>Now we use the DFs to classify objects to (known) groups. For every measurement (object <span class="math inline">\(i\)</span>), three classification scores are calculated, one for every site (group <span class="math inline">\(j\)</span>). The classification scores are mathematically relate to the discriminant functions (Eigenvectors <span class="math inline">\(\mathbf{u_k}\)</span>). See <span class="citation">Quinn and Keough (<a href="#ref-quinn2002" role="doc-biblioref">2002</a>)</span> for details. The maximum classification score determines the group classification for every object. From this, for every object a probability of belonging to each of the groups is calculated:</p>
<div class="sourceCode" id="cb201"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb201-1"><a href="multivariate.html#cb201-1"></a><span class="co"># predict group membership</span></span>
<span id="cb201-2"><a href="multivariate.html#cb201-2"></a>sediments_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(sediments_dfa)</span>
<span id="cb201-3"><a href="multivariate.html#cb201-3"></a><span class="co"># extract probabilities</span></span>
<span id="cb201-4"><a href="multivariate.html#cb201-4"></a>sediments_pred<span class="op">$</span>posterior</span></code></pre></div>
<pre><code>##             Delray     Seaspray     Woodside
##  [1,] 9.553853e-01 4.461474e-02 4.340147e-09
##  [2,] 9.995656e-01 4.343614e-04 2.164477e-12
##  [3,] 9.898169e-01 1.018312e-02 1.629359e-09
##  [4,] 9.990303e-01 9.697271e-04 7.978679e-11
##  [5,] 2.583485e-02 9.735664e-01 5.987078e-04
##  [6,] 7.737557e-02 9.226241e-01 2.921063e-07
##  [7,] 8.769258e-04 9.964792e-01 2.643924e-03
##  [8,] 9.369190e-05 9.054815e-01 9.442482e-02
##  [9,] 7.200478e-13 3.429294e-04 9.996571e-01
## [10,] 1.499728e-13 2.924074e-06 9.999971e-01
## [11,] 3.126048e-07 2.703925e-02 9.729604e-01
## [12,] 3.718937e-08 1.928730e-03 9.980712e-01</code></pre>
<p>We see that we can recover the original group membership this way: the first four samples have highest probability to be from Delray, the next four from Seaspray and the last four from Woodside. We can see this by constructing a contingency table:</p>
<div class="sourceCode" id="cb203"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb203-1"><a href="multivariate.html#cb203-1"></a><span class="co"># contingency table of prediction:group matches</span></span>
<span id="cb203-2"><a href="multivariate.html#cb203-2"></a><span class="kw">table</span>(sediments_pred<span class="op">$</span>class, sediments_log<span class="op">$</span>site)</span></code></pre></div>
<pre><code>##           
##            Delray Seaspray Woodside
##   Delray        4        0        0
##   Seaspray      0        4        0
##   Woodside      0        0        4</code></pre>
</div>
<div id="leave-one-out-cross-validation" class="section level3">
<h3><span class="header-section-number">8.4.2</span> Leave-one-out cross-validation</h3>
<p>Predicting the training data is no big deal. A more severe test of the classification approach is out-of-sample prediction (“validation”), which in small datasets we approximate by leave-one-out cross-validation (LOOCV). The algorithm is as follows:</p>
<ol style="list-style-type: decimal">
<li>For every measurement (object <span class="math inline">\(i\)</span>), do a DFA using only the remaining measurements (leaving object <span class="math inline">\(i\)</span> out)</li>
<li>Classify object <span class="math inline">\(i\)</span> with the corresponding classification equations</li>
<li>Repeat for all measurements</li>
</ol>
<p>LOOCV is implemented in the <code>lda()</code> function when we set the argument <code>CV=TRUE</code>:</p>
<div class="sourceCode" id="cb205"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb205-1"><a href="multivariate.html#cb205-1"></a><span class="co"># run DFA in LOOCV mode</span></span>
<span id="cb205-2"><a href="multivariate.html#cb205-2"></a>sediments_dfa2 &lt;-<span class="st"> </span><span class="kw">lda</span>(sediments_log[,<span class="dv">2</span><span class="op">:</span><span class="dv">5</span>], sediments_log<span class="op">$</span>site, <span class="dt">CV=</span><span class="ot">TRUE</span>)</span>
<span id="cb205-3"><a href="multivariate.html#cb205-3"></a><span class="co"># extract probabilities</span></span>
<span id="cb205-4"><a href="multivariate.html#cb205-4"></a><span class="co"># we don&#39;t have to go via predict() in LOOCV mode</span></span>
<span id="cb205-5"><a href="multivariate.html#cb205-5"></a>sediments_dfa2<span class="op">$</span>posterior</span></code></pre></div>
<pre><code>##             Delray     Seaspray     Woodside
##  [1,] 7.534088e-01 2.465911e-01 1.328822e-07
##  [2,] 9.992390e-01 7.610054e-04 9.004512e-12
##  [3,] 7.797312e-04 9.992083e-01 1.195126e-05
##  [4,] 9.970969e-01 2.903135e-03 1.975322e-09
##  [5,] 7.524195e-02 9.223035e-01 2.454566e-03
##  [6,] 9.658863e-01 3.411368e-02 3.749923e-14
##  [7,] 1.872969e-03 9.912369e-01 6.890127e-03
##  [8,] 4.963475e-05 8.438512e-01 1.560992e-01
##  [9,] 4.892510e-17 7.520442e-02 9.247956e-01
## [10,] 1.485722e-16 5.716217e-08 9.999999e-01
## [11,] 3.653954e-06 7.148432e-01 2.851531e-01
## [12,] 1.000000e+00 3.851447e-09 6.456065e-33</code></pre>
<p>Now the classification is less than perfect, as can be seen in the contingency table:</p>
<div class="sourceCode" id="cb207"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb207-1"><a href="multivariate.html#cb207-1"></a><span class="co"># contingency table of LOOCV prediction:group matches</span></span>
<span id="cb207-2"><a href="multivariate.html#cb207-2"></a><span class="kw">table</span>(sediments_dfa2<span class="op">$</span>class, sediments_log<span class="op">$</span>site)</span></code></pre></div>
<pre><code>##           
##            Delray Seaspray Woodside
##   Delray        3        1        1
##   Seaspray      1        3        1
##   Woodside      0        0        2</code></pre>
<p>A useful diagnostic, for this and other classification problems is the <strong>confusion matrix</strong> (Table <a href="multivariate.html#tab:confusion">8.1</a>).</p>
<table class=" lightable-paper lightable-hover" style='font-size: 12px; font-family: "Arial Narrow", arial, helvetica, sans-serif; margin-left: auto; margin-right: auto;'>
<caption style="font-size: initial !important;">
<span id="tab:confusion">Table 8.1: </span>Confusion matrix. Type I error means incorrectly rejecting a true null hypothesis (detecting an effect that is not present). Type II error means failing to reject a false null hypothesis (failing to detect an effect that is present).
</caption>
<thead>
<tr>
<th style="text-align:left;">
Total population
</th>
<th style="text-align:left;">
Validation positive (1)
</th>
<th style="text-align:left;">
Validation negative (0)
</th>
<th style="text-align:left;">
Prevalence = sum of validation positives / total population
</th>
<th style="text-align:left;">
<ul>
<li></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Prediction positive (1)
</td>
<td style="text-align:left;">
Sum of TRUE positives
</td>
<td style="text-align:left;">
Sum of FALSE positives (Type I error)
</td>
<td style="text-align:left;">
Precision = sum of TRUE positives / sum of prediction positives
</td>
<td style="text-align:left;">
False discovery rate = sum of FALSE positives / sum of prediction positives
</td>
</tr>
<tr>
<td style="text-align:left;">
Prediction negative (0)
</td>
<td style="text-align:left;">
Sum of FALSE negatives (Type II error)
</td>
<td style="text-align:left;">
Sum of TRUE negatives
</td>
<td style="text-align:left;">
False omission rate = sum of FALSE negatives / sum of prediction negatives
</td>
<td style="text-align:left;">
Negative predictive value = sum of TRUE negatives / sum of prediction negatives
</td>
</tr>
<tr>
<td style="text-align:left;">
Accuracy = (sum of TRUE positives + sum of TRUE negatives) / total population
</td>
<td style="text-align:left;">
TRUE positive rate (TPR), Sensitivity = sum of TRUE positives / sum of Validation positives
</td>
<td style="text-align:left;">
FALSE positive rate (FPR), Fall-out = sum of false positives / sum of validation negatives
</td>
<td style="text-align:left;">
Positive likelihood ratio (LR+) = TPR / FPR
</td>
<td style="text-align:left;">
Diagnostic odds ratio = LR+ / LR-
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
FALSE negative rate (FNR), Miss rate = sum of FALSE negatives / sum of validation positives
</td>
<td style="text-align:left;">
TRUE negative rate (TNR), Specificity = sum of TRUE negatives / sum of validation negatives
</td>
<td style="text-align:left;">
Negative likelihood ratio (LR-) = FNR / TNR
</td>
<td style="text-align:left;">
</td>
</tr>
</tbody>
</table></li>
</ul>
<p>Filled in for the marine sediments example this is Table <a href="multivariate.html#tab:confusion2">8.2</a>.</p>
<table class=" lightable-paper lightable-hover" style='font-size: 12px; font-family: "Arial Narrow", arial, helvetica, sans-serif; margin-left: auto; margin-right: auto;'>
<caption style="font-size: initial !important;">
<span id="tab:confusion2">Table 8.2: </span>Confusion matrix for marine sediments example.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Total population = 36
</th>
<th style="text-align:left;">
Validation positive (1)
</th>
<th style="text-align:left;">
Validation negative (0)
</th>
<th style="text-align:left;">
Prevalence = 1/3
</th>
<th style="text-align:left;">
<ul>
<li></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Prediction positive (1)
</td>
<td style="text-align:left;">
8 TRUE positives
</td>
<td style="text-align:left;">
4 FALSE positives (Type I error)
</td>
<td style="text-align:left;">
Precision = 2/3
</td>
<td style="text-align:left;">
False discovery rate = 1/3
</td>
</tr>
<tr>
<td style="text-align:left;">
Prediction negative (0)
</td>
<td style="text-align:left;">
4 FALSE positives (Type II error)
</td>
<td style="text-align:left;">
20 TRUE negatives
</td>
<td style="text-align:left;">
False omission rate = 1/6
</td>
<td style="text-align:left;">
Negative predictive value = 5/6
</td>
</tr>
<tr>
<td style="text-align:left;">
Accuracy = 7/9
</td>
<td style="text-align:left;">
TPR = 2/3
</td>
<td style="text-align:left;">
FPR = 1/6
</td>
<td style="text-align:left;">
LR+ = 4
</td>
<td style="text-align:left;">
Diagnostic odds ratio = 10
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
FNR = 1/3
</td>
<td style="text-align:left;">
TNR = 5/6
</td>
<td style="text-align:left;">
LR- = 2/5
</td>
<td style="text-align:left;">
</td>
</tr>
</tbody>
</table></li>
</ul>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-green1997">
<p>Green, P. T. 1997. “Red Crabs in Rain Forest on Christmas Island, Indian Ocean: Activity Patterns, Density and Biomass.” <em>Journal of Tropical Ecology</em> 13 (1): 17–38. <a href="https://doi.org/10.1017/S0266467400010221">https://doi.org/10.1017/S0266467400010221</a>.</p>
</div>
<div id="ref-haynes1995">
<p>Haynes, D., D. Toohey, D. Clarke, and D. Marney. 1995. “Temporal and Spatial Variation in Concentrations of Trace Metals in Coastal Sediments from the Ninety Mile Beach, Victoria, Australia.” <em>Marine Pollution Bulletin</em> 30 (6): 414–18. <a href="https://doi.org/https://doi.org/10.1016/0025-326X(95)00058-U">https://doi.org/https://doi.org/10.1016/0025-326X(95)00058-U</a>.</p>
</div>
<div id="ref-lovett2000">
<p>Lovett, G. M., K. C. Weathers, and W. V. Sobczak. 2000. “Nitrogen Saturation and Retention in Forested Watersheds of the Catskill Mountains, New York.” <em>Ecological Applications</em> 10 (1): 73–84. <a href="https://doi.org/https://doi.org/10.1890/1051-0761(2000)010%5B0073:NSARIF%5D2.0.CO;2">https://doi.org/https://doi.org/10.1890/1051-0761(2000)010[0073:NSARIF]2.0.CO;2</a>.</p>
</div>
<div id="ref-quinn2002">
<p>Quinn, G. P., and M. J. Keough. 2002. <em>Experimental Design and Data Analysis for Biologists</em>. Cambridge: Cambridge University Press.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="23">
<li id="fn23"><p><a href="https://corneliussenf.com/" class="uri">https://corneliussenf.com/</a><a href="multivariate.html#fnref23" class="footnote-back">↩︎</a></p></li>
<li id="fn24"><p>It’s called screeplot because it looks like the scree at the foot of a mountain.<a href="multivariate.html#fnref24" class="footnote-back">↩︎</a></p></li>
<li id="fn25"><p>The variance of a variable is of course the covariance between that variable and itself, e.g. <span class="math inline">\(var\left(y_1\right)=cov\left(y_1,y_1\right)\)</span>.<a href="multivariate.html#fnref25" class="footnote-back">↩︎</a></p></li>
<li id="fn26"><p>An example - though one that doesn’t involve a covariance matrix! - is: <span class="math inline">\(\begin{pmatrix}2&amp;3\\2&amp;1\end{pmatrix}\cdot\begin{pmatrix}3\\2\end{pmatrix}=\begin{pmatrix}12\\8\end{pmatrix}=4\cdot\begin{pmatrix}3\\2\end{pmatrix}\)</span>.<a href="multivariate.html#fnref26" class="footnote-back">↩︎</a></p></li>
<li id="fn27"><p>You could try this yourself either way to see what difference it makes.<a href="multivariate.html#fnref27" class="footnote-back">↩︎</a></p></li>
<li id="fn28"><p>As we will see in Chapter <a href="multivariate.html#dfa">8.4</a>, the mathematics of MANOVA and DFA are identical.<a href="multivariate.html#fnref28" class="footnote-back">↩︎</a></p></li>
<li id="fn29"><p>Note, the total matrix is the covariance matrix without division by <span class="math inline">\(n-1\)</span>; the diagonals are the SSYs of the individual variables.<a href="multivariate.html#fnref29" class="footnote-back">↩︎</a></p></li>
<li id="fn30"><p>Note, the diagonals of the residual matrix are the SSEs of the individual variables.<a href="multivariate.html#fnref30" class="footnote-back">↩︎</a></p></li>
<li id="fn31"><p>By comparison, in PCA the Eigenanalysis is done on the covariance or correlation matrix.<a href="multivariate.html#fnref31" class="footnote-back">↩︎</a></p></li>
<li id="fn32"><p>In fact, the MANOVA estimated the Eigenvalues of DF3 and DF4 to be negative, which is a numerical error that happens at these very small numbers.<a href="multivariate.html#fnref32" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="glms.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="solutions-to-exercises.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
