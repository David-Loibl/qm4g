[
["index.html", "Quantitative Methods for Geographers Preface", " Quantitative Methods for Geographers Tobias Krueger 2020-12-07 Preface This is the script of the course ‘Quantitative Methods for Geographers’ run at the Geography Department of Humboldt-Universität zu Berlin. "],
["orga.html", "Chapter 1 Organisational matters 1.1 Motivating example 1.2 Topics 1.3 Format", " Chapter 1 Organisational matters Welcome to the course Quantitative Methods for Geographers (online edition), which consists of a seminar and a PC lab. In the seminar, theoretical input will be provided by myself up until January and by Tobia Lakes in the last four sessions in February. Part of this course is a PC lab run by Dirk Pflugmacher and David Loibl. 1.1 Motivating example Figure 1.1 shows the spatial variation of annual average precipitation and annual average temperature over Germany. Before reading on, reflect a minute on what might control this spatial variation. Figure 1.1: Maps of annual average precipitation (left) and annual average air temperature (right) over Germany from 1961 to 1990. Source: https://www.dwd.de/EN/climate_environment/climateatlas/climateatlas_node.html. The spatial variation of average precipitation and temperature in our climatic zone is controlled mainly by elevation and continentality: the higher up we go the more it rains over the year and the colder it gets; and the further East we go the drier it gets while a temperature effect is not visible. There are of course more nuanced effects such as aspect, but these are not so clearly visible in this figure. And this is precisely the goal of large parts of this course: To try and explain patterns in data (so called response variables) over space and time - such as rainfall and temperature - with other data (so called predictor variables). We do this using statistical inference methods. This includes separating dominant predictors from not so dominant predictors. It also includes making predictions with the relationships we find, e.g. for interpolating between data points to create maps like Figure 1.1. By the end of this course you will have learned the following: You have understood the concept of statistical inference using the linear model in depth. This includes linear regression with one or more predictors (multiple regression). It also includes mildly non-linear models and specific types of responses, dealt with by so called Generalised Linear Models. You have worked with metric as well as categorical predictors and mixtures of the two. You have learned metrics to compare and simplify models and evaluate their assumptions. You have understood the principles of extending these techniques to cases of more than one response variable (multivariate methods). You have learned specific techniques for working with spatial data. You can carry out the corresponding analyses in the software R. 1.2 Topics The timing of topics is shown in Table 1.1. Table 1.1: Schedule of Quantitative Methods for Geographers. Week Reading Monday seminar Wednesday PC lab 1 Organisational matters no class due to Dies Academicus Introduction to R 2 Mathematical preliminaries Introductions Data manipulation and import/export with R 3 The linear model Mathematical preliminaries Visualization and data manipulation with R 4 Categorical variables (ANOVA) and dummy coding The linear model Linear regression 5 Multiple linear regression Categorical variables (ANOVA) and dummy coding Hypothesis testing and ANOVA 6 Maximum Likelihood and outlook to Bayesian statistics Multiple linear regression Multiple linear regression 7 Generalized linear models I Maximum Likelihood and outlook to Bayesian statistics Outlook to machine learning 8 Generalized linear models II Generalized linear models I Generalized linear models I 9 Multivariate methods I Generalized linear models II Generalized linear models II 10 Multivariate methods II Multivariate methods I Principal Component Analysis 11 tbc Multivariate methods II Discriminant Function Analysis and model validation 12 tbc Understanding spatial data (Tobia) Spatial data and cluster analysis in R 13 tbc Point pattern analysis (Tobia) Point pattern analysis and spatial auto-correlation 14 tbc Spatial autocorrelation and interpolation (Tobia) Semivariogram analysis and kriging 15 tbc Spatial weights and linear modeling (Tobia) Spatial regression models 1.3 Format During this digital semester, the learning mode will be mainly reading. You are required to read a chapter of this script each week, which will then be discussed in a ZOOM session the following Monday 13:00-15:00 (see link on Moodle). The reading listed for each week in the table above is due the following week. I will provide you with guiding questions and small quizzes to guide your reading. You are required to post questions on the topics or aspects you would like me to focus on during the seminar in the Moodle Forum by each Friday. This way we know what to discuss each Monday and I will prepare some lecture-style input. We can also discuss questions that arise in the PC labs. Some questions might already be answered via the Forum. In the PC labs, Dirk and David will give you homework, which you need to submit via Moodle to pass the course. They will explain this in detail. The final exam is a project similar to an extended homework, in which you will be able to apply and expand on topics studied in class, using datasets provided in the research context of the Geography Department. The project will be done individually (other than in previous years) and each of you will get their own topic and data. We will allocate topics towards the end of the semester via Moodle. The project has to be submitted as a HTML document created by R Markdown towards the end of the semester break (deadline tbc). "],
["math.html", "Chapter 2 Mathematical preliminaries 2.1 Logarithm and exponentiation 2.2 Centring and standardisation 2.3 Derivatives 2.4 Matrix algebra 2.5 Exercises", " Chapter 2 Mathematical preliminaries In this chapter we get a few mathematical preliminaries out of the way that are important for later chapters. If you feel rusty on any of these then please read up on them elsewhere. 2.1 Logarithm and exponentiation The following is inspired by Gelman and Nolan (2002). Suppose you have an amoeba that takes one hour to divide (Figure 2.1), and then the two amoebas each divide in one more hour, and so forth. What is the equation of the number of amoebas, \\(y\\), as a function of time, \\(t\\) (in hours)? Figure 2.1: Amoeba dividing. Source: http://www.gutenberg.org/files/18451/18451-h/images/illus002.jpg. The equation is: \\[\\begin{equation} y=2^t \\tag{2.1} \\end{equation}\\] This is an exponential function with base 2 and exponent \\(t\\). Figure 2.2 shows two plots of this function. (Don’t worry, you will start to understand the R code better as you progress in the PC labs.) t &lt;- seq(1, 6) y &lt;- 2^t plot(t, y, pch = 19, type = &#39;b&#39;) plot(t, log(y), pch = 19, type = &#39;b&#39;) Figure 2.2: Left: Plot of Equation 2.1. Right: Plot of Equation 2.1 on logarithmic scale. The inverse of the exponential function is the logarithmic function: \\[\\begin{equation} log(y)=log(2^t)=t \\cdot log(2) \\tag{2.2} \\end{equation}\\] Since the logarithm of \\(y\\) is a linear function of \\(t\\) (Equation (2.2)), the right-hand side of Figure 2.2 (\\(y\\) on logarithmic scale) displays a straight line. Common bases of the logarithmic function are: \\[\\begin{equation} log_2\\left(2^t\\right)=lb\\left(2^t\\right)=t \\tag{2.3} \\end{equation}\\] This is called the binary logarithm (lb). \\[\\begin{equation} log_{10}\\left(10^t\\right)=lg\\left(10^t\\right)=t \\tag{2.4} \\end{equation}\\] This is called the common logarithm (lg). \\[\\begin{equation} log_e\\left(e^t\\right)=ln\\left(e^t\\right)=t \\tag{2.5} \\end{equation}\\] This is called the natural logarithm (ln) with \\(e \\approx 2.7183\\) being Euler’s constant. Note, programming often uses a different notation, which will also be used from now on in this course: \\[\\begin{equation} ln()=log() \\tag{2.6} \\end{equation}\\] \\[\\begin{equation} e^t=\\exp(t) \\tag{2.7} \\end{equation}\\] Basic rules for exponentiation are: \\[\\begin{equation} a^m \\cdot a^n=a^{m+n} \\tag{2.8} \\end{equation}\\] \\[\\begin{equation} a^n \\cdot b^n=(a \\cdot b)^n \\tag{2.9} \\end{equation}\\] \\[\\begin{equation} \\frac{a^m}{a^n}=a^{m-n} \\tag{2.10} \\end{equation}\\] \\[\\begin{equation} \\frac{a^n}{b^n}=\\left(\\frac{a}{b}\\right)^n \\tag{2.11} \\end{equation}\\] \\[\\begin{equation} \\left(a^m\\right)^n=a^{m \\cdot n} \\tag{2.12} \\end{equation}\\] At this point it is also useful to remind ourselves of the meaning of the sum and product symbols: \\[\\begin{equation} \\sum_{i=1}^{n}x_i=x_1+x_2+\\ldots+x_n \\tag{2.13} \\end{equation}\\] This signifies the sum of all \\(x_i\\) for \\(i\\) taking integer values from 1 to \\(n\\). \\[\\begin{equation} \\prod_{i=1}^{n}x_i=x_1 \\cdot x_2 \\cdot \\ldots \\cdot x_n \\tag{2.14} \\end{equation}\\] This signifies the product of all \\(x_i\\) for \\(i\\) taking integer values from 1 to \\(n\\). The basic rules of logarithm are: \\[\\begin{equation} log(u \\cdot v)=log(u)+log(v) \\tag{2.15} \\end{equation}\\] \\[\\begin{equation} log\\left(\\frac{u}{v}\\right)=log(u)-log(v) \\tag{2.16} \\end{equation}\\] \\[\\begin{equation} log\\left(u^r\\right)=r \\cdot log(u) \\tag{2.17} \\end{equation}\\] 2.2 Centring and standardisation Centring and standardisation are used to transform different datasets onto the same scale. We will need this for the multivariate methods in later sessions (Chapter 8). Centring means subtracting from every data point \\(y\\) the overall mean of the dataset \\(\\bar{y}\\): \\[\\begin{equation} y^*=y-\\bar{y} \\tag{2.18} \\end{equation}\\] This yields new data points \\(y^*\\) and a new mean \\(\\bar{y^*}=0\\) while the standard deviation of the transformed data remains the same: \\(s_{y^*}=s_y\\). Centring thus shifts the data histogram to be centred on zero, but does not change its shape (Figure 2.3). # draw random sample of size 1000 from normal distribution with mean 1 and standard deviation 2, i.e. y~N(1,2) y &lt;- rnorm(1000, mean = 1, sd = 2) # mean ybar &lt;- mean(y) # standard deviation s_y &lt;- sd(y) # histogram, raw hist(y, freq = FALSE, xlim = c(-10,10), ylim = c(0, 0.4), main = &quot;&quot;, xlab = &quot;y&quot;, ylab = &quot;relative frequency&quot;) lines(c(ybar, ybar), c(0,0.4), lwd = 3) # histogram, centred hist(y-ybar, freq = FALSE, xlim = c(-10,10), ylim = c(0, 0.4), main = &quot;&quot;, xlab = &quot;y*&quot;, ylab = &quot;relative frequency&quot;) lines(c(0, 0), c(0,0.4), lwd = 3) Figure 2.3: Histogram of dataset \\(y\\) (left) and centred dataset \\(y^*\\) (right). The vertical line represents the mean. Standardisation means subtracting from every data point \\(y\\) the overall mean of the dataset \\(\\bar{y}\\) and additionally dividing by the standard deviation \\(s_y\\): \\[\\begin{equation} y^*=\\frac{y-\\bar{y}}{s_y} \\tag{2.19} \\end{equation}\\] This yields new data points \\(y^*\\), a new mean \\(\\bar{y^*}=0\\) and a new standard deviation \\(s_{y^*}=1\\). Standardisation thus shifts the data histogram to be centred on zero and expands or contracts it to have unit standard deviation (Figure 2.4). # histogram, raw hist(y, freq = FALSE, xlim = c(-10,10), ylim = c(0, 0.4), main = &quot;&quot;, xlab = &quot;y&quot;, ylab = &quot;relative frequency&quot;) lines(c(ybar, ybar), c(0,0.4), lwd = 3) # histogram, standardised hist((y-ybar)/s_y, freq = FALSE, xlim = c(-10,10), ylim = c(0, 0.4), main = &quot;&quot;, xlab = &quot;y*&quot;, ylab = &quot;relative frequency&quot;) lines(c(0, 0), c(0,0.4), lwd = 3) Figure 2.4: Histogram of dataset \\(y\\) (left) and standardised dataset \\(y^*\\) (right). The vertical line represents the mean. If the original data \\(y\\) were normally distributed (like we set it up for the plots above) then standardisation would transform \\(y\\) to the scale of the standard normal distribution, i.e. a normal distribution with mean 0 and standard deviation 1. If you feel rusty on the normal distribution (or probability distributions in general) then Wikipedia is as good a source as any: https://en.wikipedia.org/wiki/Normal_distribution. 2.3 Derivatives The first derivative of a function \\(f(x)\\), written as \\(f&#39;(x)\\) or \\(\\frac{df(x)}{dx}\\), can be interpreted graphically as the slope of that function, i.e. the tangent line of a certain point of the function (Figure 2.5, left). Figure 2.5: Left: Tangent line of function \\(f(x)\\). Centre: Secant line of function \\(f(x)\\) between point \\(f\\left(x_0\\right)\\) and point \\(f\\left(x_0+h\\right)\\); the horizontal distance between these two points is \\(\\Delta x\\) and the vertical distance is \\(\\Delta f(x)\\). Right: Set of secant lines of function \\(f(x)\\) between point \\(f\\left(x_0\\right)\\) and point \\(f\\left(x_0+h\\right)\\) for progressively decreasing increments \\(h\\). Source: https://en.wikipedia.org/wiki/Derivative. Mathematically, the slope amounts to the limiting value of the ratio of the (vertical) increment of the function, \\(\\Delta f(x)\\), for an (horizontal) increment of \\(x\\), \\(\\Delta x\\), for \\(\\Delta x\\) approaching zero, \\(\\Delta x \\to 0\\): \\[\\begin{equation} f&#39;(x)=\\frac{df(x)}{dx}=\\lim_{\\Delta x \\to 0}\\frac{\\Delta f(x)}{\\Delta x} \\tag{2.20} \\end{equation}\\] This can be visualised as a secant line of the function between two points, \\(x_0\\) and \\(x_0+h\\) (Figure 2.5, centre), whose slope is: \\[\\begin{equation} \\frac{\\Delta f(x)}{\\Delta x}=\\frac{f\\left(x_0+h\\right)-f\\left(x_0\\right)}{\\left(x_0+h\\right)-\\left(x_0\\right)}=\\frac{f\\left(x_0+h\\right)-f\\left(x_0\\right)}{h} \\tag{2.21} \\end{equation}\\] As \\(h\\) approaches zero (Figure 2.5, right) we reach the limiting value of the slope at point \\(x_0\\), which is the first derivative: \\[\\begin{equation} \\frac{df(x)}{dx}=\\lim_{h \\to 0}\\frac{f\\left(x_0+h\\right)-f\\left(x_0\\right)}{h} \\tag{2.22} \\end{equation}\\] The first derivative is useful for finding minima, maxima and inflexion points of a function, because this is where the slope is zero, \\(\\frac{df(x)}{dx}=0\\) (Figure 2.6). The second derivative, measuring the curvature of the function, tells us whether these points are minima \\(\\left(\\frac{d^2f(x)}{dx^2}&gt;0\\right)\\), maxima \\(\\left(\\frac{d^2f(x)}{dx^2}&lt;0\\right)\\) or inflexion points \\(\\left(\\frac{d^2f(x)}{dx^2}=0\\right)\\), but often we already know that a function has only a single minimum or maximum and then we do not need the second derivative. Figure 2.6: Use of first and second derivative to determine minima, maxima and inflexion points of a function. Source: http://hyperphysics.phy-astr.gsu.edu/hbase/math/maxmin.html. The differentiation rules are listed below: If \\(y=f(t)=t^a\\) then \\(\\frac{dy}{dt}=a \\cdot t^{a-1}\\), i.e. multiplying the function with the exponent and reducing the exponent by one gives you the derivative of \\(f(x)\\). Constant factor rule: If \\(y=c \\cdot u(t)\\) then \\(\\frac{dy}{dt}=c \\cdot \\frac{du}{dt}\\). Sum rule: If \\(y=u(t) \\pm v(t)\\) then \\(\\frac{dy}{dt}=\\frac{du}{dt} \\pm \\frac{dv}{dt}\\). Product rule: If \\(y=u(t) \\cdot v(t)\\) then \\(\\frac{dy}{dt}=\\frac{du}{dt} \\cdot v+u \\cdot \\frac{dv}{dt}\\). Quotient rule: If \\(y=\\frac{u(t)}{v(t)}\\) then \\(\\frac{dy}{dt}=\\frac{\\left(\\frac{du}{dt} \\cdot v-u \\cdot \\frac{dv}{dt}\\right)}{v^2}\\). Chain rule: If \\(y=f[g(t)]\\) then \\(\\frac{dy}{dt}=\\frac{df[g]}{dg} \\cdot \\frac{dg}{dt}\\), i.e. “outer times inner derivative”. 2.4 Matrix algebra The following is based on Tabachnick and Fidell (2013). 2.4.1 Simple matrix operations Let \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) be matrices: \\[\\mathbf{A} = \\begin{pmatrix} a &amp; b &amp; c\\\\ d &amp; e &amp; f\\\\ g &amp; h &amp; i \\end{pmatrix} = \\begin{pmatrix} 3 &amp; 2 &amp; 4\\\\ 7 &amp; 5 &amp; 0\\\\ 1 &amp; 0 &amp; 8 \\end{pmatrix} \\] \\[\\mathbf{B} = \\begin{pmatrix} r &amp; s &amp; t\\\\ u &amp; v &amp; w\\\\ x &amp; y &amp; z \\end{pmatrix} = \\begin{pmatrix} 6 &amp; 1 &amp; 0\\\\ 2 &amp; 8 &amp; 7\\\\ 3 &amp; 4 &amp; 5 \\end{pmatrix} \\] Addition/subtraction of a constant to a matrix happens element-wise: \\[\\mathbf{A} + k = \\begin{pmatrix} a+k &amp; b+k &amp; c+k\\\\ d+k &amp; e+k &amp; f+k\\\\ g+k &amp; h+k &amp; i+k \\end{pmatrix} \\] \\[\\mathbf{A} - k = \\begin{pmatrix} a-k &amp; b-k &amp; c-k\\\\ d-k &amp; e-k &amp; f-k\\\\ g-k &amp; h-k &amp; i-k \\end{pmatrix} \\] Multiplication/division of a matrix by a constant also happens element-wise: \\[k \\cdot \\mathbf{A} = \\begin{pmatrix} k \\cdot a &amp; k \\cdot b &amp; k \\cdot c\\\\ k \\cdot d &amp; k \\cdot e &amp; k \\cdot f\\\\ k \\cdot g &amp; k \\cdot h &amp; k \\cdot i \\end{pmatrix} \\] \\[\\frac{1}{k} \\cdot \\mathbf{A} = \\begin{pmatrix} \\frac{1}{k} \\cdot a &amp; \\frac{1}{k} \\cdot b &amp; \\frac{1}{k} \\cdot c\\\\ \\frac{1}{k} \\cdot d &amp; \\frac{1}{k} \\cdot e &amp; \\frac{1}{k} \\cdot f\\\\ \\frac{1}{k} \\cdot g &amp; \\frac{1}{k} \\cdot h &amp; \\frac{1}{k} \\cdot i \\end{pmatrix} \\] Addition/subtraction of two matrices happens element-wise again: \\[\\mathbf{A} + \\mathbf{B} = \\begin{pmatrix} a+r &amp; b+s &amp; c+t\\\\ d+u &amp; e+v &amp; f+w\\\\ g+x &amp; h+y &amp; i+z \\end{pmatrix} \\] \\[\\mathbf{A} - \\mathbf{B} = \\begin{pmatrix} a-r &amp; b-s &amp; c-t\\\\ d-u &amp; e-v &amp; f-w\\\\ g-x &amp; h-y &amp; i-z \\end{pmatrix} \\] Finally, the so called transpose of a matrix refers to the mirroring of a matrix along its diagonal. Hence, the transpose of \\(\\mathbf{A}\\) is: \\[\\mathbf{A}&#39; = \\begin{pmatrix} a &amp; d &amp; g\\\\ b &amp; e &amp; h\\\\ c &amp; f &amp; i \\end{pmatrix} \\] 2.4.2 Matrix multiplication Now, the multiplication of two matrices is the only operation that is a bit complicated at first. It may be best to consider an example to work out the rules: \\[\\begin{eqnarray} \\mathbf{A} \\cdot \\mathbf{B}&amp;=&amp; \\begin{pmatrix} a &amp; b &amp; c\\\\ d &amp; e &amp; f\\\\ g &amp; h &amp; i \\end{pmatrix} \\cdot \\begin{pmatrix} r &amp; s &amp; t\\\\ u &amp; v &amp; w\\\\ x &amp; y &amp; z \\end{pmatrix}\\\\ &amp;=&amp;\\begin{pmatrix} a \\cdot r + b \\cdot u + c \\cdot x &amp; a \\cdot s + b \\cdot v + c \\cdot y &amp; a \\cdot t + b \\cdot w + c \\cdot z\\\\ d \\cdot r + e \\cdot u + f \\cdot x &amp; d \\cdot s + e \\cdot v + f \\cdot y &amp; d \\cdot t + e \\cdot w + f \\cdot z\\\\ g \\cdot r + h \\cdot u + i \\cdot x &amp; g \\cdot s + h \\cdot v + i \\cdot y &amp; g \\cdot t + h \\cdot w + i \\cdot z \\end{pmatrix}\\\\ &amp;=&amp;\\begin{pmatrix} 34 &amp; 35 &amp; 34\\\\ 52 &amp; 47 &amp; 35\\\\ 30 &amp; 33 &amp; 40 \\end{pmatrix} \\end{eqnarray}\\] The result of a matrix multiplication has as many rows as the 1st matrix and as many columns as the 2nd. For this to work, the number of columns of the 1st matrix must match the number of rows of the 2nd matrix. In our example, this does not matter as the matrices are square, i.e. they have as many rows as columns. To construct each cell of the results matrix, one row of the 1st matrix is combined with one column of the 2nd matrix. For cell (1,1) (top-left), for example, we combine the 1st row of matrix 1 (here \\(\\mathbf{A}\\)) and the 1st column of matrix 2 (here \\(\\mathbf{B}\\)). Moving to the right, for cell (1,2) (top-middle), we combine the 1st row of matrix 1 and the 2nd column of matrix 2. For cell (2,1) (middle-left), we combine the 2nd row of matrix 1 and the 1st column of matrix 2. And so on and so forth. The combination of the two respective vectors is the sum of the products of the vector elements paired in order. So for cell (1,1) in our example this is \\(a \\cdot r + b \\cdot u + c \\cdot x\\). Try and recreate the following example to get a feeling for the matrix multiplication rules. \\[\\begin{eqnarray} \\mathbf{B} \\cdot \\mathbf{A}&amp;=&amp; \\begin{pmatrix} r &amp; s &amp; t\\\\ u &amp; v &amp; w\\\\ x &amp; y &amp; z \\end{pmatrix} \\cdot \\begin{pmatrix} a &amp; b &amp; c\\\\ d &amp; e &amp; f\\\\ g &amp; h &amp; i \\end{pmatrix}\\\\ &amp;=&amp;\\begin{pmatrix} r \\cdot a + s \\cdot d + t \\cdot g &amp; r \\cdot b + s \\cdot e + t \\cdot h &amp; r \\cdot c + s \\cdot f + t \\cdot i\\\\ u \\cdot a + v \\cdot d + w \\cdot g &amp; u \\cdot b + v \\cdot e + w \\cdot h &amp; u \\cdot c + v \\cdot f + w \\cdot i\\\\ x \\cdot a + y \\cdot d + z \\cdot g &amp; x \\cdot b + y \\cdot e + z \\cdot h &amp; x \\cdot c + y \\cdot f + z \\cdot i \\end{pmatrix}\\\\ &amp;=&amp;\\begin{pmatrix} 25 &amp; 17 &amp; 24\\\\ 69 &amp; 44 &amp; 64\\\\ 42 &amp; 26 &amp; 52 \\end{pmatrix} \\end{eqnarray}\\] The point with this example is that \\(\\mathbf{A} \\cdot \\mathbf{B}\\) is not the same as \\(\\mathbf{B} \\cdot \\mathbf{A}\\). The order matters when multiplying matrices! Let’s consider two more example: \\[\\begin{eqnarray} \\mathbf{A} \\cdot \\mathbf{A}&amp;=&amp; \\begin{pmatrix} a &amp; b &amp; c\\\\ d &amp; e &amp; f\\\\ g &amp; h &amp; i \\end{pmatrix} \\cdot \\begin{pmatrix} a &amp; b &amp; c\\\\ d &amp; e &amp; f\\\\ g &amp; h &amp; i \\end{pmatrix}\\\\ &amp;=&amp;\\begin{pmatrix} a^2 + b \\cdot d + c \\cdot g &amp; a \\cdot b + b \\cdot e + c \\cdot h &amp; a \\cdot c + b \\cdot f + c \\cdot i\\\\ d \\cdot a + e \\cdot d + f \\cdot g &amp; d \\cdot b + e^2 + f \\cdot h &amp; d \\cdot c + e \\cdot f + f \\cdot i\\\\ g \\cdot a + h \\cdot d + i \\cdot g &amp; g \\cdot b + h \\cdot e + i \\cdot h &amp; g \\cdot c + h \\cdot f + i^2 \\end{pmatrix}\\\\ &amp;=&amp;\\begin{pmatrix} 27 &amp; 16 &amp; 44\\\\ 56 &amp; 39 &amp; 28\\\\ 11 &amp; 2 &amp; 68 \\end{pmatrix} \\end{eqnarray}\\] This matrix multiplied with itself, \\(\\mathbf{A} \\cdot \\mathbf{A}\\), is different to the same matrix multiplied with its transpose \\(\\mathbf{A} \\cdot \\mathbf{A}&#39;\\): \\[\\begin{eqnarray} \\mathbf{A} \\cdot \\mathbf{A}&#39;&amp;=&amp; \\begin{pmatrix} a &amp; b &amp; c\\\\ d &amp; e &amp; f\\\\ g &amp; h &amp; i \\end{pmatrix} \\cdot \\begin{pmatrix} a &amp; d &amp; g\\\\ b &amp; e &amp; h\\\\ c &amp; f &amp; i \\end{pmatrix}\\\\ &amp;=&amp;\\begin{pmatrix} a^2 + b^2 + c^2 &amp; a \\cdot d + b \\cdot e + c \\cdot f &amp; a \\cdot g + b \\cdot h + c \\cdot i\\\\ d \\cdot a + e \\cdot b + f \\cdot c &amp; d^2 + e^2 + f^2 &amp; d \\cdot g + e \\cdot h + f \\cdot i\\\\ g \\cdot a + h \\cdot b + i \\cdot c &amp; g \\cdot d + h \\cdot e + i \\cdot f &amp; g^2 + h^2 + i^2 \\end{pmatrix}\\\\ &amp;=&amp;\\begin{pmatrix} 29 &amp; 31 &amp; 35\\\\ 31 &amp; 74 &amp; 7\\\\ 35 &amp; 7 &amp; 65 \\end{pmatrix} \\end{eqnarray}\\] This last matrix is symmetrical, i.e. mirrored along its diagonal. Diagonal elements are so called sums of squares, off-diagonal elements are so called cross-products. This will be useful later on when working with variance-covariance matrices in Chapter 8. 2.4.3 Matrix division, inverse of a matrix, identity matrix Division of two matrices means multiplication of one matrix with the so called inverse of the other, here \\(\\mathbf{B}^{-1}\\): \\[\\frac{\\mathbf{A}}{\\mathbf{B}} = \\mathbf{A} \\cdot \\mathbf{B}^{-1}\\] The inverse of a matrix is different to the transpose. It is rather complicated to calculate, using in most cases numerical (and not analytical) techniques that we do not go into here. At the most general level, the inverse is found so that the following equation holds: \\[\\mathbf{A} \\cdot \\mathbf{A}^{-1} = \\mathbf{A}^{-1} \\cdot \\mathbf{A} = \\mathbf{I}\\] With \\(\\mathbf{I}\\) being the identity matrix, i.e. a matrix with diagonal elements 1 and off-diagonal elements 0: \\[\\mathbf{I} = \\begin{pmatrix} 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix} \\] 2.5 Exercises Exercise 1 Apply the rules in the script to the following equation to get rid of the product operator, and then simplify the resultant equation as much as you can: \\[\\begin{equation} \\prod_{i=1}^{n}\\frac{1}{\\sigma \\cdot \\sqrt{2 \\cdot \\pi}} \\cdot \\exp\\left(\\frac{\\left(y_i - \\beta_0 - \\beta_1 \\cdot x_i\\right)^2}{-2 \\cdot \\sigma^2}\\right)= \\tag{2.23} \\end{equation}\\] Exercise 2 Apply the differentiation rules in the script to take the derivative of the following equation: \\[\\begin{equation} \\frac{d\\sum_{i=1}^{n}\\left(y_i - \\beta_0 - \\beta_1 \\cdot x_i\\right)^2}{d\\beta_0}= \\tag{2.24} \\end{equation}\\] Exercise 3 Consider the following vectors and matrix: \\(y = \\begin{pmatrix} y_1\\\\ y_2\\\\ y_3 \\end{pmatrix}\\), \\(\\beta = \\begin{pmatrix} \\beta_0\\\\ \\beta_1\\\\ \\beta_2\\\\ \\beta_3 \\end{pmatrix}\\), \\(\\epsilon = \\begin{pmatrix} \\epsilon_1\\\\ \\epsilon_2\\\\ \\epsilon_3 \\end{pmatrix}\\) and \\(\\mathbf{X} = \\begin{pmatrix} 1 &amp; x_{11} &amp; x_{12} &amp; x_{13}\\\\ 1 &amp; x_{21} &amp; x_{22} &amp; x_{23}\\\\ 1 &amp; x_{31} &amp; x_{32} &amp; x_{33} \\end{pmatrix}\\). Now solve the following equation using matrix algebra: \\[y = \\mathbf{X} \\cdot \\beta + \\epsilon = \\] References "],
["linreg.html", "Chapter 3 Linear regression 3.1 Motivation 3.2 The linear model 3.3 Description versus prediction 3.4 Linear Regression 3.5 Significance of regression 3.6 Confidence in parameter estimates 3.7 Goodness of fit", " Chapter 3 Linear regression 3.1 Motivation The questions we wish to answer with linear regression are of the kind depicted in Figure 1.1: What drives spatial variation in annual average precipitation and annual average temperature? In the case of precipitation the drivers seem to be continentality and elevation, while temperature appears to be dominantly controlled by elevation only. Linear regression examines this question by modelling a response variable against one or more predictor variables, while the relationship between the two is linear in its parameters. 3.2 The linear model The most general from of a linear model is: \\[\\begin{equation} y = \\beta_0 + \\sum_{j=1}^{p}\\beta_j \\cdot x_j + \\epsilon \\tag{3.1} \\end{equation}\\] In this equation, \\(y\\) is the response variable (also called dependent or output variable), \\(x_j\\) are the predictor variables (also called independent, explanatory, input variables or covariates), \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) are the parameters and \\(\\epsilon\\) is the residual, i.e. that part of the response which remains unexplained by the predictors. In the case of one predictor, which has come to be known as linear regression, the linear model is: \\[\\begin{equation} y = \\beta_0 + \\beta_1 \\cdot x + \\epsilon \\tag{3.2} \\end{equation}\\] It can be visualised as a line, with \\(\\beta_0\\) being the intercept, where the line intersects the vertical axis \\((x=0)\\), and \\(\\beta_1\\) being the slope of the line (Figure 3.1). Note, the point \\(\\left(\\bar{x},\\bar{y}\\right)\\), the centroid of the data, always lies on the line. Figure 3.1: Linear model with one predictor variable (linear regression). Linear means linear in terms of the model parameters, not (necessarily) in terms of the predictor variables. With this in mind, consider the following five models. Which are linear models, which are non-linear models? (Q1)1 \\[\\begin{equation} y = \\beta_0 + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 + \\epsilon \\tag{3.3} \\end{equation}\\] \\[\\begin{equation} y = \\beta_0 + \\beta_1 \\cdot x_1^{\\beta_2} + \\epsilon \\tag{3.4} \\end{equation}\\] \\[\\begin{equation} y = \\beta_0 + \\beta_1 \\cdot x_1^3 + \\beta_2 \\cdot x_1 \\cdot x_2 + \\epsilon \\tag{3.5} \\end{equation}\\] \\[\\begin{equation} y = \\beta_0 + \\exp(\\beta_1 \\cdot x_1) + \\beta_2 \\cdot x_2 + \\epsilon \\tag{3.6} \\end{equation}\\] \\[\\begin{equation} y = \\beta_0 + \\beta_1 \\cdot \\log x_1 + \\beta_2 \\cdot x_2 + \\epsilon \\tag{3.7} \\end{equation}\\] We can also write the linear model equation with the data points explicitly indexed by \\(i\\) for \\(i=1, 2, \\ldots, n\\). We have omitted the index previously for ease of reading: \\[\\begin{equation} y_i = \\beta_0 + \\sum_{j=1}^{p}\\beta_j \\cdot x_{ij} + \\epsilon_i \\tag{3.8} \\end{equation}\\] These data points could be repeat measurements in time or in space. We can also write the model more compactly in a matrix format: \\[\\begin{equation} y = \\mathbf{X} \\cdot \\beta + \\epsilon \\tag{3.9} \\end{equation}\\] With \\(y = \\begin{pmatrix} y_1\\\\ y_2\\\\ y_3 \\end{pmatrix}\\), \\(\\beta = \\begin{pmatrix} \\beta_0\\\\ \\beta_1\\\\ \\beta_2\\\\ \\beta_3 \\end{pmatrix}\\), \\(\\epsilon = \\begin{pmatrix} \\epsilon_1\\\\ \\epsilon_2\\\\ \\epsilon_3 \\end{pmatrix}\\) and \\(\\mathbf{X} = \\begin{pmatrix} 1 &amp; x_{11} &amp; x_{12} &amp; x_{13}\\\\ 1 &amp; x_{21} &amp; x_{22} &amp; x_{23}\\\\ 1 &amp; x_{31} &amp; x_{32} &amp; x_{33} \\end{pmatrix}\\), the latter being the design matrix which summarises the predictor data. When we talk about the linear model, the response variable is always continuous, while the predictor variables can be continuous, categorical or mixed. In principle, each of these variants can be treated mathematically in the same way, e.g. all can be analysed using the lm() function in R. However, historically different names have been established for these variants, which are worth mentioning here to avoid confusion (Tables 3.1 and 3.2). Table 3.1: Historical names for the variants of the linear model, depending on whether the predictors are continuous, categorical or mixed. The response is always continuous. Continuouspredictors Categoricalpredictors Mixedpredictors Regression Analysis of variance(ANOVA) Analysis of covariance(ANCOVA) Table 3.2: Historical names for the regression, depending on whether we have one or more predictors and one or more responses. 1 predictor variable &gt;1 predictor variables 1 response variable Regression Multiple regression &gt;1 response variables Multivariate regression Multivariate multiple regression 3.3 Description versus prediction The primary purpose of regression analysis is the description (or explanation) of the data in terms of a general relationship pertaining to the population that these data are sampled from. Being a property of the population, this relationship should then also allow us to make predictions, but we need to be careful. Consider the relationship between year and world record time for the men’s mile depicted in Figure 3.2. When the predictor is time, as shown here, regression becomes a form of trend analysis, in this case of how the record time in the male competition decreased over the years. We will talk about the R code and output further below, here we’re just interested in the linear predictions. # load mile data from remote repository mile &lt;- read.csv(&quot;https://raw.githubusercontent.com/avehtari/ROS-Examples/master/Mile/data/mile.csv&quot;, header=TRUE) # fit linear model to data from 1st half of 20th century fit1 &lt;- lm(seconds ~ year, data = mile[mile$year&lt;1950,]) # extract information about parameter estimates coef(summary(fit1)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 912.2339944 67.90139506 13.434687 3.614623e-08 ## year -0.3438721 0.03509494 -9.798337 9.058700e-07 # fit linear model to complete dataset fit2 &lt;- lm(seconds ~ year, data = mile) coef(summary(fit2)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1006.8760057 21.5319332 46.76199 1.360809e-29 ## year -0.3930488 0.0109992 -35.73431 3.779773e-26 # plot fit for 1st half of 20th century plot(mile$year[mile$year&lt;1950], mile$seconds[mile$year&lt;1950], xlim = c(1900, 2000), ylim = c(200, 260), pch = 19, type = &#39;p&#39;, xlab = &quot;Year&quot;, ylab = &quot;World record, men&#39;s mile (seconds)&quot;) abline(coef(fit1), lwd = 3, col = &quot;red&quot;) # plot extrapolation to 2nd half of 20th century plot(mile$year, mile$seconds, xlim = c(1900, 2000), ylim = c(200, 260), pch = 19, type = &#39;p&#39;, xlab = &quot;Year&quot;, ylab = &quot;World record, men&#39;s mile (seconds)&quot;) abline(coef(fit1), lwd = 3, col = &quot;red&quot;) # plot all-data fit until 2050 plot(mile$year, mile$seconds, xlim = c(1900, 2050), ylim = c(200, 260), pch = 19, type = &#39;p&#39;, xlab = &quot;Year&quot;, ylab = &quot;World record, men&#39;s mile (seconds)&quot;) abline(coef(fit2), lwd = 3, col = &quot;red&quot;) Figure 3.2: Left: Trend of the world record for the men`s mile over the first half of the 20th century (description). Centre: Extrapolation of this trend over the 2nd half of the 20th century (prediction). Right: Extrapolation of the overall trend until the year 2050 (longer prediction). After: Wainer (2009) The world record for the men`s mile improved linearly over the first half of the 20th century (Figure 3.2, left). This trend provides a remarkably accurate fit for the second half of the century as well (Figure 3.2, centre). However, for how long can the world record continue to improve at the same rate (Figure 3.2, right)? This example clearly shows that the scope for prediction by regression lies within certain bounds, while highlighting the limits of these simple models for making distant predictions (e.g. in time and space). In the case of the world record we would expect the rate of improvement to decline with time, i.e. the world record to level off, which calls for a non-linear model. 3.4 Linear Regression Typically, regression problems are solved, i.e. the lines in Figures 3.1 and 3.2 are fitted to the data, by minimising the Sum of Squared Errors (SSE) between the regression line and the data points. This method has become known as Least Squares. Graphically, it means that in Figure 3.1 we try different lines with different intercepts \\(\\left(\\beta_0\\right)\\) and slopes \\(\\left(\\beta_1\\right)\\) and ultimately choose the one where the sum over all vertical distances \\(\\epsilon_i\\) squared is smallest. Mathematically, SSE is defined as: \\[\\begin{equation} SSE=\\sum_{i=1}^{n}\\left(\\epsilon_i\\right)^2=\\sum_{i=1}^{n}\\left(y_i-\\left(\\beta_0+\\beta_1 \\cdot x_i\\right)\\right)^2 \\tag{3.10} \\end{equation}\\] The terms \\(\\epsilon_i=y_i-\\left(\\beta_0+\\beta_1 \\cdot x_i\\right)\\) are called the residuals, i.e. that part of the variation in the data which the linear model cannot explain. In the case of linear regression, SSE can be minimised analytically, which is not the case for non-linear models, for example. Analytically, we find the minimum of SSE where its partial derivatives with respect to the two model parameters are both zero (compare Chapter 2): \\(\\frac{\\partial SSE}{\\partial \\beta_0}=0\\) and \\(\\frac{\\partial SSE}{\\partial \\beta_1}=0\\). Using the definition of SEE of Equation (3.10) we thus begin with a system of two differential equations: \\[\\begin{equation} \\frac{\\partial SSE}{\\partial \\beta_0}=-2 \\cdot \\sum_{i=1}^{n}\\left(y_i-\\beta_0-\\beta_1 \\cdot x_i\\right)=0 \\tag{3.11} \\end{equation}\\] \\[\\begin{equation} \\frac{\\partial SSE}{\\partial \\beta_1}=-2 \\cdot \\sum_{i=1}^{n}x_i \\cdot \\left(y_i-\\beta_0-\\beta_1 \\cdot x_i\\right)=0 \\tag{3.12} \\end{equation}\\] We have already calculated these derivatives in an exercise in Chapter 2 using the sum rule and the chain rule in particular. Since Equations (3.11) and (3.12) form a system of two differential equations with two unknowns (\\(\\beta_0\\) and \\(\\beta_1\\); the data points \\(x_i\\) and \\(y_i\\) are known) we can solve it exactly. First, we solve Equation (3.11) for \\(\\beta_0\\) (after dividing by -2): \\[\\begin{equation} \\sum_{i=1}^{n}y_i-n \\cdot \\beta_0-\\beta_1 \\cdot \\sum_{i=1}^{n}x_i=0 \\tag{3.13} \\end{equation}\\] \\[\\begin{equation} n \\cdot \\hat\\beta_0=\\sum_{i=1}^{n}y_i-\\hat\\beta_1 \\cdot \\sum_{i=1}^{n}x_i \\tag{3.14} \\end{equation}\\] \\[\\begin{equation} \\hat\\beta_0=\\bar{y}-\\hat\\beta_1 \\cdot \\bar{x} \\tag{3.15} \\end{equation}\\] Note, at some point we have renamed \\(\\beta_0\\) to \\(\\hat\\beta_0\\) and \\(\\beta_1\\) to \\(\\hat\\beta_1\\) to denote these as estimates. The parameter notation up to now has been general but as we approach actual numerical values for the data at hand we are using the “hat” symbol to signify that we are now calculating estimates of those general parameters for a given dataset. Second, we insert Equation (3.15) into Equation (3.12) (again after dividing by -2 and rearranging): \\[\\begin{equation} \\sum_{i=1}^{n}\\left(x_i \\cdot y_i-\\beta_0 \\cdot x_i-\\beta_1 \\cdot x_i^2\\right)=0 \\tag{3.16} \\end{equation}\\] \\[\\begin{equation} \\sum_{i=1}^{n}\\left(x_i \\cdot y_i-\\bar{y} \\cdot x_i+\\hat\\beta_1 \\cdot \\bar{x} \\cdot x_i-\\hat\\beta_1 \\cdot x_i^2\\right)=0 \\tag{3.17} \\end{equation}\\] Third, we solve Equation (3.17) for \\(\\beta_1\\): \\[\\begin{equation} \\sum_{i=1}^{n}\\left(x_i \\cdot y_i-\\bar{y} \\cdot x_i\\right)-\\hat\\beta_1 \\cdot \\sum_{i=1}^{n}\\left(x_i^2-\\bar{x} \\cdot x_i\\right)=0 \\tag{3.18} \\end{equation}\\] \\[\\begin{equation} \\hat\\beta_1=\\frac{\\sum_{i=1}^{n}\\left(x_i \\cdot y_i-\\bar{y} \\cdot x_i\\right)}{\\sum_{i=1}^{n}\\left(x_i^2-\\bar{x} \\cdot x_i\\right)} \\tag{3.19} \\end{equation}\\] Via a series of steps that I skip here, we arrive at: \\[\\begin{equation} \\hat\\beta_1=\\frac{SSXY}{SSX} \\tag{3.20} \\end{equation}\\] Where \\(SSX=\\sum_{i=1}^{n}\\left(x_i-\\bar{x}\\right)^2\\) and \\(SSXY=\\sum_{i=1}^{n}\\left(x_i-\\bar{x}\\right) \\cdot \\left(y_i-\\bar{y}\\right)\\). Note, analogously \\(SSY=\\sum_{i=1}^{n}\\left(y_i-\\bar{y}\\right)^2\\). Equation (3.20) is an exact solution for \\(\\hat\\beta_1\\). We then insert Equation (3.20) back into Equation (3.10) and have an exact solution for \\(\\hat\\beta_0\\). 3.5 Significance of regression Having estimates for the regression parameters we need to ask ourselves whether these estimates are statistically significant or could have arisen by chance from the (assumed) random process of sampling the data. We do this via Analysis of Variance (ANOVA), which begins by constructing the ANOVA table (Table 3.3). This is often done in the background in software like R and not actually looked at that much. Table 3.3: ANOVA table for linear regression. Source Sum ofsquares Degrees of freedom \\((df)\\) Mean squares F statistic \\(\\left(F_s\\right)\\) \\(\\Pr\\left(Z\\geq F_s\\right)\\) Regression \\(SSR=\\\\SSY-SSE\\) \\(1\\) \\(\\frac{SSR}{df_{SSR}}\\) \\(\\frac{\\frac{SSR}{df_{SSR}}}{s^2}\\) \\(1-F\\left(F_s,1,n-2\\right)\\) Error \\(SSE\\) \\(n-2\\) \\(\\frac{SSE}{df_{SSE}}=s^2\\) Total \\(SSY\\) \\(n-1\\) In the second column of Table 3.3, \\(SSY=\\sum_{i=1}^{n}\\left(y_i-\\bar{y}\\right)^2\\) is a measure of the total variance of the data, i.e. how much the data points are varying around the overall mean (Figure 3.3, left). \\(SSE=\\sum_{i=1}^{n}\\left(\\epsilon_i\\right)^2=\\sum_{i=1}^{n}\\left(y_i-\\left(\\beta_0+\\beta_1 \\cdot x_i\\right)\\right)^2\\) is a measure of the error variance, i.e. how much the data points are varying around the regression line (Figure 3.3, right). This is the variance not explained by the model. \\(SSR=SSY-SSE\\) then is a measure of the variance explained by the model. Figure 3.3: Variation of the data points around the mean, summarised by \\(SSY\\) (left), and around the regression line, summarised by \\(SSE\\) (right). The third column of Table 3.3 lists the so called degrees of freedom of the three variance terms, which can be understood as the number of free parameters for the respective term that is controlled by the (assumed) random process of sampling the data. It is the number of possibilities for the chance process to unfold. \\(SSY\\) requires one parameter \\(\\left(\\bar{y}\\right)\\) to be calculated from the data (see above). Hence the degrees of freedom are \\(n-1\\); if I know \\(\\bar{y}\\) then there are \\(n-1\\) data points left that can be generated by chance, the nth one I can calculate from all the others and \\(\\bar{y}\\). \\(SSE\\), in turn, requires two parameters (\\(\\beta_0\\) and \\(\\beta_1\\)) to be calculated from the data (Equations (3.15) and (3.20)). Hence the degrees of freedom are \\(n-2\\). The degrees of freedom of \\(SSR\\) then are just the difference between the former two; \\(df_{SSR}=df_{SSY}-df_{SSE}=1\\). The degrees of freedom are used to normalise the variance terms in the fourth column of Table 3.3, where \\(s^2\\) is called the error variance. In the fifth column of Table 3.3 we find the ratio of two variances; regression variance over error variance. Naturally, for a significant regression we want the regression variance (explained by the model) to be much larger than the error variance (unexplained by the model). This is an F-Test problem, testing whether the variance explained by the model is significantly different from the variance unexplained by the model. The ratio of the two variances serves as the F statistic \\(\\left(F_s\\right)\\). The sixth column of Table 3.3 then shows the p-value of the F-Test, i.e. the probability of getting \\(F_s\\) or a larger value (i.e. an even better model) by chance if the Null hypothesis \\(\\left(H_0\\right)\\)) is true. \\(H_0\\) here is that the two variances are equal. It can be shown mathematically that \\(F_s\\) follows an F-distribution with parameters \\(1\\) and \\(n-2\\) under the Null hypothesis (Figure 3.4). The red line in Figure 3.4 marks a particular value of \\(F_s\\) (between 10 and 11) and the corresponding value of the cumulative distribution function of the F-distribution \\(\\left(F\\left(F_s,1,n-2\\right)\\right)\\). The p-value is \\(\\Pr\\left(Z\\geq F_s\\right)=1-F\\left(F_s,1,n-2\\right)\\), i.e. the probability of getting this variance ratio or a greater one by chance (due to the random sampling process) even if the two variances are actually equal. Here this value is very small and hence we conclude that the regression is significant. Figure 3.4: Cumulative distribution function (CDF) of the F-distribution of the F statistic \\(\\left(F_s\\right)\\), with a particular value and corresponding value of the CDF marked in red. The correct interpretation of the p-value is a bit tricky. In the words of philosopher of science Ian Hacking (2001), if we have a p-value of say 0.01 this means “either the Null hypothesis is true, in which case something unusual happened by chance (probability 1%), or the Null hypothesis is false.” This means, strictly speaking, the p-value is not the probability of the Null hypothesis being true; it is the probability of the data to come about if the Null hypothesis were true. If this is a very low probability then we think this tells us something about the Null hypothesis (that perhaps we should reject it), but in a roundabout way. Note, in the case of linear regression, the Null model is \\(\\beta_1=0\\), i.e. \\(y=\\beta_0\\) with \\(\\hat\\beta_0=\\bar{y}\\), which means the overall mean is the best model summarising the data (Figure 3.3, left). 3.6 Confidence in parameter estimates Having established the statistical significance of the regression, we should look at the uncertainty around the parameter estimates. In classic linear regression this uncertainty is conceptualised as arising purely from the random sampling process; the data at hand are just one possibility of many, and in each alternative case the parameter estimates would have turned out slightly different. The linear model itself is assumed to be correct. The first step in establishing how confident we should be that the parameter estimates are correct is the calculation of standard errors. For \\(\\hat\\beta_0\\) this is (derivation not shown here): \\[\\begin{equation} s_{\\hat\\beta_0}=\\sqrt{\\frac{\\sum_{i=1}^{n}x_i^2}{n} \\cdot \\frac{s^2}{SSX}} \\tag{3.21} \\end{equation}\\] Breaking down this formula into its individual parts, we can see that the more data points \\(n\\) we have, the smaller the standard error, i.e. the more confidence we have in the estimate. Also, the larger the variation in \\(x\\) \\((SSX)\\) the smaller the standard error. Both effects make intuitive sense: the more data points we have and the more possibilities for \\(x\\) we have covered, the more we can be confident that we have not missed much in our random sample. Conversely, the larger the error variance \\(s^2\\), i.e. the smaller the explanatory power of our model, the larger the standard error. And, the more \\(x\\) data points we have away from zero, i.e. the greater \\(\\sum_{i=1}^{n}x_i^2\\), the smaller our confidence in the intercept (where \\(x=0\\)) and hence the standard error increases. The standard error for \\(\\hat\\beta_1\\) is: \\[\\begin{equation} s_{\\hat\\beta_1}=\\sqrt{\\frac{s^2}{SSX}} \\tag{3.22} \\end{equation}\\] The same interpretation applies, except there is no influence of the magnitude of the \\(x\\) data points. We can also establish a standard error for new predictions \\(\\hat y\\) for given new predictor values \\(\\hat x\\): \\[\\begin{equation} s_{\\hat y}=\\sqrt{s^2 \\cdot \\left(\\frac{1}{n}+\\frac{\\left(\\hat x-\\bar x\\right)^2}{SSX}\\right)} \\tag{3.23} \\end{equation}\\] The same interpretation applies again, except there now is an added term \\(\\left(\\hat x-\\bar x\\right)^2\\) which means the further the new \\(x\\) value is away from the centre of the original data (the training or calibration data) the greater the standard error of the new prediction, i.e. the lower the confidence in it being correct. Note, the formulae for the standard errors arise from the fundamental assumptions of linear regression, which will be covered below. This can be shown mathematically but is omitted here. From the standard errors we can calculate confidence intervals for the parameter estimates as follows: \\[\\begin{equation} \\Pr\\left(\\hat\\beta_0-t_{n-2;0.975} \\cdot s_{\\hat\\beta_0}\\leq \\beta_0\\leq \\hat\\beta_0+t_{n-2;0.975} \\cdot s_{\\hat\\beta_0}\\right)=0.95 \\tag{3.24} \\end{equation}\\] The symbol \\(\\Pr(\\cdot)\\) means probability. The symbol \\(t_{n-2;0.975}\\) stands for the 0.975-percentile of the t-distribution with \\(n-2\\) degrees of freedom. Equation (3.24) is the central 95% confidence interval, which is defined as the bounds in which the true parameter, here \\(\\beta_0\\), lies with a probability of 0.95. We can write the interval like this: \\[\\begin{equation} CI=\\left[\\hat\\beta_0-t_{n-2;0.975} \\cdot s_{\\hat\\beta_0};\\hat\\beta_0+t_{n-2;0.975} \\cdot s_{\\hat\\beta_0}\\right] \\tag{3.25} \\end{equation}\\] As can be seen, the confidence interval \\(CI\\) is symmetric around the parameter estimate \\(\\hat\\beta_0\\) and arises from a t-distribution with parameter \\(n-2\\) whose width is modulated by the standard error \\(s_{\\hat\\beta_0}\\). Note, the width of the t-distribution is also controlled by sample size, becoming narrower with increasing \\(n\\). The same formulae apply for \\(\\beta_1\\) and \\(y\\): \\[\\begin{equation} \\Pr\\left(\\hat\\beta_1-t_{n-2;0.975} \\cdot s_{\\hat\\beta_1}\\leq \\beta_1\\leq \\hat\\beta_1+t_{n-2;0.975} \\cdot s_{\\hat\\beta_1}\\right)=0.95 \\tag{3.26} \\end{equation}\\] \\[\\begin{equation} \\Pr\\left(\\hat y-t_{n-2;0.975} \\cdot s_{\\hat y}\\leq y\\leq \\hat y+t_{n-2;0.975} \\cdot s_{\\hat y}\\right)=0.95 \\tag{3.27} \\end{equation}\\] As with the p-values, we need to be clear about the meaning of probability here, which in classic statistics is predicated on the repeated sampling principle. The meaning of the 95% confidence interval then is that in an assumed infinite number of regression experiments the 95% confidence interval captures the true parameter value in 95% of the cases. Again, this is not a probability of the true parameter value lying within the confidence interval for any one experiment! The formulae for the confidence intervals (Equations (3.24), (3.26) and (3.27)) arise from the fundamental assumptions of linear regression; the residuals are independent identically distributed (iid) according to a normal distribution and the linear model is correct. Then it can be shown mathematically that \\(\\frac{\\hat\\beta_0-\\beta_0}{s_{\\hat\\beta_0}}\\), \\(\\frac{\\hat\\beta_1-\\beta_1}{s_{\\hat\\beta_1}}\\) and \\(\\frac{\\hat y-y}{s_{\\hat y}}\\) are \\(t_{n-2}\\)-distributed (t-distribution with \\(n-2\\) degrees of freedom). Since the central 95% confidence interval of an arbitrary \\(t_{n-2}\\)-distributed random variable \\(Z\\) is \\(\\Pr\\left(-t_{n-2;0.975}\\leq Z\\leq t_{n-2;0.975}\\right)=0.95\\) (Figure 3.5), we can substitute any of the aforementioned three terms for \\(Z\\) and rearrange to arrive at Equations (3.24), (3.26) and (3.27). Figure 3.5: Left: Probability density function (PDF) of a t-distributed random variable \\(Z\\), with central 95% confidence interval marked in red. 95% of the PDF lies between the two bounds, 2.5% lies left of the lower bound and 2.5% right of the upper bound. Right: Cumulative distribution function (CDF) of the same t-distributed random variable \\(Z\\). The upper bound of the 95% confidence interval is defined as \\(t_{n-2;0.975}\\), i.e. the 0.975-percentile of the distribution, while the lower bound is defined as \\(t_{n-2;0.025}\\), which is equivalent to \\(-t_{n-2;0.975}\\) due to the symmetry of the distribution. The t-distribution property of the parameter estimates can further be exploited to test each parameter estimate separately for its statistical significance. This becomes especially important for multiple regression problems where we have more than one possible predictor, not all of which will have a statistically significant effect. The significance of the parameter estimates is determined via a t-test. The Null hypothesis is that the true parameters are zero, i.e. the parameter estimates are not significant: \\[\\begin{equation} H_0:\\beta_0=0 \\tag{3.28} \\end{equation}\\] \\[\\begin{equation} H_0:\\beta_1=0 \\tag{3.29} \\end{equation}\\] This hypothesis is tested against the alternative hypothesis that the true parameters are different from zero, i.e. the parameter estimates are significant: \\[\\begin{equation} H_1:\\beta_0\\neq 0 \\tag{3.30} \\end{equation}\\] \\[\\begin{equation} H_1:\\beta_1\\neq 0 \\tag{3.31} \\end{equation}\\] The test statistics are: \\[\\begin{equation} t_s=\\frac{\\hat\\beta_0-0}{s_{\\hat\\beta_0}}\\sim t_{n-2} \\tag{3.32} \\end{equation}\\] \\[\\begin{equation} t_s=\\frac{\\hat\\beta_1-0}{s_{\\hat\\beta_1}}\\sim t_{n-2} \\tag{3.33} \\end{equation}\\] The “tilde” symbol \\((\\sim)\\) means the test statistics follow a certain distribution, here the t-distribution. This arises again from the regression assumptions noted above. The assumptions are the same as for the common t-test of means, except in the case of linear regression the residuals are assumed iid normal while in the case of means the actual data points \\(y\\) are assumed iid normal. Analogous to the common 2-sided t-test, the p-value is defined as: \\[\\begin{equation} 2 \\cdot \\Pr\\left(t&gt;|t_s|\\right)=2 \\cdot \\left(1-F_t\\left(|t_s|\\right)\\right) \\tag{3.34} \\end{equation}\\] The symbol \\(F_t\\left(|t_s|\\right)\\) signifies the value of the CDF of the t-distribution at the location of the absolute value of the test statistic (\\(|t_s|\\), Figure 3.6). With a significance level of say \\(\\alpha=0.05\\) we arrive at critical values of the test statistic \\(t_c=t_{n-2;0.975}\\) and \\(-t_c\\) beyond which we reject the Null hypothesis and call the parameter estimates significant (Figure 3.6). Figure 3.6: Schematic of the t-test of significance of parameter estimates. The test statistic follows a t-distribution under the Null hypothesis. The actual value of the test statistic \\(t_s\\) is marked in blue and mirrored at zero for the 2-sided test. The critical value of the test statistic \\(t_c\\), which we get from a significance level of \\(\\alpha=0.05\\), is marked in red; this too is mirrored for the 2-sided test. We reject the Null hypothesis if \\(|t_s|&gt;t_c\\), i.e. for values of \\(t_s\\) below \\(-t_c\\) and above \\(t_c\\), and then call this parameter estimate significant. We keep the Null hypothesis if \\(|t_s|\\leq t_c\\), i.e. for values of \\(t_s\\) between \\(-t_c\\) and \\(t_c\\), and then call this parameter estimate insignificant (for now). In the example shown the parameter estimate is insignificant. 3.7 Goodness of fit The final step in regression analysis is assessing the goodness of fit of the linear model. In the first instance this may be done through the coefficient of determination \\(\\left(r^2\\right)\\), which is defined as the proportion of variation (in y-direction) that is explained by the model: \\[\\begin{equation} r^2=\\frac{SSY-SSE}{SSY}=1-\\frac{SSE}{SSY} \\tag{3.35} \\end{equation}\\] As can be seen, when the model fails to explain more variation than the total variation around the mean, i.e. \\(SSE=SSY\\), then \\(r^2=0\\). Conversely, when the model fits the data perfectly, i.e. \\(SSE=0\\), then \\(r^2=1\\). Any value in between signifies varying levels of goodness of fit. This can be visualised again with Figure 3.3, with the left panel signifying \\(SSY\\) and the right panel \\(SSE\\). When it comes to comparing models of varying complexity (i.e. with more or less parameters) using \\(r^2\\), then penalising the metric by the number of model parameters makes sense since more complex models (more parameters) automatically lead to better fits, simply due to the greater degrees of freedom that more complex models have for fitting the data. This leads to the adjusted \\(r^2\\): \\[\\begin{equation} \\bar r^2=1-\\frac{\\frac{SSE}{df_{SSE}}}{\\frac{SSY}{df_{SSY}}}=1-\\frac{SSE}{SSY} \\cdot \\frac{df_{SSY}}{df_{SSE}} \\tag{3.36} \\end{equation}\\] The coefficient of determination alone, however, is insufficient for assessing goodness of fit. Consider the four datasets depicted in Figure 3.7, which together form the Anscombe (1973) dataset. # plot 4 individual datasets plot(anscombe$x1, anscombe$y1, xlim = c(0, 20), ylim = c(0, 14), pch = 19, type = &#39;p&#39;) plot(anscombe$x2, anscombe$y2, xlim = c(0, 20), ylim = c(0, 14), pch = 19, type = &#39;p&#39;) plot(anscombe$x3, anscombe$y3, xlim = c(0, 20), ylim = c(0, 14), pch = 19, type = &#39;p&#39;) plot(anscombe$x4, anscombe$y4, xlim = c(0, 20), ylim = c(0, 14), pch = 19, type = &#39;p&#39;) Figure 3.7: The four Anscombe (1973) datasets. The individual datasets have purposely been constructed to yield virtually the same parameter estimates and coefficients of determination, despite wildly different relationships between \\(x\\) and \\(y\\) (Figure 3.8): # perform individual regressions fit1 &lt;- lm(y1 ~ x1, data = anscombe) fit2 &lt;- lm(y2 ~ x2, data = anscombe) fit3 &lt;- lm(y3 ~ x3, data = anscombe) fit4 &lt;- lm(y4 ~ x4, data = anscombe) # extract information about parameter estimates and R2 coef(summary(fit1)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0000909 1.1247468 2.667348 0.025734051 ## x1 0.5000909 0.1179055 4.241455 0.002169629 summary(fit1)$r.squared ## [1] 0.6665425 coef(summary(fit2)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.000909 1.1253024 2.666758 0.025758941 ## x2 0.500000 0.1179637 4.238590 0.002178816 summary(fit2)$r.squared ## [1] 0.666242 coef(summary(fit3)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0024545 1.1244812 2.670080 0.025619109 ## x3 0.4997273 0.1178777 4.239372 0.002176305 summary(fit3)$r.squared ## [1] 0.666324 coef(summary(fit4)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0017273 1.1239211 2.670763 0.025590425 ## x4 0.4999091 0.1178189 4.243028 0.002164602 summary(fit4)$r.squared ## [1] 0.6667073 In these summary tables, “(Intercept)” stands for \\(\\beta_0\\), while “x1” to “x4” stand for \\(\\beta_1\\). The column “Estimate” gives \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\), the column “Std. Error” gives \\(s_{\\hat\\beta_0}\\) and \\(s_{\\hat\\beta_1}\\), the column “t value” gives the individual \\(t_s\\) and the column “Pr(&gt;|t|)” gives the respective p-value. # plot individual datasets with regression lines plot(anscombe$x1, anscombe$y1, xlim = c(0, 20), ylim = c(0, 14), pch = 19, type = &#39;p&#39;) abline(coef(fit1), lwd = 3, col = &quot;red&quot;) plot(anscombe$x2, anscombe$y2, xlim = c(0, 20), ylim = c(0, 14), pch = 19, type = &#39;p&#39;) abline(coef(fit2), lwd = 3, col = &quot;red&quot;) plot(anscombe$x3, anscombe$y3, xlim = c(0, 20), ylim = c(0, 14), pch = 19, type = &#39;p&#39;) abline(coef(fit3), lwd = 3, col = &quot;red&quot;) plot(anscombe$x4, anscombe$y4, xlim = c(0, 20), ylim = c(0, 14), pch = 19, type = &#39;p&#39;) abline(coef(fit4), lwd = 3, col = &quot;red&quot;) Figure 3.8: Regression analysis of the four Anscombe (1973) datasets, yielding virtually the same parameter estimates and coefficients of determination (see above), despite wildly different relationships between \\(x\\) and \\(y\\). The coefficient of determination is insensitive to these and similar systematic deviations from the regression line. But we can detect these deficiencies of the model by looking at plots like Figure 3.8, and more generally by performing residual diagnostics that check model assumptions. The fundamental assumptions of linear regression are: The residuals are independent, in which case there will be no serial correlation in the residual plot – this can be tested using the Durbin-Watson test The residuals are normally distributed – this can be visually assessed using the quantile-quantile plot (QQ plot) and the residual histogram, and can be tested using the Kolmogorov-Smirnov test and the Shapiro-Wilk test The variance is the same across residuals, i.e. residuals are homoscedastic, in which case there is no “fanning out” of the residuals If these assumptions are not met then we can resort to data transformation, weighted regression or Generalised Linear Models (this is the preferred option), which we will cover in chapter 7. A first useful diagnostic plot is of the residuals in series, i.e. by index \\(i\\), to see if there is a pattern due to the data collection process (Figure 3.9). For the Anscombe dataset, this detects the nonlinearity in dataset 2 (top-right) and the outlier in dataset 3 (bottom-left), compare Figure 3.8. # plot residuals against index plot(residuals(fit1), xlim = c(0, 12), ylim = c(-2, 2), pch = 19, type = &#39;p&#39;) abline(h = 0, lwd = 3, col = &quot;red&quot;) plot(residuals(fit2), xlim = c(0, 12), ylim = c(-2, 2), pch = 19, type = &#39;p&#39;) abline(h = 0, lwd = 3, col = &quot;red&quot;) plot(residuals(fit3), xlim = c(0, 12), ylim = c(-2, 4), pch = 19, type = &#39;p&#39;) abline(h = 0, lwd = 3, col = &quot;red&quot;) plot(residuals(fit4), xlim = c(0, 12), ylim = c(-2, 2), pch = 19, type = &#39;p&#39;) abline(h = 0, lwd = 3, col = &quot;red&quot;) Figure 3.9: Anscombe (1973) datasets. Plot of residuals in series, i.e. by index \\(i\\). We should also plot the residuals by predicted value of \\(y\\) to see if there is a pattern as a function of magnitude (Figure 3.10). For the Anscombe dataset, this emphasizes the non-linearity of dataset 2 (top-right) and the outlier in dataset 3 (bottom-left) and also detects the singular extreme point in dataset 4 (bottom-right). In sum, the independence and homoscedasticity assumptions seem to be violated in all datasets except dataset 1. This would have to be formally tested using the Durbin-Watson test, for example. # plot residuals against predicted value of y plot(fitted.values(fit1),residuals(fit1), xlim = c(0, 14), ylim = c(-2, 2), pch = 19, type = &#39;p&#39;) abline(h = 0, lwd = 3, col = &quot;red&quot;) plot(fitted.values(fit2),residuals(fit2), xlim = c(0, 14), ylim = c(-2, 2), pch = 19, type = &#39;p&#39;) abline(h = 0, lwd = 3, col = &quot;red&quot;) plot(fitted.values(fit3),residuals(fit3), xlim = c(0, 14), ylim = c(-2, 4), pch = 19, type = &#39;p&#39;) abline(h = 0, lwd = 3, col = &quot;red&quot;) plot(fitted.values(fit4),residuals(fit4), xlim = c(0, 14), ylim = c(-2, 2), pch = 19, type = &#39;p&#39;) abline(h = 0, lwd = 3, col = &quot;red&quot;) Figure 3.10: Anscombe (1973) datasets. Plot of residuals by predicted value of \\(y\\). The normality assumption can be assessed using the QQ plot (Figure 3.11). # QQ plots qqnorm(residuals(fit1), xlim = c(-4, 4), ylim = c(-4, 4)) qqline(residuals(fit1)) qqnorm(residuals(fit2), xlim = c(-4, 4), ylim = c(-4, 4)) qqline(residuals(fit2)) qqnorm(residuals(fit3), xlim = c(-4, 4), ylim = c(-4, 4)) qqline(residuals(fit3)) qqnorm(residuals(fit4), xlim = c(-4, 4), ylim = c(-4, 4)) qqline(residuals(fit4)) Figure 3.11: Anscombe (1973) datasets. Quantile-quantile plot (QQ plot) of residuals. In the QQ plot, every data point represents a certain quantile of the empirical distribution. This quantile (after standardisation) is plotted (vertical axis) against the value of that quantile expected under a standard normal distribution (horizontal axis). The resultant shapes say something about the distribution of the residuals (Figure 3.12), e.g. in case of a normal distribution they all fall on a straight line. In the Anscombe dataset, the only clearly non-normal dataset seems to be #3 (bottom-left). This would have to be formally tested using the Kolmogorov-Smirnov test or the Shapiro-Wilk test, for example. # 1) simulate normal data x_norm &lt;- rnorm(100, 0, 1) # QQ plot qqnorm(x_norm, xlim = c(-4, 4), ylim = c(-4, 4), main=&#39;Normal data&#39;) qqline(x_norm) # 2) simulate right-skewed data x_right &lt;- rlnorm(100, 0, 1) # QQ plot qqnorm(x_right, xlim = c(-4, 4), ylim = c(-2, 6), main=&#39;Right-skewed data&#39;) qqline(x_right) # 3) simulate left-skewed data x_left &lt;- -x_right # QQ plot qqnorm(x_left, xlim = c(-4, 4), ylim = c(-6, 2), main=&#39;Left-skewed data&#39;) qqline(x_left) # 4) simulate thick-tailed data x_thick &lt;- rcauchy(100, 0, 1) # QQ plot qqnorm(x_thick, xlim = c(-4, 4), ylim = c(-8, 8), main=&#39;Thick-tailed data&#39;) qqline(x_thick) Figure 3.12: Characteristic shapes of the QQ plot and what they mean for the residuals in our case. Note, I couldn’t find an easy distribution that has thinner tails than the normal, but such case would exhibit an S-shape, just like the thick-tailed variant mirrored on the diagonal line. The non-normality of dataset 3 becomes apparent also in the residual histograms (Figure 3.13). They also emphasize the outlier in dataset 3. Note, it is generally difficult to reject the hypothesis of normally distributed residuals with so few data points. # histograms of residuals hist(residuals(fit1), breaks = seq(-4,4,0.5)) hist(residuals(fit2), breaks = seq(-4,4,0.5)) hist(residuals(fit3), breaks = seq(-4,4,0.5)) hist(residuals(fit4), breaks = seq(-4,4,0.5)) Figure 3.13: Anscombe (1973) datasets. Quantile-quantile plot (QQ plot) of residuals. References "],
["categoricalvars.html", "Chapter 4 Categorical predictors", " Chapter 4 Categorical predictors The linear model where the predictor variables are categorical - so called factors - has come to be known as Analysis of Variance (ANOVA). In this chapter we first introduce ANOVA in a classic sense before showing how this is essentially a special case of the linear model. As an example we will look at a dataset of crop yields for different soil types from Crawley (2012) - see Figure 4.1 - asking the question: Does soil type significantly affect crop yield? # load yields data yields &lt;- read.table(&quot;data/yields.txt&quot;,header=T) # means per soil type mu_j &lt;- sapply(list(yields$sand,yields$clay,yields$loam),mean) # overall mean mu &lt;- mean(mu_j) # boxplots of yield per soil type boxplot(yields, ylim = c(0, 20), ylab = &quot;yield&quot;) abline(h = mu, lwd = 3, col = &quot;red&quot;) points(seq(1,3), mu_j, pch = 23, lwd = 3, col = &quot;red&quot;) Figure 4.1: Boxplots of yield per soil type (the factor) with individual and overall means marked in red. Data from: Crawley (2012) The factor in this example is “soil type”. The categories of a factor are called levels, groups or treatments depending on the experimental setting and the research field. In the yields example the factor levels are “sand”, “clay” and “loam”. The parameters of an ANOVA are called effects - more below. ANOVA with one factor is called one-way ANOVA. The typical question answered by ANOVA is: Are means across factor levels significantly different? This is tested by comparing the variation between levels (i.e. the overlap or not between boxplots in Figure 4.1) to the variation within levels (i.e. the size of the individual boxplots). In the case of one factor with two levels, ANOVA is equivalent to the familiar t-test. The Linear effects model formulation for ANOVA is: \\[\\begin{equation} y_{ji}=\\mu+a_j+\\epsilon_{ji} \\tag{4.1} \\end{equation}\\] With \\(j=1, 2, \\ldots, k\\) indexing the levels (e.g. sand, clay, loam), \\(i=1, 2, \\ldots, n_j\\) indexing the data points at level \\(j\\), \\(y_{ji}\\) being the \\(i\\)th observation of the response variable (e.g. yield) at level \\(j\\), \\(\\mu\\) being the overall mean of the response variable (Figure 4.2, left), \\(\\mu_j\\) being the mean of the response variable at level \\(j\\), \\(a_j=\\mu_j-\\mu\\) being the effect of level \\(j\\), and \\(\\epsilon_{ji}\\) being the residual error. So effectively the response variable for each level is predicted by its mean plus random noise (Figure 4.2, right): \\[\\begin{equation} y_{ji}=\\mu_j+\\epsilon_{ji} \\tag{4.2} \\end{equation}\\] Figure 4.2: Left: Variation of data points around overall mean \\(\\mu\\), summarised by \\(SSY\\). Right: Variation of data points around individual means \\(\\mu_1, \\mu_2, \\mu_3\\), summarised by \\(SSE\\). To see how the ANOVA model is essentially a linear model we look at what is called dummy coding of the categorical predictor (here soil type). This is the original data table: yields ## sand clay loam ## 1 6 17 13 ## 2 10 15 16 ## 3 8 3 9 ## 4 6 11 12 ## 5 14 14 15 ## 6 17 12 16 ## 7 9 12 17 ## 8 11 8 13 ## 9 7 10 18 ## 10 11 13 14 Now we expand this to a long table with dummy or indicator variables “clay” and “loam”: yields2 &lt;- data.frame(yield = c(yields$sand, yields$clay, yields$loam), clay = c(rep(0,10), rep(1,10), rep(0,10)), loam = c(rep(0,10), rep(0,10), rep(1,10))) yields2 ## yield clay loam ## 1 6 0 0 ## 2 10 0 0 ## 3 8 0 0 ## 4 6 0 0 ## 5 14 0 0 ## 6 17 0 0 ## 7 9 0 0 ## 8 11 0 0 ## 9 7 0 0 ## 10 11 0 0 ## 11 17 1 0 ## 12 15 1 0 ## 13 3 1 0 ## 14 11 1 0 ## 15 14 1 0 ## 16 12 1 0 ## 17 12 1 0 ## 18 8 1 0 ## 19 10 1 0 ## 20 13 1 0 ## 21 13 0 1 ## 22 16 0 1 ## 23 9 0 1 ## 24 12 0 1 ## 25 15 0 1 ## 26 16 0 1 ## 27 17 0 1 ## 28 13 0 1 ## 29 18 0 1 ## 30 14 0 1 When “clay” is 1 and “loam” is 0 then we are in the clay category, when “clay” is 0 and “loam” is 1 then we are in the loam category, and if both are 0 we are in the “sand” category. This is visualised in 3D in Figure 4.3. Figure 4.3: 3D representation of the dummy coding of the yields dataset. Now we can use the familiar linear model, but with two predictors, to represent ANOVA: \\[\\begin{equation} y_i=\\beta_0+\\beta_1\\cdot x_{i1}+\\beta_2\\cdot x_{i2}+\\epsilon_i \\tag{4.3} \\end{equation}\\] In this formulation, \\(x_1\\) is the indicator variable “clay” and \\(x_2\\) is the indicator variable “loam”. Both can only take values of 0 and 1, they are binary. Hence, looking at Figure 4.3, when both \\(x_1\\) and \\(x_2\\) are 0 then we are in the front corner of the plot where we see the yield data for “sand”. The model is then reduced to \\(y_i=\\beta_0+\\epsilon_i\\), with \\(\\beta_0\\) being the mean yield for “sand”. When \\(x_1=0\\) and \\(x_2=1\\) then we are in the left corner of Figure 4.3 where we see the “loam” yields. The model is then \\(y_i=\\beta_0+\\beta_2+\\epsilon_i\\) with \\(\\beta_0+\\beta_2\\) being the mean yield for “loam”. Finally, when \\(x_1=1\\) and \\(x_2=0\\) then we are in the right corner of Figure 4.3 where we see the “clay” yields. The model is then \\(y_i=\\beta_0+\\beta_1+\\epsilon_i\\) with \\(\\beta_0+\\beta_1\\) being the mean yield for “clay”. \\(\\beta_1\\) and \\(\\beta_2\\) are thus increments added to what’s called the base category (in this case “sand”) to arrive at the new means for “clay” and “loam”, respectively. This is symbolised by the red lines in Figure 4.3. We are basically modelling unique means for each factor level; Equations (4.3) and (4.2) are equivalent. If you remember, we already looked at an ANOVA table under linear regression (chapter 3). Let’s now see how this is essentially similar to the actual ANOVA, while noting some key differences. Table 4.1 shows the one-way ANOVA table. Table 4.1: One-way ANOVA table. Source Sum ofsquares Degrees of freedom \\((df)\\) Mean squares F statistic \\(\\left(F_s\\right)\\) \\(\\Pr\\left(Z\\geq F_s\\right)\\) Level \\(SSA=\\\\SSY-SSE\\) \\(k-1\\) \\(\\frac{SSA}{df_{SSA}}\\) \\(\\frac{\\frac{SSA}{df_{SSA}}}{s^2}\\) \\(1-F\\left(F_s,1,n-k\\right)\\) Error \\(SSE\\) \\(n-k\\) \\(\\frac{SSE}{df_{SSE}}=s^2\\) Total \\(SSY\\) \\(n-1\\) Compare this to Table 3.3. The essential differences are: The explained variation is now labeled “level” with the notation \\(SSA\\), instead of “regression” and \\(SSR\\). The number of parameters subtracted from the data degrees of freedom is now \\(k\\), the number of levels, instead of 2. The error variation is now calculated as \\(SSE=\\sum_{j=1}^{k}\\sum_{i=1}^{n_j}\\left(y_{ji}-\\bar y_j\\right)^2\\) (Figure 4.2, right), instead of \\(SSE=\\sum_{i=1}^{n}\\left(y_i-\\left(\\beta_0+\\beta_1\\cdot x_i\\right)\\right)^2\\). The total variation, however, is the same: \\(SSY=\\sum_{j=1}^{k}\\sum_{i=1}^{n_j}\\left(y_{ji}-\\bar y\\right)^2=\\sum_{i=1}^{n}\\left(y_i-\\bar y\\right)^2\\) (Figure 4.2, left). For comparison, a perfect model fit would look like Figure 4.4. Figure 4.4: Hypothetical perfect fit of an ANOVA model. Left: Variation of data points around overall mean, summarised by \\(SSY\\). Right: Variation of data points around individual means, summarised by \\(SSE\\). When we estimate the individual means of the factor levels, they come with a standard error just like the linear regression parameters. This is: \\[\\begin{equation} s_{\\mu_j}=\\sqrt{\\frac{s^2}{n_j}} \\tag{4.4} \\end{equation}\\] With \\(n_j\\) being 10 in our example, which does not need to be the same across the factor levels, and \\(s^2=\\frac{SSE}{n-k}\\), the error variance from the ANOVA table (Table 4.1). Let’s calculate the various means, effect sizes and corresponding errors again: # define constants n &lt;- 30 k &lt;- 3 n_j &lt;- 10 # means per soil type mu_j &lt;- sapply(list(yields$sand,yields$clay,yields$loam),mean) mu_j ## [1] 9.9 11.5 14.3 # overall mean mu &lt;- mean(mu_j) mu ## [1] 11.9 # effect size a_j &lt;- mu_j - mu a_j ## [1] -2.0 -0.4 2.4 # SSE SSE &lt;- sum(sapply(list(yields$sand,yields$clay,yields$loam),function (x) sum((x-mean(x))^2) )) SSE ## [1] 315.5 # error variance s2 &lt;- SSE/(n-k) s2 ## [1] 11.68519 # standard error of individual means # here the same across factor levels as n_j is homogeneous s_mu_j &lt;- sqrt(s2/n_j) s_mu_j ## [1] 1.08098 Finally, what we are really interested in are the differences between factor level means \\(\\mu_j\\) and whether they are statistically significant, i.e. large enough compared to the variation within each factor level. This is a t-test problem (comparing two means) for which we need the differences of means, let’s call them \\(\\Delta\\mu_j\\), and the standard errors of these differences: \\[\\begin{equation} s_{\\Delta\\mu_j}=\\sqrt{\\frac{2\\cdot s^2}{n_j}} \\tag{4.5} \\end{equation}\\] The t-test statistics then is: \\[\\begin{equation} t_s=\\frac{\\Delta\\mu_j}{s_{\\Delta\\mu_j}} \\tag{4.6} \\end{equation}\\] Compare the standard t-test where the test statistic is \\(t_s=\\frac{\\bar x_1-\\bar x_2}{\\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}}\\). Since in this dataset we have equal sample sizes across levels and homogeneous variance (homoscedasticity) is a fundamental assumption of ANOVA (as it is of linear regression), the standard t-test statistic simplifies to Equation (4.6). The corresponding p-value of the 2-sided test then is: \\[\\begin{equation} \\Pr\\left(|Z|\\geq t_s\\right)=2\\cdot\\left(1-t\\left(t_s,n&#39;-2\\right)\\right) \\tag{4.7} \\end{equation}\\] With \\(t\\left(t_s,n&#39;-2\\right)\\) being the value of the cumulative distribution function (cdf) of the t-distribution with parameter \\(n&#39;-2\\) at the location \\(t_s\\), and \\(n&#39;=20\\) in the yields example. Let’s calculate these t-tests “by hand” in R: # differences of means delta_mu_j &lt;- c(mu_j[2]-mu_j[1],mu_j[3]-mu_j[1],mu_j[3]-mu_j[2]) delta_mu_j ## [1] 1.6 4.4 2.8 # standard error of differences # here the same across factor levels as n_j is homogeneous s_delta_mu_j &lt;- sqrt(2*s2/n_j) s_delta_mu_j ## [1] 1.528737 # t-test statistics t_s &lt;- delta_mu_j/s_delta_mu_j t_s ## [1] 1.046616 2.878193 1.831577 # p-values pvalue &lt;- 2*(1-pt(t_s,20-2)) pvalue ## [1] 0.30912862 0.01000534 0.08361664 We conclude that the yields on sand and loam are just about significantly different at a conventional significance level of 0.01, with yields on sand being 4.4 units lower on average. The other yield differences are not statistically significant (compare Figure 4.1). References "],
["multiplelinreg.html", "Chapter 5 Multiple linear regression 5.1 Model selection 5.2 Collinearity 5.3 Overfitting 5.4 Information criteria 5.5 Mixed continuous-categorical predictors 5.6 General advise", " Chapter 5 Multiple linear regression The extension of linear regression to the case of more than one predictor - be they continuous or categorical or a mix of both - is called multiple linear regression. This means we go from the equation \\(y=\\beta_0+\\beta_1\\cdot x+\\epsilon\\) (Equation (3.2)) to the equation \\(y=\\beta_0+\\sum_{j=1}^{p}\\beta_j\\cdot x_j+\\epsilon\\) (Equation (3.1)). As an example we will look at a dataset of air quality, again from Crawley (2012) - see Figure 5.1 - asking the question: How is ground-level ozone concentration related to wind speed, air temperature and solar radiation intensity? A useful first thing to do is to plot what’s often called a scatterplot matrix (Figure 5.1). # load air quality data ozone &lt;- read.table(&quot;data/ozone.txt&quot;,header=T) # scatterplot matrix of ozone dataset # this requires running example(pairs) first so that the histogramms can be drawn on the diagonal # here this is done in the background pairs(ozone, diag.panel = panel.hist, lower.panel = panel.cor) Figure 5.1: Scatterplot matrix of ozone dataset: rad = solar radiation intensity; temp = air temperature; wind = wind speed; ozone = ground-level ozone concentration. The diagonal shows the histograms of the individual variables. The lower triangle shows the linear correlation coefficients, with font size proportional to size of correlation. Data from: Crawley (2012) On the diagonal you see the histograms of each variable. On the upper triangle you see scatterplots between two variables. On the lower triangle you see linear correlation coefficients, with font size proportional to size of correlation. From this we already see that “ozone” is correlated with all three other variables, but perhaps less so with “radiation”. We also see that the other variables are correlated, at least “wind” and “temperature”. The challenge we now face is typical for multiple regression - it is one of model selection: Which predictor variables to include? E.g. possibly \\(x_1=rad\\), \\(x_2=temp\\), \\(x_3=wind\\) Which interactions between variables to include? E.g. possibly \\(x_4=rad\\cdot temp\\), \\(x_5=rad\\cdot wind\\), \\(x_6=temp\\cdot wind\\), \\(x_7=rad\\cdot temp\\cdot wind\\) We haven’t talked about interactions so far - because we had just one predictor variable to think about - but interactions are quite an interesting element of regression models. Essentially, one predictor could modulate the effect that another predictor has on the response - we will discuss an example below in Chapter 5.5. Mathematically, this is achieved by adding the product of the two predictors as a predictor to the linear model, on top of the two individual predictors. In addition to such 2-way interactions, we can have 3-way interactions (three predictors multiplied) and so on and so forth, but these get increasingly complicated to interpret. In principle, we could also include higher-order terms, such as \\(x_8=rad^2\\), \\(x_9=temp^2\\), \\(x_{10}=wind^2\\) etc., in a regression model, as well as other predictor transformations. In practice, though, such choices will only be included if there is an process reason to include them. We then face two main problems in model selection: Collinearity of variables: Predictors may be correlated with each other, which complicates their estimation. Interactions (and higher-order terms) certainly introduce collinearity as we will see below. Overfitting: The more predictors (and hence parameters) we add the better we can fit the data; but with an increasing risk of fitting the noise and not just the signal in the data, which will lead to poor predictions. We discuss these points in turn in the next section. 5.1 Model selection Model selection often appeals to the Parsimony Principle or Occam’s Razor. It goes roughly like this: Given a set of models with “similar explanatory power”, the “simplest” of these shall be preferred. This is called a “philosophical razor”, i.e. a rule of thumb that narrows down the choices of possible explanation or action. It dates back to English Franciscan friar William of Ockham (also Occam; c. 1287-1347). His was a time of controversy within the church; and William of Ockham was one of those who advocated for a “simple” life (and by extension a poor and not a rich church, which got him into trouble).2 I believe this quest for simplicity carried over to his view on epistemology (how we know things) - hence Occam’s Razor. Anyhow, for us in statistical modelling Occam’s Razor roughly translates to the following guidelines (after Crawley (2012)): Prefer a model with \\(m-1\\) parameters to a model with \\(m\\) parameters Prefer a model with \\(k-1\\) explanatory variables to a model with \\(k\\) explanatory variables Prefer a linear model to a non-linear model Prefer a model without interactions to a model containing interactions between explanatory variables A model shall be simplified until it is minimal adequate To understand “minimal adequate”, consider Table 5.1. Table 5.1: Model complexity types in model selection. After: Crawley (2012). Saturated model Maximal model Minimal adequate model Null model One parameter for every data point Contains all (\\(p\\)) explanatory variables and interactions that might be of interest (many likely insignificant) Simplified model with \\(p&#39;\\) parameters (\\(0\\leq p&#39;\\leq p\\)) Just one parameter, the overall mean \\(\\bar y\\) Fit: perfect Fit: less than perfect Fit: less than maximal model, but not significantly so Fit: none, \\(SSE=SSY\\) Degrees of freedom: \\(0\\) Degrees of freedom: \\(n-p-1\\) Degrees of freedom: \\(n-p&#39;-1\\) Degrees of freedom: \\(n-1\\) Explanatory power: none Explanatory power: \\(r^2=1-\\frac{SSE}{SSY}\\) Explanatory power: \\(r^2=1-\\frac{SSE}{SSY}\\) Explanatory power: none At one end of the complexity spectrum of potential models is the so called saturated model. Is has one parameter for every data point and hence fits the data perfectly - this can be shown mathematically and is displayed in Figure 5.2 for a polynomial of \\((n-1)\\)th order. But this model has zero degrees of freedom, hence has no explanatory power; it fits the noise around the signal perfectly, which has no use in prediction - just imagine to use the \\((n-1)\\)th-order polynomial (or even lower-order ones) in Figure 5.2 for extrapolation. Figure 5.2: A dataset of nine data points is fitted by polynomials of varying order; poly1=1st-order to poly8=8th-order. The 8th-order polynomial (equation at top of figure) fits the data perfectly as it is a saturated model; it has as many parameters as data points. The Null model (intercept only) is just the mean of \\(y\\); equation at bottom right. At the other end of the complexity spectrum is what’s called the Null model. Its only parameter is the intercept \\(\\beta_0\\), whose best estimate minimising the sum of squared errors \\(SSE\\) is the mean of \\(y\\), i.e. \\(\\bar y\\) (see also Figure 5.2). The Null model doesn’t fit anything beyond \\(SSY\\), the variation around the mean, hence doesn’t explain any relations in the data. In between those polar opposites are the so called maximal model and the minimal adequate model. These terms are only loosely defined but mark the space of model complexity that we navigate in model selection. The maximal model contains all explanatory variables and interactions that might be of interest, of which many will likely turn out insignificant. It fits the data less than perfect - but that’s not the goal anyway given noise - and its explanatory power can be judged with \\(r^2\\), for example. I suggest that the maximal model be strongly informed by our underlying (theoretical) understanding of the relations to be modelled, and that predictors and interactions that don’t make any sense in relation to that understanding be excluded. This approach, of course, will limit our exposure to surprises, which we could learn a lot from, but aims at keeping the model selection problem manageable. The minimal adequate model then includes the subset of predictors of the maximal model that “really matter”, naturally compromising some goodness of fit of the maximal model, but not significantly so - that’s the trick. What “really matters” in that sense depends. Under the classic statistical paradigm, this has a lot to do with statistical significance (the p-values of the parameter estimates); we rarely include parameters that are insignificant.3 But since anything can be significant with enough data points, the cutoff at a significance level of say \\(\\alpha=0.01\\) seems arbitrary. But p-values can be taken as functions of the standard errors of the parameter estimates - this is simply what they are - and this is what they are useful for. In general we don’t want too many parameters in our models because this inflates their standard errors, making their interpretation essentially meaningless - so looking at standard errors (via p-values if you must) is important. But we want to be able to include the occasional parameter that has a large standard error (and is insignificant in a classic sense), simply because there might a mechanistic reason to do so, especially for prediction. The standard error of that parameter may be large because there is little information in the data at hand about the parameter, but that’s no reason to exclude it if we have reason to believe it is important. In this case the large standard error just helps to be honest about the capabilities of our model given the data at hand. So, in sum, let’s not be overly concerned about p-values during model selection. Instead, as we will see below (Chapter 5.4), it is a good idea to base model selection on information criteria as these approximate the models predictive performance. Model selection involves a lot of trial and error and personal judgement, but there are a few guidelines. In general, we can follow an up-ward or a down-ward selection of models. Up-ward model selection starts with a minimal set of predictors and sequentially adds more when this increases some information criterion or other measure. Down-ward model selection starts with the maximal model and sequentially simplifies this. Along each way there will be steps where we test several models - different routes to take for complication or simplification, respectively. Again, information criteria are crucial here. We can do this manually but automatic model selection algorithms are available in R - you will learn some of them in the PC-lab. But I would not trust them blindly - always confirm the result by testing the final model against some alternatives. 5.2 Collinearity Predictors are collinear (sometimes called multicollinear) when they are perfectly correlated, i.e. when a linear combination of them exists that equals zero for all the data (the estimated parameter values can compensate each other). The parameters then cannot be estimated uniquely (the estimates have standard errors of infinity) - they are said to be nonidentifiable. In practice, predictors are seldom perfectly correlated, so near-collinearity and poor identifiability are the issues to worry about. In the ozone dataset, we see weak collinearity between predictors, so nothing to worry about too much at this stage (Figure 5.1). Modelling interactions between predictors, however, introduces collinearity (Figure 5.3). # generate new dataset w interactions ozone2 &lt;- ozone ozone2$rad_temp &lt;- ozone$rad * ozone$temp ozone2$rad_wind &lt;- ozone$rad * ozone$wind ozone2$temp_wind &lt;- ozone$temp * ozone$wind ozone2$rad_temp_wind &lt;- ozone$rad * ozone$temp * ozone$wind pairs(ozone2, diag.panel = panel.hist, lower.panel = panel.cor) Figure 5.3: Scatterplot matrix of ozone dataset, including interactions. Interaction terms are generally correlated with the predictors interacting. Data from: Crawley (2012) We can live with mild levels of collinearity, for reasons discussed above, especially if including interactions, for example, is important for mechanistic reasons. However, if standard errors become so large as to make the parameter estimates essentially meaningless, we need to leave out some of the correlated predictors - even if we find them important from a mechanistic perspective. Another option is to transform the predictors by Principal Component Analysis (PCA; Chapter 8) into a set of new, uncorrelated predictors that combine the information of the original predictors. 5.3 Overfitting Overfitting occurs when we try to estimate too many parameters compared to the size of the dataset at hand. Then we will unduly fit the noise around the signal that interests us, from which there is nothing to be learned. We will also inflate standard errors as more and more parameters become less and less identifiable. This is illustrated with polynomials in Figure 5.2. To get an idea of how common this problem is we can look at another air quality dataset, also from Crawley (2012) (Figure 5.4). # load 2nd air quality data sulphur &lt;- read.table(&quot;data/sulphur.txt&quot;,header=T) pairs(sulphur, diag.panel = panel.hist, lower.panel = panel.cor) Figure 5.4: Scatterplot matrix of sulphur dataset: Pollution = sulphur dioxide concentration; Temp = air temperature; Industry = prevalence of industry; Population = population size; Wind = wind speed; Rain = rainfall; Wet-days = number of wet days. Data from: Crawley (2012) Here we have 41 data points and six possible main predictors. How many possible predictors can we have by including all possible interactions? (Q2)4 Once you’ve worked this out you see that we have to be really selective if we want to include interactions, because we approach the saturated model (Figure 5.2) very quickly. 5.4 Information criteria Information criteria help us select models because they approximate the models’ predictive performance, i.e. how well they would fit observations not included when fitting the models (“out-of-sample”). Information criteria penalise models for overfitting because overfitting makes predictive performance worse. Under the classical statistical paradigm, the preferred information criterion is arguably the Akaike Information Criterion (AIC):5 \\[\\begin{equation} AIC=-2\\cdot logL\\left(\\hat\\beta, \\hat\\sigma|\\mathbf{y}\\right)+2\\cdot p \\tag{5.1} \\end{equation}\\] \\(logL\\left(\\hat \\beta, \\hat \\sigma|\\mathbf{y}\\right)\\) is the log-likelihood function at the maximum likelihood estimate of the parameters \\(\\hat \\beta\\), with \\(\\hat \\sigma=\\sqrt{\\frac{SSE}{n-p}}\\), given the data \\(\\mathbf{y}\\). The maximum likelihood estimate is the Least Squares estimate for linear regression. We will learn more about the likelihood function and its relation to Least Squares in Chapter 6. \\(p\\) is the number of parameters in our model. Under the classic statistical paradigm, AIC is a reliable approximation of out-of-sample performance only when the likelihood function is approximately normal (which is a standard assumption anyway) and when the sample size is much greater than the number of parameters (McElreath 2020). So our estimate for predictive performance given by AIC is the log-likelihood of the “best” parameter estimates, penalised by the number of model parameters. We don’t have to worry about the factor “-2” in front of the log-likelihood,6 but need to get used to the fact that smaller (possibly negative) values indicate better models. Also note that AIC is not an absolute measure of model out-of-sample performance, as we don’t have an absolute benchmark like a true process; AIC only makes sense relatively when comparing two models. AIC is implemented in automated model selection algorithms, such as the step() function in R. Other information criteria exist but they are based on assumptions that, under the classic paradigm, are similar or even less realistic than those of AIC. Let’s analyse the ozone dataset now. We start up-ward with just the individual predictors. Prior to that we standardise the predictors (see Chapter 2). This brings all predictors onto the same scale and hence makes parameter estimates comparable. It also makes parameters easier to interpret. The intercept now is the ozone concentration when all predictors are at their mean values. And each parameter measures the change in ozone concentration when that predictor changes by one standard deviation. Standardising the predictors is common practice (Gelman, Hill, and Vehtari 2020). # standardise predictors ozone_std &lt;- ozone ozone_std$rad &lt;- (ozone$rad-mean(ozone$rad))/sd(ozone$rad) ozone_std$temp &lt;- (ozone$temp-mean(ozone$temp))/sd(ozone$temp) ozone_std$wind &lt;- (ozone$wind-mean(ozone$wind))/sd(ozone$wind) # multiple linear regression model with 3 predictors ozone_fit &lt;- lm(ozone ~ rad+temp+wind, data = ozone_std) # extract information about parameter estimates summary(ozone_fit) ## ## Call: ## lm(formula = ozone ~ rad + temp + wind, data = ozone_std) ## ## Residuals: ## Min 1Q Median 3Q Max ## -40.485 -14.210 -3.556 10.124 95.600 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42.099 2.010 20.949 &lt; 2e-16 *** ## rad 5.451 2.113 2.580 0.0112 * ## temp 15.736 2.415 6.516 2.43e-09 *** ## wind -11.879 2.327 -5.105 1.45e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 21.17 on 107 degrees of freedom ## Multiple R-squared: 0.6062, Adjusted R-squared: 0.5952 ## F-statistic: 54.91 on 3 and 107 DF, p-value: &lt; 2.2e-16 Before we interpret this, let’s look at the residuals: # residuals by index plot(residuals(ozone_fit), pch = 19, type = &#39;p&#39;) # residuals by modelled value plot(fitted.values(ozone_fit), residuals(ozone_fit), pch = 19, type = &#39;p&#39;) # residual histogram hist(residuals(ozone_fit)) # residual QQ-plot qqnorm(residuals(ozone_fit)) qqline(residuals(ozone_fit)) The residuals are heteroscedastic and right-skewed, which is something we might be able to fix by log-transforming the response. This is something to try in any case when the response varies over orders of magnitude (Gelman, Hill, and Vehtari 2020). # add log-transform of ozone to data.frame ozone_std$log_ozone &lt;- log(ozone_std$ozone) # fit log_ozone_fit &lt;- lm(log_ozone ~ rad+temp+wind, data = ozone_std) summary(log_ozone_fit) ## ## Call: ## lm(formula = log_ozone ~ rad + temp + wind, data = ozone_std) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.06212 -0.29968 -0.00223 0.30767 1.23572 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.41593 0.04826 70.775 &lt; 2e-16 *** ## rad 0.22922 0.05074 4.518 1.62e-05 *** ## temp 0.46852 0.05800 8.078 1.07e-12 *** ## wind -0.21922 0.05589 -3.922 0.000155 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5085 on 107 degrees of freedom ## Multiple R-squared: 0.6645, Adjusted R-squared: 0.6551 ## F-statistic: 70.65 on 3 and 107 DF, p-value: &lt; 2.2e-16 Log-transforming ozone stabilised the predictors and also increased \\(r^2\\) by 0.05. The relationships of ozone to the three predictors has apparently turned more linear on the log-scale. The residuals, too, conform better to the assumptions, except for one outlier at the very low end: plot(residuals(log_ozone_fit), pch = 19, type = &#39;p&#39;) plot(fitted.values(log_ozone_fit), residuals(log_ozone_fit), pch = 19, type = &#39;p&#39;) hist(residuals(log_ozone_fit)) qqnorm(residuals(log_ozone_fit)) qqline(residuals(log_ozone_fit)) Finally, the AIC of the log-ozone model got a lot better compared to the ozone model (remember that a smaller AIC indicates a better model): AIC(ozone_fit) ## [1] 998.6276 AIC(log_ozone_fit) ## [1] 170.7949 So we can proceed from this base model and see if adding interactions improves fit \\(\\left(r^2\\right)\\) and predictive performance (AIC). We start by adding the interaction of the largest effects (Gelman, Hill, and Vehtari 2020): log_ozone_fit2 &lt;- lm(log_ozone ~ rad*temp+wind, data = ozone_std) summary(log_ozone_fit2) ## ## Call: ## lm(formula = log_ozone ~ rad * temp + wind, data = ozone_std) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.18244 -0.30194 0.00665 0.31242 1.18972 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.40218 0.05061 67.229 &lt; 2e-16 *** ## rad 0.25245 0.05682 4.443 2.19e-05 *** ## temp 0.46918 0.05805 8.082 1.10e-12 *** ## wind -0.21934 0.05594 -3.921 0.000157 *** ## rad:temp 0.04717 0.05179 0.911 0.364443 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5089 on 106 degrees of freedom ## Multiple R-squared: 0.6671, Adjusted R-squared: 0.6546 ## F-statistic: 53.11 on 4 and 106 DF, p-value: &lt; 2.2e-16 AIC(log_ozone_fit2) ## [1] 171.9295 This interaction actually makes the predictive performance a little worse (greater AIC). Let’s try the next: log_ozone_fit3 &lt;- lm(log_ozone ~ rad+temp*wind, data = ozone_std) summary(log_ozone_fit3) ## ## Call: ## lm(formula = log_ozone ~ rad + temp * wind, data = ozone_std) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.98696 -0.32076 -0.05428 0.30238 1.19016 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.36697 0.05245 64.190 &lt; 2e-16 *** ## rad 0.23644 0.04998 4.731 6.93e-06 *** ## temp 0.46806 0.05700 8.211 5.70e-13 *** ## wind -0.21984 0.05493 -4.002 0.000117 *** ## temp:wind -0.09938 0.04545 -2.187 0.030967 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4997 on 106 degrees of freedom ## Multiple R-squared: 0.679, Adjusted R-squared: 0.6669 ## F-statistic: 56.05 on 4 and 106 DF, p-value: &lt; 2.2e-16 AIC(log_ozone_fit3) ## [1] 167.8976 Predictive performance is a little better than the base model (smaller AIC) and \\(r^2\\) increased by 0.02, even if the interaction comes out fairly uncertain. On to the last interaction: log_ozone_fit4 &lt;- lm(log_ozone ~ rad*wind+temp, data = ozone_std) summary(log_ozone_fit4) ## ## Call: ## lm(formula = log_ozone ~ rad * wind + temp, data = ozone_std) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.99855 -0.33075 -0.01627 0.26636 1.22283 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.40650 0.04837 70.423 &lt; 2e-16 *** ## rad 0.25227 0.05267 4.790 5.45e-06 *** ## wind -0.21686 0.05558 -3.902 0.000168 *** ## temp 0.46830 0.05765 8.123 8.93e-13 *** ## rad:wind -0.07466 0.04915 -1.519 0.131751 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5054 on 106 degrees of freedom ## Multiple R-squared: 0.6717, Adjusted R-squared: 0.6593 ## F-statistic: 54.21 on 4 and 106 DF, p-value: &lt; 2.2e-16 AIC(log_ozone_fit4) ## [1] 170.4048 This interaction is not estimated precisely and we only gain an improvement in \\(r^2\\) of 0.01 and a negligible gain in AIC. So I’m inclined to go with model 3 (all three predictors and “temp:wind” interaction). Due to the imprecision of the interaction parameter it doesn’t make sense to add more predictors at this stage. The residuals of model 3 still look ok: plot(residuals(log_ozone_fit3), pch = 19, type = &#39;p&#39;) plot(fitted.values(log_ozone_fit3), residuals(log_ozone_fit3), pch = 19, type = &#39;p&#39;) hist(residuals(log_ozone_fit3)) qqnorm(residuals(log_ozone_fit3)) qqline(residuals(log_ozone_fit3)) The model that we settled with is: \\[\\begin{equation} \\log(ozone)=3.4+0.2\\cdot rad_{std}+0.5\\cdot temp_{std}-0.2\\cdot wind_{std}-0.1\\cdot temp_{std}\\cdot wind_{std}+\\epsilon \\tag{5.2} \\end{equation}\\] It explains 68% of the variation in ozone concentration (at the log-scale, judged by \\(r^2\\)) - which is pretty good - and tells us the following: The most important driver (of those we had data for) of ozone concentration is air temperature, followed by radiation intensity and wind speed. We get this from comparing the size of the parameters of the different predictors, which we can only do if predictors are standardised. Air temperature and radiation intensity increase ozone concentrations, while wind speed decreases it. The negative interaction of air temperature and wind speed tells us that the air temperature effects is down-regulated with increasing wind speeds.7 We can thus group Equation (5.2) as follows: \\[\\begin{equation} \\log(ozone)=3.4+0.2\\cdot rad_{std}+\\left(0.5-0.1\\cdot wind_{std}\\right)\\cdot temp_{std}-0.2\\cdot wind_{std}+\\epsilon \\tag{5.3} \\end{equation}\\] This is a useful way of understanding interactions, which we will revisit now in the last section. 5.5 Mixed continuous-categorical predictors The special case of a mix of continuous and categorical predictors has got the special name of analysis of covariance (ANCOVA), though as we said before (chapter 4) it’s really just another variant of a linear model. For illustration we use an example from Gelman, Hill, and Vehtari (2020), modelling childrens’ IQ score by their mothers’ IQ score and whether or not the mothers have a high school degree (Figure 5.5). # load IQ data from remote repository iq &lt;- read.csv(&quot;https://raw.githubusercontent.com/avehtari/ROS-Examples/master/KidIQ/data/kidiq.csv&quot;, header=TRUE) # plot kid&#39;s IQ against mom&#39;s IQ # with symbols differentiated by whether or not the mom has a high school degree plot(iq$mom_iq, iq$kid_score, pch = c(1, 20)[iq$mom_hs+1], col = c(&quot;black&quot;, &quot;gray&quot;)[iq$mom_hs+1], xlab = &quot;Mom&#39;s IQ&quot;, ylab = &quot;Kid&#39;s IQ&quot;) Figure 5.5: Childrens’ IQ score against their mothers’ IQ score, with symbol and shading indicating whether or not the mothers have a high school degree (open black dots: no high school degree; closed grey dots: highschool degree. Data from: Gelman, Hill, and Vehtari (2020) We first centre the predictor “mom_iq” (the IQ score of the mothers) to make the corresponding parameter easier to interpret. Note, standardisation is not necessary here because there is no other continuous predictor to compare against, just a binary predictor.8 The binary predictor is “mom_hs”, with “1” indicating mother has a high school degree and “0” indicating mother hasn’t got one. # centre continuous predictor iq_cen &lt;- iq iq_cen$mom_iq &lt;- iq$mom_iq-mean(iq$mom_iq) We then fit all model variants with and without interaction all at once and compare them via AIC. Normally we would do this step by step as in the previous example and check residuals at every stage. I skip this here and only check residuals at the end because I’m more interested in showcasing the different model variants and what they mean mathematically and mechanistically. # fit Null model iq_fit0 &lt;- lm(kid_score ~ 1, data = iq_cen) coef(summary(iq_fit0)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 86.79724 0.9797444 88.59171 1.330993e-279 AIC(iq_fit0) ## [1] 3852.576 # fit common slope and intercept model iq_fit1 &lt;- lm(kid_score ~ mom_iq, data = iq_cen) coef(summary(iq_fit1)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 86.7972350 0.87680195 98.99298 5.127032e-299 ## mom_iq 0.6099746 0.05852092 10.42319 7.661950e-23 AIC(iq_fit1) ## [1] 3757.216 # fit individual Null models (2 intercepts) iq_fit2 &lt;- lm(kid_score ~ mom_hs, data = iq_cen) coef(summary(iq_fit2)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 77.54839 2.058612 37.670231 1.392224e-138 ## mom_hs 11.77126 2.322427 5.068516 5.956524e-07 AIC(iq_fit2) ## [1] 3829.506 # fit common slope model (2 intercepts) iq_fit3 &lt;- lm(kid_score ~ mom_iq+mom_hs, data = iq_cen) coef(summary(iq_fit3)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 82.122143 1.94370047 42.250411 2.435765e-155 ## mom_iq 0.563906 0.06057408 9.309362 6.609618e-19 ## mom_hs 5.950117 2.21181218 2.690155 7.419327e-03 AIC(iq_fit3) ## [1] 3751.989 # fit common intercept model (2 slopes) iq_fit4 &lt;- lm(kid_score ~ mom_iq+mom_iq:mom_hs, data = iq_cen) coef(summary(iq_fit4)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 87.7805823 0.8998447 97.550808 6.602618e-296 ## mom_iq 1.0549920 0.1288807 8.185802 3.085494e-15 ## mom_iq:mom_hs -0.5657798 0.1465785 -3.859909 1.307991e-04 AIC(iq_fit4) ## [1] 3744.467 # fit maximal model iq_fit5 &lt;- lm(kid_score ~ mom_iq*mom_hs, data = iq_cen) coef(summary(iq_fit5)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 85.4068999 2.2182219 38.50242 1.970160e-141 ## mom_iq 0.9688892 0.1483437 6.53138 1.843084e-10 ## mom_hs 2.8407569 2.4266700 1.17064 2.423919e-01 ## mom_iq:mom_hs -0.4842747 0.1622171 -2.98535 2.994237e-03 AIC(iq_fit5) ## [1] 3745.086 The most complex model (#5) comes out on top here according to AIC. Its residuals look ok too: plot(residuals(iq_fit5), pch = 19, type = &#39;p&#39;) plot(fitted.values(iq_fit5), residuals(iq_fit5), pch = 19, type = &#39;p&#39;) hist(residuals(iq_fit5)) qqnorm(residuals(iq_fit5)) qqline(residuals(iq_fit5)) Let’s plot the six variants to look at the meaning of interactions once more (Figure 5.6). # Null model plot(iq_cen$mom_iq, iq_cen$kid_score, pch = c(1, 20)[iq_cen$mom_hs+1], col = c(&quot;black&quot;, &quot;gray&quot;)[iq_cen$mom_hs+1], main =&quot;Null model&quot;, xlab = &quot;Mom&#39;s IQ (centred)&quot;, ylab = &quot;Kid&#39;s IQ&quot;) abline(h = coef(iq_fit0), lwd = 3, col = &quot;black&quot;) # common slope and intercept plot(iq_cen$mom_iq, iq_cen$kid_score, pch = c(1, 20)[iq_cen$mom_hs+1], col = c(&quot;black&quot;, &quot;gray&quot;)[iq_cen$mom_hs+1], main = &quot;common slope and intercept&quot;, xlab = &quot;Mom&#39;s IQ (centred)&quot;, ylab = &quot;Kid&#39;s IQ&quot;) abline(coef(iq_fit1), lwd = 3, col = &quot;black&quot;) # individual Null models plot(iq_cen$mom_iq, iq_cen$kid_score, pch = c(1, 20)[iq_cen$mom_hs+1], col = c(&quot;black&quot;, &quot;gray&quot;)[iq_cen$mom_hs+1], main = &quot;individual Null models&quot;, xlab = &quot;Mom&#39;s IQ (centred)&quot;, ylab = &quot;Kid&#39;s IQ&quot;) abline(h = coef(iq_fit2)[1], lwd = 3, col = &quot;black&quot;) abline(h = sum(coef(iq_fit2)), lwd = 3, col = &quot;gray&quot;) # common slope plot(iq_cen$mom_iq, iq_cen$kid_score, pch = c(1, 20)[iq_cen$mom_hs+1], col = c(&quot;black&quot;, &quot;gray&quot;)[iq_cen$mom_hs+1], main = &quot;common slope&quot;, xlab = &quot;Mom&#39;s IQ (centred)&quot;, ylab = &quot;Kid&#39;s IQ&quot;) abline(coef(iq_fit3)[c(1,2)], lwd = 3, col = &quot;black&quot;) abline(c(sum(coef(iq_fit3)[c(1,3)]),coef(iq_fit3)[2]), lwd = 3, col = &quot;gray&quot;) # common intercept plot(iq_cen$mom_iq, iq_cen$kid_score, pch = c(1, 20)[iq_cen$mom_hs+1], col = c(&quot;black&quot;, &quot;gray&quot;)[iq_cen$mom_hs+1], main = &quot;common intercept&quot;, xlab = &quot;Mom&#39;s IQ (centred)&quot;, ylab = &quot;Kid&#39;s IQ&quot;) abline(coef(iq_fit4)[c(1,2)], lwd = 3, col = &quot;black&quot;) abline(c(coef(iq_fit4)[1],sum(coef(iq_fit4)[c(2,3)])), lwd = 3, col = &quot;gray&quot;) # maximal model plot(iq_cen$mom_iq, iq_cen$kid_score, pch = c(1, 20)[iq_cen$mom_hs+1], col = c(&quot;black&quot;, &quot;gray&quot;)[iq_cen$mom_hs+1], main = &quot;maximal model&quot;, xlab = &quot;Mom&#39;s IQ (centred)&quot;, ylab = &quot;Kid&#39;s IQ&quot;) abline(coef(iq_fit5)[c(1,2)], lwd = 3, col = &quot;black&quot;) abline(c(sum(coef(iq_fit5)[c(1,3)]),sum(coef(iq_fit5)[c(2,4)])), lwd = 3, col = &quot;gray&quot;) Figure 5.6: Six model variants for the IQ dataset. Open black dots and black lines: mother has no high school degree. Closed grey dots and grey lines: mother has a highschool degree. Data from: Gelman, Hill, and Vehtari (2020). Mathematically, the Null model is: \\[\\begin{equation} IQ_{kid}=87+\\epsilon \\tag{5.4} \\end{equation}\\] I.e. the Null model is nothing more than the children’s mean IQ score, which is 87. The common slope and intercept model is: \\[\\begin{equation} IQ_{kid}=87+0.6\\cdot \\left(IQ_{mom}-\\bar{IQ}_{mom}\\right)+\\epsilon \\tag{5.5} \\end{equation}\\] So when mother’s IQ score is at its average then the child’s IQ score is again the overall average, 87, but increases or decreases by 0.6 for every unit increase or decrease in mother’s IQ score. The individual Null models formulation is: \\[\\begin{equation} IQ_{kid}=78+12\\cdot hs_{mom}+\\epsilon \\tag{5.6} \\end{equation}\\] These are two different intercepts now, the mean children’s IQ score for mothers without a high school degree (when \\(hs_{mom}=0\\)), which is 78, and the mean children’s IQ score for mothers with a high school degree (when \\(hs_{mom}=1\\)), which is 78+12=90. The common slope model is: \\[\\begin{equation} IQ_{kid}=82+6\\cdot hs_{mom}+0.6\\cdot \\left(IQ_{mom}-\\bar{IQ}_{mom}\\right)+\\epsilon \\tag{5.7} \\end{equation}\\] This model assumes different mean children’s IQ scores for mothers with and without high school degree, but the same dependence on the mothers’ IQ score, which again comes out at 0.6. The common intercept model now includes the interaction term, but not \\(hs_{mom}\\) as an individual predictor: \\[\\begin{equation} IQ_{kid}=88+1.1\\cdot \\left(IQ_{mom}-\\bar{IQ}_{mom}\\right)-0.6\\cdot hs_{mom}\\cdot \\left(IQ_{mom}-\\bar{IQ}_{mom}\\right)+\\epsilon \\tag{5.8} \\end{equation}\\] Regrouping leads to: \\[\\begin{equation} IQ_{kid}=88+\\left(1.1-0.6\\cdot hs_{mom}\\right)\\cdot \\left(IQ_{mom}-\\bar{IQ}_{mom}\\right)+\\epsilon \\tag{5.9} \\end{equation}\\] According to this model we embark from a common intercept but then follow different slopes depending on mother’s high school degree. When mother has a high school degree, the child’s IQ increases by 1.1-0.6=0.5 for very unit increase in mother’s IQ. When mother has no high school degree, then mother’s IQ effect is stronger, increasing the child’s IQ by 1.1 for very unit increase in mother’s IQ. Finally, the maximal model includes all predictors: \\[\\begin{equation} IQ_{kid}=85+3\\cdot hs_{mom}+1.0\\cdot \\left(IQ_{mom}-\\bar{IQ}_{mom}\\right)-0.5\\cdot hs_{mom}\\cdot \\left(IQ_{mom}-\\bar{IQ}_{mom}\\right)+\\epsilon \\tag{5.10} \\end{equation}\\] Regrouping leads to: \\[\\begin{equation} IQ_{kid}=85+3\\cdot hs_{mom}+\\left(1.0-0.5\\cdot hs_{mom}\\right)\\cdot \\left(IQ_{mom}-\\bar{IQ}_{mom}\\right)+\\epsilon \\tag{5.11} \\end{equation}\\] So positing two different average children’s IQ scores for mothers with and without high school degree, 88 and 85 respectively, and two different dependencies on mothers’ IQ score, 0.5 and 1.0 respectively. If we believe this model that turned out best according AIC, then there is a difference in average children’s IQ score depending on whether or not the children’s mothers have a high school degree (with high school degrees improving IQ scores by 3 units on average). What also matters is the mothers’ IQ score, with a positive relationship between mothers’ and children’s score. This relationship is stronger (mother’s IQ matters more) when mothers have no high school degree than when they have one. 5.6 General advise Let’s finish with some general advise for building regression models, taken from Gelman, Hill, and Vehtari (2020): Include all predictors that we consider important for mechanistic reasons. Consider combining several predictors into a “total score” by averaging or summation (this is something we haven’t looked at so far). If predictors have large effects, consider including their interactions. Use standard errors to get a sense of uncertainties in parameter estimates. Decide upon including or excluding predictors based on a combination of contextual understanding, data, and the uses to which the regression will be put: If the parameter of a predictor is estimated precisely (small standard error), then it generally makes sense to keep it in the model as it should improve predictions. If the standard error of a parameter is large and there is no good mechanistic reason for the predictor to be included, then it can make sense to remove it, as this can allow the other model parameters to be estimated more stably and can even reduce prediction errors. If a predictor is important for the problem at hand, then Gelman, Hill, and Vehtari (2020) generally recommend keeping it in, even if the estimate has a large standard error and is not “statistically significant”. In such settings we must acknowledge the resulting uncertainty and perhaps try to reduce it, e.g. by gathering more data. If a coefficient doesn’t make sense, then we should try to understand how this could happen. If the standard error is large, the estimate could be explainable from random variation. If the standard error is small, it can make sense to put more effort into understanding the coefficient. References "],
["mlbayes.html", "Chapter 6 Probabilistic underpinnings 6.1 Inference via Maximum Likelihood 6.2 Outlook: Bayesian inference", " Chapter 6 Probabilistic underpinnings In this chapter we will see how the assumptions of linear regression - which are needed for the quantification of uncertainty in our results - come about. We will first derive the Least Squares parameter estimators from Maximum Likelihood theory (the historically dominant approach) before giving an introduction to the more general approach of Bayesian statistics. The latter is the focus of my course Applied Statistical Modelling in the summer term and also features at a basic level in Risk and Uncertainty in Science and Policy (offered summers and winters). 6.1 Inference via Maximum Likelihood The so called likelihood of parameters conditional on some calibration data is defined as the probability of the data conditional on the parameters (and implicitly the model): \\[\\begin{equation} L(\\boldsymbol{\\theta}|\\mathbf{y})=\\Pr(\\mathbf{y}|\\boldsymbol{\\theta}) \\tag{6.1} \\end{equation}\\] \\(\\boldsymbol{\\theta}\\) is a vector of parameters, in the case of linear regression \\(\\boldsymbol{\\theta}=\\begin{pmatrix}\\beta_0 &amp; \\beta_1 &amp; \\sigma\\end{pmatrix}\\), and \\(\\mathbf{y}\\) is a vector of response data points \\(y_i\\). If all \\(y_i\\) are independent - here comes the first assumption of linear regression - then the joint probability in Equation (6.1) equals the product of the individual probabilities: \\[\\begin{equation} L(\\boldsymbol{\\theta}|\\mathbf{y})=\\prod_{i=1}^{n}\\Pr(y_i|\\boldsymbol{\\theta}) \\tag{6.2} \\end{equation}\\] This follows from the product rule of probability calculus. If we further assume the residuals of the linear model to be normally distributed then the likelihood is: \\[\\begin{equation} L(\\beta_0,\\beta_1,\\sigma|\\mathbf{y})=\\prod_{i=1}^{n}\\frac{1}{\\sigma\\cdot\\sqrt{2\\cdot\\pi}}\\cdot\\exp\\left(\\frac{\\left(y_i-\\beta_0-\\beta_1\\cdot x_i\\right)^2}{-2\\cdot\\sigma^2}\\right) \\tag{6.3} \\end{equation}\\] This means, the probability of individual data points to arise given certain parameter values, \\(\\Pr(y_i|\\boldsymbol{\\theta})\\), is \\(\\frac{1}{\\sigma\\cdot\\sqrt{2\\cdot\\pi}}\\cdot\\exp\\left(\\frac{\\left(y_i-\\beta_0-\\beta_1\\cdot x_i\\right)^2}{-2\\cdot\\sigma^2}\\right)\\). This is the formula of the probability density function (pdf) of the normal distribution, \\(\\frac{1}{\\sigma\\cdot\\sqrt{2\\cdot\\pi}}\\cdot\\exp\\left(\\frac{\\left(y_i-\\mu\\right)^2}{-2\\cdot\\sigma^2}\\right)\\), with \\(\\mu\\) being substituted with the linear predictor \\(\\beta_0+\\beta_1\\cdot x_i\\). In effect, we’re saying that the response data are normally distributed, with the mean represented by the linear model, i.e. not constant but changing as a function of the predictor \\(x_i\\): \\[\\begin{equation} y_i\\sim N\\left(\\beta_0+\\beta_1\\cdot x_i,\\sigma\\right) \\tag{6.4} \\end{equation}\\] Put differently, Equation (6.4) arises from combining the linear model \\(y_i=\\beta_0+\\beta_1\\cdot x_i+\\epsilon_i\\) with the normality assumption for the residuals \\(\\epsilon_i\\sim N(0,\\sigma)\\). Note, the mean of the residual distribution is zero because - based on our fundamental assumption that the model is correct - on average we expect no deviation from the regression line. Please spend some time understanding how the likelihood function is constructed - this is useful for understanding many advanced techniques later on. On our way to construct the maximum likelihood estimates, getting rid of the product operator in Equation (6.3) yields: \\[\\begin{equation} L(\\beta_0,\\beta_1,\\sigma|\\mathbf{y})=\\frac{1}{\\left(\\sigma\\cdot\\sqrt{2\\cdot\\pi}\\right)^n}\\cdot\\exp\\left(\\frac{-1}{2\\cdot\\sigma^2}\\cdot\\sum_{i=1}^{n}\\left(y_i-\\beta_0-\\beta_1\\cdot x_i\\right)^2\\right) \\tag{6.5} \\end{equation}\\] Compare exercises in chapter 2. The log-likelihood is often mathematically easier to handle, while locations of maxima (this is all about maximum likelihood) remain unchanged: \\[\\begin{equation} \\log L(\\beta_0,\\beta_1,\\sigma|\\mathbf{y})=\\log\\left(\\sigma^{-n}\\cdot (2\\cdot\\pi)^{-\\frac{n}{2}}\\right)-\\frac{1}{2\\cdot\\sigma^2}\\cdot\\sum_{i=1}^{n}\\left(y_i-\\beta_0-\\beta_1\\cdot x_i\\right)^2 \\tag{6.6} \\end{equation}\\] \\[\\begin{equation} \\log L(\\beta_0,\\beta_1,\\sigma|\\mathbf{y})=-n\\cdot \\log (\\sigma)-\\frac{n}{2}\\cdot\\log(2\\cdot\\pi)-\\frac{1}{2\\cdot\\sigma^2}\\cdot\\sum_{i=1}^{n}\\left(y_i-\\beta_0-\\beta_1\\cdot x_i\\right)^2 \\tag{6.7} \\end{equation}\\] Compare logarithm calculus of chapter 2. The maximum likelihood is where all partial derivatives with respect to the parameters are zero: \\(\\frac{\\partial\\log L}{\\partial \\beta_0}=0\\) and \\(\\frac{\\partial\\log L}{\\partial \\beta_1}=0\\) and \\(\\frac{\\partial\\log L}{\\partial \\sigma}=0\\). This yields: \\[\\begin{equation} \\frac{\\partial\\log L\\left(\\beta_0,\\beta_1,\\sigma\\right)}{\\partial \\beta_0}=\\frac{1}{\\sigma^2}\\cdot \\sum_{i=1}^{n}\\left(y_i-\\beta_0-\\beta_1 \\cdot x_i\\right)=0 \\tag{6.8} \\end{equation}\\] \\[\\begin{equation} \\frac{\\partial\\log L\\left(\\beta_0,\\beta_1,\\sigma\\right)}{\\partial \\beta_1}=\\frac{1}{\\sigma^2}\\cdot \\sum_{i=1}^{n}x_i\\cdot\\left(y_i-\\beta_0-\\beta_1 \\cdot x_i\\right)=0 \\tag{6.9} \\end{equation}\\] Hence, the maximum likelihood estimator for \\(\\beta_0\\) and \\(\\beta_1\\) under normal residuals is identical to the Least Squares parameter estimator (Equations (3.11) and (3.12) in chapter 3). For \\(\\sigma\\), we have: \\[\\begin{equation} \\frac{\\partial\\log L\\left(\\beta_0,\\beta_1,\\sigma\\right)}{\\partial \\sigma}=-\\frac{n}{\\sigma}+\\frac{1}{\\sigma^3}\\cdot\\sum_{i=1}^{n}\\left(y_i-\\beta_0-\\beta_1 \\cdot x_i\\right)^2=0 \\tag{6.10} \\end{equation}\\] \\[\\begin{equation} \\frac{\\partial\\log L\\left(\\beta_0,\\beta_1,\\sigma\\right)}{\\partial \\sigma}=-n\\cdot\\sigma^2+\\sum_{i=1}^{n}\\left(y_i-\\beta_0-\\beta_1 \\cdot x_i\\right)^2=0 \\tag{6.11} \\end{equation}\\] This yields the estimator: \\[\\begin{equation} \\sigma=\\sqrt{\\frac{SSE}{n}} \\tag{6.12} \\end{equation}\\] Note, the Least Squares estimator is \\(s=\\sqrt{\\frac{SSE}{df_{SSE}}}=\\sqrt{\\frac{SSE}{n-2}}\\), which doesn’t make much of a difference for large \\(n\\). Now it should be clear that the assumptions underpinning linear regression come from maximum likelihood theory; even if parameter estimators can be motivated via Least Squares, their standard errors, confidence intervals and significance tests rely on the assumptions that the residuals be independent and identically distributed according to a normal distribution (“iid normal”). In chapter 7 we will see how we can expand these assumptions, by making other distributional choices in Equation (6.4) as well as transformations of the linear model inside those distributions. We will effectively construct different likelihood functions - different formulations of \\(\\Pr(y_i|\\boldsymbol{\\theta})\\) in Equation (6.2) - motivated by our conceptualisation of the process that generates the response data at hand. 6.2 Outlook: Bayesian inference Bayesian statistics is based on a different philosophical understanding of probability than classic (so called frequentist) statistics, even if both share the same probability calculus. In frequentist statistics, probability is a long-run relative frequency. For example, if we toss a fair coin a thousand times then we will see approximately 500 heads and 500 tails; we say the probability of heads is \\(\\frac{500}{1000}=0.5\\). Of course 1000 tosses is not really enough to approach 0.5, so probability in this sense is defined mathematically as the limit when \\(n\\), the number of tosses in the example, goes to infinity. In Bayesian statistics9, probability is a degree of plausibility of a proposition, like that the coin will come up heads in our example. This degree of plausibility is informed by some observed (long-run) behaviour, like repeated tossing of the coin, but also other sources, like physical reasoning about the coin. In simple games of chance like coin tossing - processes that can be repeated a large number of times - it’s hard to see the philosophical difference between the two types of probability and the two types of statistics. Where the difference is clearer - and important - is in the uncertain information we construct around statistical estimates, i.e. confidence intervals in frequentist statistics. Most important, however, are the many cases where there is no long-run relative frequency at all (or we cannot observe it). Consider, for example, the probability of exceeding a 1.5 degree Celsius rise in global average temperature by the end of this century; this is not a frequentist probability but a Bayesian one - a degree of plausibility given some data, models and other information that go into these kinds of assessments. 6.2.1 Frequentist sampling distributions Now on to uncertainty estimates, where I said the differences between frequentist and Bayesian probability matters. Frequentist estimates like means, test statistics (t-test, F-test, …) and regression parameters all come with so called sampling distributions; probability density functions (PDFs) that describe the variation in those estimates if the estimation procedure were repeated an infinite number of times. These are PDFs in a long-run relative frequency sense. For the mean, if the population from which data \\(x\\) are a sample (of size \\(n\\)) is normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), we can show mathematically that the deviation of the estimate \\(\\hat \\mu=\\bar x\\) from the unknown \\(\\mu\\), scaled by the standard error \\(s_{\\hat \\mu}=\\frac{s}{\\sqrt{n}}\\) (with \\(s=\\sqrt{\\frac{1}{n-1}\\cdot\\sum_{i=1}^{n}\\left(x_i-\\bar x\\right)^2}\\) being the standard deviation of \\(x\\)) would follow a t-distribution in repeated sampling with parameter \\(n-1\\): \\[\\begin{equation} \\frac{\\hat \\mu-\\mu}{s_{\\hat \\mu}}\\sim t_{n-1} \\tag{6.13} \\end{equation}\\] For the t-test statistic, the sampling distribution is similar. If two samples came from normal populations with identical means, the Null hypothesis of the t-test, then the scaled difference between the two mean estimates would follow a t-distribution with parameter \\(n_1+n_2-2\\) in repeated sampling: \\[\\begin{equation} t_s=\\frac{\\hat \\mu_1-\\hat \\mu_2}{\\sqrt{s_{\\hat\\mu_1}^2+s_{\\hat\\mu_2}^2}}\\sim t_{n_1+n_2-2} \\tag{4.6} \\end{equation}\\] The F-test statistic, the ratio of two variance estimates \\(\\hat\\sigma_1\\) and \\(\\hat\\sigma_2\\), would follow a F-distribution10 in repeated sampling if the two samples came from normal populations with identical variances (the Null hypothesis of the F-test): \\[\\begin{equation} F_s=\\frac{\\hat \\sigma_1^2}{\\hat \\sigma_2^2}\\sim F_{n_1-1;n_2-1} \\tag{6.14} \\end{equation}\\] Finally, the regression parameter estimates too would vary around the true parameter value according to a t-distribution in repeated sampling if the residuals were normally distributed (see chapter 6.1): \\[\\begin{equation} \\frac{\\hat \\beta-\\beta}{s_{\\hat \\beta}}\\sim t_{n-2} \\tag{6.15} \\end{equation}\\] Based on these sampling distributions we can now construct confidence intervals (and p-values for tests), which we will only do here for the case of regression parameters, repeating what we did in Chapter 3: \\[\\begin{equation} \\Pr\\left(\\hat\\beta-t_{n-2;0.975} \\cdot s_{\\hat\\beta}\\leq \\beta\\leq \\hat\\beta+t_{n-2;0.975} \\cdot s_{\\hat\\beta}\\right)=0.95 \\tag{6.16} \\end{equation}\\] As mentioned in Chapter 3, this is the central interval in which the true parameter value \\(\\beta\\) lies with a probability of 0.95. But this is a frequentist probability, meaning that in an assumed infinite number of regression experiments the 95% confidence interval captures the true parameter value in 95% of the cases. It is some measure of confidence, but not a probability of the true parameter value lying within the confidence interval for any one experiment. This, by contrast, is what the Bayesian approach provides, as we will see next. 6.2.2 Bayesian posterior distributions The Bayesian approach gives us an actual probability density function (PDF) of the parameters \\(\\boldsymbol{\\theta}\\), i.e. degrees of plausibility for different values of these parameters. This is the so called posterior distribution \\(\\Pr(\\boldsymbol{\\theta}|\\mathbf{y})\\), i.e. the probability distribution of the parameters conditional on the data \\(\\mathbf{y}\\) at hand. Posterior here means “after seeing the data”. We get the posterior distribution from Bayes rule:11 \\[\\begin{equation} \\Pr(\\boldsymbol{\\theta}|\\mathbf{y})=\\frac{\\Pr(\\mathbf{y}|\\boldsymbol{\\theta})\\cdot\\Pr(\\boldsymbol{\\theta})}{\\int\\Pr(\\mathbf{y}|\\boldsymbol{\\theta})\\cdot\\Pr(\\boldsymbol{\\theta})\\;d\\boldsymbol{\\theta}} \\tag{6.17} \\end{equation}\\] Bayes rule involves the likelihood function \\(\\Pr(\\mathbf{y}|\\boldsymbol{\\theta})\\), which we already know from maximum likelihood estimation (Equation (6.1)). But this time the complete likelihood function is used, not just its maximum. Bayes rule also requires us to specify a probability distribution of the parameters unconditioned on the data, the so called prior distribution \\(\\Pr(\\boldsymbol{\\theta})\\). The denominator in Equation (6.17) can be viewed simply as a normalising constant and we don’t have to worry about it much. The likelihood function is the same that we would use in the frequentist approach - just that we use it fully here. So for a linear model with assumed iid normal residuals the likelihood function is Equation (6.3). We will see other choices in Chapter 7. The prior distribution is the only new choice and requires some thought. The prior is meant to capture our uncertainty about plausible parameter values before considering the data at hand. Ideally, this is informed by previous experience and can thus be “informative”, i.e. the PDF is narrowly centred on certain values. If we don’t have any clue about plausible parameter values then we might use an “uninformative” prior, e.g. a uniform distribution over the real line or some plausible range. For simple problems this gives the same results as maximum likelihood estimation, but with the different meaning of probability discussed above. Using uniform priors can, however, be numerically unstable. In practice, we will most likely resort to “weakly informative” priors in regression problems, e.g. wide normal distributions for the parameters centred on zero. Weakly informative means that we need moderately strong evidence in the data to pull the parameter estimates away from zero (no effect), which is an efficient measure against overfitting. What happens in Bayesian inference can be illustrated with Figure 6.1: The prior is effectively updated by the likelihood to yield the posterior. The likelihood thereby encodes the information in the data about plausible parameter values, mediated by our model of the data generation process, e.g. the linear model with iid normal residuals to stay with our example of linear regression. Figure 6.1: Bayesian updating: The prior PDF of a hypothetical parameter \\(\\theta\\) (green) is updated by the likelihood function (blue) to yield the posterior PDF (red). We see clearly how the posterior is a compromise between the prior and the likelihood. From the posterior, confidence interval-like metrics can be calculated, though these are called compatibility intervals in Bayesian statistics according to recent terminology.12 This is generally done numerically by sampling from the posterior - we will do this below. Once we’ve got our head round this it’s quite straightforward. And we have direct probabilistic estimates of the parameters, without having to invoke any sampling distributions.13 6.2.3 A Bayesian analysis of the yield dataset Let’s illustrate the Bayesian approach briefly for the yield dataset of Chapter 4. Implementation details, prior choices, model comparison and more complex models will be covered in Applied Statistical Modelling in the summer term. We use the brms package as the interface from R to the Bayesian inference engine Stan:14 # load brms package library(brms) # load yields data yields &lt;- read.table(&quot;data/yields.txt&quot;,header=T) # expand to a long variable &quot;yield&quot; and a index variable &quot;soiltype&quot; yields_long &lt;- data.frame(yield = c(yields$sand, yields$clay, yields$loam), soiltype = as.factor(c(rep(1,10), rep(2,10), rep(3,10)))) # fit linear model using brms with default priors yield_fit &lt;- brm(yield ~ 0 + soiltype, data = yields_long, family = gaussian(), silent = TRUE, refresh = 0) We have fitted the model using default priors so let’s check quickly which these are: # check default priors prior_summary(yield_fit) ## prior class coef group resp dpar nlpar bound source ## (flat) b default ## (flat) b soiltype1 (vectorized) ## (flat) b soiltype2 (vectorized) ## (flat) b soiltype3 (vectorized) ## student_t(3, 0, 4.4) sigma default This rather cryptic output tells us that brms has used flat, i.e. uniform, priors for the three parameters, which are the unique means for the three soil types (compare Chapter 4), and a t-distribution with 3 degrees of freedom, centred on 0 and scaled by 4.4, as prior for \\(\\sigma\\). Let’s look at the parameter estimates: # summarise posterior posterior_summary(yield_fit, pars = c(&#39;soiltype1&#39;,&#39;soiltype2&#39;,&#39;soiltype3&#39;,&#39;sigma&#39;)) ## Estimate Est.Error Q2.5 Q97.5 ## b_soiltype1 9.915792 1.1512475 7.602761 12.228036 ## b_soiltype2 11.505041 1.1349487 9.251969 13.777522 ## b_soiltype3 14.288702 1.1250285 12.044883 16.468347 ## sigma 3.549096 0.5134979 2.710401 4.722174 This output gives us the median of the posterior for each parameter (“Estimate”), the standard error of that estimate via the standard deviation of the posterior (“Est.Error”) and the central 95% compatibility interval between the bounds “Q2.5” and “Q97.5”. The central parameter estimates are essentially the same as in the frequentist approach (Chapter 4). The standard errors are slightly higher than the frequentist estimate of 1.08, and not homogeneous. The residual standard deviation, here \\(\\sigma\\), is slightly higher than the frequentist estimate of \\(\\sqrt{\\frac{SSE}{n-k}}=\\sqrt{11.7}=3.4\\). These are just summaries. To get a sense of the full posterior we need to extract the numerical samples of the posterior and then we can plot these as histograms, for example. Note, these are discrete approximations of the posterior PDFs, which already for moderately complex problems don’t exist in closed form, so working with samples is the most general and often the only option. # extract posterior samples s &lt;- posterior_samples(yield_fit, pars = c(&#39;soiltype1&#39;,&#39;soiltype2&#39;,&#39;soiltype3&#39;,&#39;sigma&#39;)) # plot parameter posteriors as histograms hist(s$b_soiltype1, freq = FALSE) hist(s$b_soiltype2, freq = FALSE) hist(s$b_soiltype3, freq = FALSE) The beauty now is that we can calculate a probability distribution of the average yield differences between soil types, without having to worry about the two-sample t-test we did back in Chapter 4 and its sampling distribution and all that. It’s very simple: # plot average yield differences between soil types as histograms hist(s$b_soiltype2-s$b_soiltype1, freq = FALSE) hist(s$b_soiltype3-s$b_soiltype1, freq = FALSE) hist(s$b_soiltype3-s$b_soiltype2, freq = FALSE) # express these differences as median and 95% compatibility interval quantile(s$b_soiltype2-s$b_soiltype1, probs = c(0.025, 0.5, 0.975)) ## 2.5% 50% 97.5% ## -1.439621 1.557710 4.715988 quantile(s$b_soiltype3-s$b_soiltype1, probs = c(0.025, 0.5, 0.975)) ## 2.5% 50% 97.5% ## 1.120923 4.379288 7.535135 quantile(s$b_soiltype3-s$b_soiltype2, probs = c(0.025, 0.5, 0.975)) ## 2.5% 50% 97.5% ## -0.3361721 2.7717930 5.8957226 We see that the only yield difference that is uniquely positive at the 95% compatibility level is that between soil types 3 and 1 - sand and loam. All other compatibility intervals overlap with zero meaning there are sizable probabilities of the differences going in either direction. In frequentist language we would call these differences insignificant, but looking at the full posterior distribution of the differences is much more useful than the binary split. All in all, this example shows how for simple models frequentist and Bayesian inference with uninformative priors yield essentially the same conclusions, although the Bayesian probabilities are much more intuitive to interpret and easier to post-process into any quantities of interest. Bayesian statistics is named after 18th century Presbyterian minister Thomas Bayes, who conducted a famous inferential experiment by applying what was later called Bayes rule. This type of inferential reasoning, however, predates Bayes and there were more influential figures since, but somehow the name stuck. Not even Bayes rule is anything special; it arises simply from rearranging the product rule of probability calculus.↩︎ These tests got their names from the sampling distributions of their test statistics.↩︎ As said previously, Bayes rule is just the rearranged product rule of basic probability calculus: \\(\\Pr(A,B)=\\Pr(A|B)\\cdot\\Pr(B)=\\Pr(B|A)\\cdot\\Pr(A)\\)↩︎ Gelman, Hill, and Vehtari (2020); alternative terms are “uncertainty intervals” or, somewhat outdated, “credible intervals”.↩︎ Note, another problem with frequentist statistics is that, even if sampling distributions may provide useful approximations of real-world uncertainties, already for moderately complex models there exist no closed-form sampling distributions. Here the Bayesian approach, and its numerical sampling, are much more general.↩︎ Other packages are available, most notably rstanarm, which is a little more intuitive but less comprehensive.↩︎ "],
["glms.html", "Chapter 7 Generalised Linear Models (GLMs) 7.1 Generalising the normal distribution through the exponential class of probability functions 7.2 The link function between mean response and predictor variables 7.3 Log-linear regression 7.4 Logistic regression 7.5 Goodness of fit of GLMs 7.6 Over-dispersion", " Chapter 7 Generalised Linear Models (GLMs) Up to this point we have looked at the linear model \\[\\begin{equation} y_i = \\beta_0 + \\sum_{j=1}^{p}\\beta_j \\cdot x_{ij} + \\epsilon_i \\tag{7.1} \\end{equation}\\] with potentially multiple predictor variables \\(x_j\\) that could be continuous, categorical or mixed. If \\(\\epsilon_i\\sim N\\left(\\mu_i=0,\\sigma\\right)\\) then: \\[\\begin{equation} y_i\\sim N\\left(\\mu_i=\\beta_0 + \\sum_{j=1}^{p}\\beta_j \\cdot x_{ij},\\sigma\\right) \\tag{7.2} \\end{equation}\\] So in effect we have modelled the response variable \\(y_i\\) as realisations from a normal distribution whose mean \\(\\mu_i\\) (the mean response) is not a constant but a linear function of the predictors, hence the index \\(i\\). In this chapter - with Generalised Linear Models (GLMs) - we generalise this model to allow: Distributions other than the normal distribution in Equation (7.2) Transformations of the linear function linking predictors and mean response inside that distribution This is important as it allows to to deal with a number of shortcomings of linear regression. Equation (7.2) implies that \\(y_i\\) are continuous observations between \\(-\\infty\\) and \\(\\infty\\), which is the so called support space of the normal distribution (Figure 7.1, left). Equation (7.2) further implies that the variance \\(Var\\left(y_i\\right)=\\sigma^2\\) remains constant with changing mean response \\(E\\left[y_i\\right]=\\mu_i\\) (Figure 7.1, right).15 library(&quot;latex2exp&quot;) # generate y values from -5 to 5 in increments of 0.1 y &lt;- seq(-5,5,0.1) # plot pdf of normal distribution with mu=0 and sigma=1 for y plot(y, dnorm(y, mean=0, sd=1), type=&#39;l&#39;, xlab=&#39;y&#39;, ylab=&#39;pdf&#39;) # plot schematic of constant variance for changing mean response plot(c(-5,5), c(0,0), type=&#39;l&#39;, xlim=c(-5,5), yaxt=&#39;n&#39;, xlab=TeX(&#39;mean response $E\\\\left[y_i\\\\right]$&#39;), ylab=TeX(&#39;variance $Var\\\\left(y_i\\\\right)$&#39;)) Figure 7.1: Left: Probability density function (pdf) of the normal distribution of a response variable \\(y\\) (for this schematic the standard normal distribution \\(N(\\mu=0,\\sigma=1)\\) is shown). Right: In linear regression the variance of \\(y\\) doesn’t change with the mean response of \\(y\\) as Equation (7.2) implies. These two assumption, however, often don’t hold for geographical data, e.g. in case of: Continuous observations restricted to positive outcomes Data bounded over a fixed interval Non-constant variance Discrete data such as counts and proportions To some extent these shortcomings can be solved by transforming the data. But often it will be better to extend the linear model in the form of a GLM. In this course we use GLMs that are based on mathematics similar to the linear model to estimate the parameters, so we don’t go into the particularities here. In my course Applied Statistical Modelling in the summer term we extend the realm of possible model choices even further. In this chapter we will deal with two examples, one of count data and another of proportion data. The count data example is from Piegorsch and Bailer (2005); it models the number of bird species on the Indian subcontinent as a function of average altitudinal relief (Figure 7.2, left).16 These data are bounded below; we cannot have counts less than zero. The residual variance is non-constant (Figure 7.2, right). The residual error is non-normal. And the data require a discrete distribution. # load birds data birds &lt;- read.table(&quot;data/birds.txt&quot;,header=T) # plot plot(birds$relief, birds$y, pch = 19, type = &#39;p&#39;, xlim=c(0,5), ylim=c(0,600), xlab=&#39;Average relief&#39;, ylab=&#39;Number of bird species&#39;) # plot schematic of increasing variance for increasing mean response plot(c(0,600), c(-1,1), type=&#39;l&#39;, xlim=c(0,600), ylim=c(-2,2), yaxt=&#39;n&#39;, xlab=TeX(&#39;mean response $E\\\\left[y_i\\\\right]$&#39;), ylab=TeX(&#39;variance $Var\\\\left(y_i\\\\right)$&#39;)) Figure 7.2: Left: Number of bird species as a function of average relief. Data from: Piegorsch and Bailer (2005). Right: In this example the variance of \\(y\\) increases with the mean response of \\(y\\). The proportion data example is from Dormann (2013); it models the proportion of organisms surviving as a function of the concentration of a toxic substance (Figure 7.3, left). These data are strictly bounded; we cannot have a proportion greater than 1 or less than 0. The residual variance is non-constant (Figure 7.3, right). And the residual error is non-normal; confidence intervals are asymmetric whenever predictions are large (close to 1) or small (close to 0). # load survival data survival &lt;- read.table(&quot;data/survival.txt&quot;,header=T) # these data are given in the form &quot;trials&quot; (here total no. organisms) # and &quot;successes&quot; (here no. organisms surviving) # so transform into proportions survival$proportion &lt;- survival$surviving / survival$total # plot plot(survival$concentration, survival$proportion, pch = 19, type = &#39;p&#39;, xlim=c(0,4), ylim=c(0,1), xlab=&#39;Concentration (mg/l)&#39;, ylab=&#39;Proportion&#39;) # plot schematic of variance peaking at average mean response plot(seq(-5,5,0.1), -seq(-5,5,0.1)^2, type=&#39;l&#39;, xlim=c(-5,5), ylim=c(-25,0), xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, xlab=TeX(&#39;mean response $E\\\\left[y_i\\\\right]$&#39;), ylab=TeX(&#39;variance $Var\\\\left(y_i\\\\right)$&#39;)) axis(side=1, at=seq(-5,5,2),labels=c(&quot;0&quot;,&quot;0.2&quot;,&quot;0.4&quot;,&quot;0.6&quot;,&quot;0.8&quot;,&quot;1&quot;)) Figure 7.3: Left: Proportion of organisms surviving as a function of the concentration of a toxic substance. Data from: Dormann (2013). Right: In this example the variance of \\(y\\) peaks at the average mean response of \\(y\\). 7.1 Generalising the normal distribution through the exponential class of probability functions In this section we see how the normal distribution of linear regression can be generalised via the exponential class of probability functions. The normal distribution is a member of this class and so are other distributions, like the Poisson and the binomial that we need for our examples. It can then be shown - which we don’t do here - that the mathematics of linear regression can be extended to those other distributional assumptions. This is not to say that distributions that are not in the exponential class cannot be used for regression analysis; it’s just that such regression problems are not so straightforward to solve in classical statistics.17 The exponential class of probability functions is formalised as: \\[\\begin{equation} f(y)=\\exp\\left\\{\\frac{y\\cdot\\theta-\\color{red}{B(\\theta)}}{\\color{green}{A(\\varphi)}}+\\color{blue}{C(y,\\varphi)}\\right\\} \\tag{7.3} \\end{equation}\\] With the additional constraint that the support space \\(\\mathbb{S}\\) of \\(y\\) cannot depend on \\(\\theta\\). \\(\\color{green}{A(\\varphi)}\\), \\(\\color{red}{B(\\theta)}\\) and \\(\\color{blue}{C(y,\\varphi)}\\) are functions that take on different forms for different members of the exponential class. The parameter \\(\\theta\\) is called the natural parameter, and the parameter \\(\\varphi&gt;0\\) is called the dispersion (or scale) parameter. We can now transform the pdf of the normal distribution (Figure 7.1, left) to the form of Equation (7.3) to demonstrate how the normal distribution is a member of the exponential class, and which the natural and dispersion parameters are in this case: \\[\\begin{equation} f(y)=\\frac{1}{\\sigma\\cdot\\sqrt{2\\cdot\\pi}}\\cdot\\exp{-\\frac{(y-\\mu)^2}{2\\cdot\\sigma^2}}\\cdot I_{(-\\infty,\\infty)}(y) \\tag{7.4} \\end{equation}\\] \\[\\begin{equation} f(y)=\\exp\\left\\{\\frac{y\\cdot\\mu-\\color{red}{\\frac{1}{2}\\cdot\\mu}}{\\color{green}{\\sigma^2}}\\color{blue}{-\\frac{y^2}{2\\cdot\\sigma^2}-\\frac{1}{2}\\cdot\\log\\left(2\\cdot\\pi\\cdot\\sigma^2\\right)+\\log\\left[I_{(-\\infty,\\infty)}(y)\\right]}\\right\\} \\tag{7.5} \\end{equation}\\] Note, the indicator function \\(I_{(-\\infty,\\infty)}(y)\\) is 1 when \\(y\\) is between \\(-\\infty\\) and \\(\\infty\\) and 0 otherwise, indicating the support space \\(\\mathbb{S}\\). By comparing Equation (7.3) and Equation (7.5) we can see that for the normal distribution the natural parameter \\(\\theta\\) is \\(\\mu\\) and the dispersion parameter \\(\\varphi\\) is \\(\\sigma\\). The mean response is \\(E[y]=\\mu\\). Next we see how the Poisson distribution (Figure 7.4, left), which we will need for count data, is also a member of the exponential class. The pdf of the Poisson distribution is: \\[\\begin{equation} f(y)=\\frac{\\lambda^y\\cdot\\exp(-\\lambda)}{y!}\\cdot I_{(0,1,\\ldots,n)}(y) \\tag{7.6} \\end{equation}\\] \\[\\begin{equation} f(y)=\\exp\\left\\{y\\cdot\\log(\\lambda)-\\color{red}{\\lambda}\\color{blue}{-\\log(y!)+\\log\\left[y!\\cdot I_{(0,1,\\ldots,n)}(y)\\right]}\\right\\} \\tag{7.7} \\end{equation}\\] Note, the symbol \\(!\\) stands for the factorial.18 The support space \\(\\mathbb{S}\\) is here restricted to positive whole numbers. By comparing Equation (7.3) and Equation (7.7) we can see that for the Poisson distribution the natural parameter \\(\\theta\\) is \\(\\log(\\lambda)\\) and the dispersion parameter \\(\\varphi\\) is 1; the denominator in Equation (7.3) has vanished in Equation (7.7). The mean response is \\(E[y]=\\lambda\\). The one parameter of the Poisson distribution \\(\\lambda\\) controls both the mean as well as the variance of the distribution, i.e. if the mean goes up then the variance goes up too (Figure 7.4, left). Not only is the Poisson distribution, due to its discreteness and positive support space, a natural distribution for count data, it can also capture the increase in variance with increase in mean response that we often see in count data such as our bird species example (Figure 7.2). Figure 7.4: Left: Probability density function (pdf) of the Poisson distribution of a response variable \\(y\\) for three variants of the parameter \\(\\lambda\\). Right: Probability density function (pdf) of the binomial distribution of a response variable \\(y\\), normalised by the number of “trials” \\(n\\), for three variants of the parameter \\(\\pi\\). Lastly, the binomial distribution (Figure 7.4, right), which we will need for proportion data, is also a member of the exponential class. The pdf of the binomial distribution is: \\[\\begin{equation} f(y)=\\binom{n}{y}\\cdot\\pi^y\\cdot (1-\\pi)^{n-y}\\cdot I_{(0,1,\\ldots,n)}(y) \\tag{7.8} \\end{equation}\\] \\[\\begin{equation} f(y)=\\exp\\left\\{y\\cdot\\log(\\frac{\\pi}{1-\\pi})\\color{red}{+n\\cdot\\log(1-\\pi)}+\\color{blue}{\\log\\left[\\binom{n}{y}\\cdot I_{(0,1,\\ldots,n)}(y)\\right]}\\right\\} \\tag{7.9} \\end{equation}\\] Note, \\(\\binom{n}{y}\\) stands for the binomial coefficient.19 The support space \\(\\mathbb{S}\\) is again restricted to positive whole numbers. By comparing Equation (7.3) and Equation (7.9) we can see that for the binomial distribution the natural parameter \\(\\theta\\) is \\(\\log(\\frac{\\pi}{1-\\pi})\\), the so called logit function or log-odds20 and the dispersion parameter \\(\\varphi\\) is again 1. The mean response is \\(E[\\frac{y}{n}]=\\pi\\). Note, here we normalise the response \\(y\\) by the number of “trials” \\(n\\) to deal with proportions, while the binomial distribution per se is a count data distribution. The one parameter of the binomial distribution \\(\\pi\\) controls the mean, the variance as well as the shape of the distribution (Figure 7.4, right). It thus captures the change in variance with change in mean response as well as the asymmetry of the response distribution near 1 and 0 that we see in strictly bounded data such as our survival proportions example (Figure 7.3). 7.2 The link function between mean response and predictor variables Now we come to the second move of GLMs, the link function between mean response and predictor variables. To understand this, we first introduce the symbol \\(\\eta_i\\) as a shortcut for the linear predictor: \\[\\begin{equation} \\eta_i = \\beta_0 + \\sum_{j=1}^{p}\\beta_j \\cdot x_{ij} \\tag{7.10} \\end{equation}\\] The link function \\(g(\\cdot)\\) then relates the linear predictor to the mean response: \\[\\begin{equation} \\eta_i = g\\left(E\\left[y_i\\right]\\right) \\tag{7.11} \\end{equation}\\] Remember that in linear regression the linear predictor was simply equated with the parameter \\(\\mu_i\\) (Equation (7.2)), which is just the mean response \\(E\\left[y_i\\right]\\) of the normal model. This is called the identity link: \\(\\eta_i=E\\left[y_i\\right]=\\mu_i\\). The inverse link function \\(g^{-1}(\\cdot)\\) then solves the same equation for the mean response: \\[\\begin{equation} E\\left[y_i\\right]=g^{-1}\\left(\\eta_i\\right) \\tag{7.12} \\end{equation}\\] This formulation of the mean response is eventually entered into the assumed response distribution, e.g. in case of the normal distribution (compare Equation (7.2)): \\(y_i\\sim N\\left(E\\left[y_i\\right]=\\mu_i=\\eta_i,\\sigma\\right)\\). Note, it’s only ever the inverse link-function that is mathematically used in the model, not the link function itself. The choice of link function is best motivated by our mechanistic understanding of the process to be modelled. If not, then the natural parameter \\(\\theta\\) - whatever it is for the response distribution we’re using - may be chosen as the link function – this is called the canonical link function (Table 7.1). Table 7.1: Canonical link functions. Response distribution Canonical link function Normal Identity-link: \\(\\eta_i=\\mu_i\\) Poisson Log-link: \\(\\eta_i=\\log\\left(\\lambda_i\\right)\\) Binomial Logit-link: \\(\\eta_i=\\log\\left\\{\\frac{\\pi_i}{1-\\pi_i}\\right\\}\\) The resultant GLMs are compared in Table 7.2. Table 7.2: Components of three widely used GLMs. Normal Poisson Binomial Linear predictor \\(\\eta_i = \\beta_0 + \\sum_{j=1}^{p}\\beta_j \\cdot x_{ij}\\) \\(\\eta_i = \\beta_0 + \\sum_{j=1}^{p}\\beta_j \\cdot x_{ij}\\) \\(\\eta_i = \\beta_0 + \\sum_{j=1}^{p}\\beta_j \\cdot x_{ij}\\) Response distribution \\(N\\left(\\mu_i,\\sigma\\right)\\) \\(Pois\\left(\\lambda_i\\right)\\) \\(B\\left(n,\\pi_i\\right)\\) Mean response \\(E\\left[y_i\\right]=\\mu_i\\) \\(E\\left[y_i\\right]=\\lambda_i\\) \\(E\\left[\\frac{y_i}{n}\\right]=\\pi_i\\) Link function \\(\\eta_i=\\mu_i\\) \\(\\eta_i=\\log\\left(\\lambda_i\\right)\\) \\(\\eta_i=\\log\\left\\{\\frac{\\pi_i}{1-\\pi_i}\\right\\}\\) Inverse link function \\(\\mu_i=\\eta_i\\) \\(\\lambda_i=\\exp(\\eta_i)\\) \\(\\pi_i=\\frac{\\exp\\left(\\eta_i\\right)}{1+\\exp\\left(\\eta_i\\right)}\\) GLM \\(N\\left(\\eta_i,\\sigma\\right)\\) \\(Pois\\left(\\exp\\left(\\eta_i\\right)\\right)\\) \\(B\\left(n,\\frac{\\exp\\left(\\eta_i\\right)}{1+\\exp\\left(\\eta_i\\right)}\\right)\\) Linear regression Log-linear regression Logistic regression 7.3 Log-linear regression Regression analysis with the Poisson distribution and log-link is called log-linear regression. Let’s look at this for the bird species dataset (Figure 7.5, left). Note that it is the linear predictor \\(\\eta_i\\) that gets transformed non-linearly (through \\(g^{-1}(\\cdot)\\)), not the data \\(y_i\\)! Nevertheless, we can transform the data (through \\(g(\\cdot)\\)) to get an idea of what happens (Figure 7.5, right). # fit log-linear model to birds data birds_fit &lt;- glm(y ~ relief, family = poisson(link = &quot;log&quot;), data = birds) # plot data plot(birds$relief, birds$y, pch = 19, type = &#39;p&#39;, xlim=c(0,5), ylim=c(0,600), xlab=&#39;Average relief&#39;, ylab=&#39;Number of bird species&#39;) # extract modelled mean response and plot on top of data newdat &lt;- data.frame(relief=seq(0,5,0.01)) y_new &lt;- predict(birds_fit, newdata=newdat, type = &quot;response&quot;) lines(newdat$relief, y_new, lwd = 3, col = &quot;red&quot;) # plot data on log-scale plot(birds$relief, log(birds$y), pch = 19, type = &#39;p&#39;, xlim=c(0,5), ylim=c(4.5,6.5), xlab=&#39;Average relief&#39;, ylab=&#39;log(Number of bird species)&#39;) # extract modelled mean linear predictor and plot on top of log-data logy_new &lt;- predict(birds_fit, newdata=newdat, type = &quot;link&quot;) lines(newdat$relief, logy_new, lwd = 3, col = &quot;red&quot;) Figure 7.5: Left: Bird species data with fitted log-linear model. The linear predictor is transformed via the inverse log-link to curve upwards. Right: If we transform the response data using the log-link then we can see the linear predictor at the heart of the model. To get a sense of what’s happening here, consider Figure 7.6. In order to represent the variation in the response we could use one overall Poisson distribution (a), which is equivalent to log-linear regression with an intercept only. This clearly doesn’t capture the variation in the data. By introducing “relief” as a predictor we effectively model every relief level with its own Poisson distribution. This is especially easy to show here as the dataset has only four levels (b)-(e). What log-linear regression does is average those individual Poisson distributions to form an overall distribution of the response (f). This is clearly better at capturing the variation in the data, the predictor “relief” explains some of that variation, but the model is still underestimating the increase in variance with increasing mean response; this is especially visible in (d)-(e). We say the data are over-dispersed with respect to the Poisson process, a topic we will come back to in Chapter 7.6. Figure 7.6: Illustration of log-linear regression with the birds example. (a) One overall Poisson distribution fitted to the data; equivalent to log-linear regression with an intercept only. This clearly doesn’t capture the variation in the data. (b)-(e) Individual Poisson distributions for the four relief levels (compare Figure 7.5) as estimated by log-linear regression with relief as predictor. (f) Overall distribution estimated by the log-linear model, obtained by averaging the four individual distributions. This is clearly better at capturing the variation in the data, the predictor “relief” explains some of that variation, but the model is still underestimating the increase in variance with increasing mean response, e.g. (d)-(e). The data are over-dispersed with respect to the Poisson process (see Chapter 7.6). A multiple regression example of the log-linear model is the number of plant species as a function of biomass (a continuous predictor) and soil pH class (a categorical predictor) which you can find in Crawley (2012), section 14.3, p. 586ff. 7.4 Logistic regression Regression analysis with the binomial distribution and logit-link is called logistic regression. Let’s look at this for the survival dataset (Figure 7.7, left). Again, it is the linear predictor \\(\\eta_i\\) that gets transformed non-linearly (through \\(g^{-1}(\\cdot)\\)), not the data \\(y_i\\), but we can transform the data (through \\(g(\\cdot)\\)) to get an idea of what happens (Figure 7.7, right)! # fit logistic model to survival data # note, this is the proportions &amp; weights formulation; # there are two more ways to code the response survival_fit &lt;- glm(proportion ~ concentration, family = binomial(link = &quot;logit&quot;), data = survival, weights = total) # plot data plot(survival$concentration, survival$proportion, pch = 19, type = &#39;p&#39;, xlim=c(0,4), ylim=c(0,1), xlab=&#39;Concentration (mg/l)&#39;, ylab=&#39;Proportion&#39;) # extract modelled mean response and plot on top of data newdat &lt;- data.frame(concentration=seq(0,4,0.01)) y_new &lt;- predict(survival_fit, newdata=newdat, type = &quot;response&quot;) lines(newdat$concentration, y_new, lwd = 3, col = &quot;red&quot;) # plot data on logit-scale plot(survival$concentration, log(survival$proportion/(1-survival$proportion)), pch = 19, type = &#39;p&#39;, xlim=c(0,4), ylim=c(-4,6), xlab=&#39;Concentration (mg/l)&#39;, ylab=&#39;logit(Proportion)&#39;) # extract modelled mean linear predictor and plot on top of logit-data logity_new &lt;- predict(survival_fit, newdata=newdat, type = &quot;link&quot;) lines(newdat$concentration, logity_new, lwd = 3, col = &quot;red&quot;) Figure 7.7: Left: Survival data with fitted logistic model. The linear predictor is transformed via the inverse logit-link to form an inverted s-shape. Right: If we transform the response data using the logit-link then we can see the linear predictor at the heart of the model. The logistic model also works for presence/absence data of, for example, animal species. This type of data is a binary response variable that can only ever be 0 (absence) or 1 (presence). Nevertheless, we can fit a logistic regression to these data and thus model the probability of presence of the species as a function of some predictors.21 An example is the presence and absence of Bison as a function of forest distance, a dataset that was given to me by Benjamin Bleyl (Figure 7.8). Another example is the presence and absence of the Uta lizard as a function of island perimeter/area ratio which you can find in Quinn and Keough (2002), section 13.2.1, p. 360ff. # load bison data bison &lt;- read.table(&quot;data/bison.txt&quot;,header=T) # plot plot(bison$d2f, bison$occurrence, pch = 19, type = &#39;p&#39;, xlim=c(0,6000), ylim=c(0,1), xlab=&#39;Distance to forest&#39;, ylab=&#39;Bison occurrence&#39;) # fit logistic model # note, this is the factor formulation, # which is the only one that makes sense for presence/absence bison_fit &lt;- glm(occurrence ~ d2f, family = binomial(link = &quot;logit&quot;), data = bison) # extract modelled mean response and plot on top of data newdat &lt;- data.frame(d2f=seq(0,6000,10)) y_new &lt;- predict(bison_fit, newdata=newdat, type = &quot;response&quot;) lines(newdat$d2f, y_new, lwd = 3, col = &quot;red&quot;) # generate binned averages of response # after: https://avehtari.github.io/ROS-Examples/LogitGraphs/logitgraphs.html K &lt;- 60 bins &lt;- as.numeric(cut(bison$d2f, K)) x_bar &lt;- rep(NA, K) y_bar &lt;- rep(NA, K) for (k in 1:K){ x_bar[k] &lt;- mean(bison$d2f[bins==k]) y_bar[k] &lt;- mean(bison$occurrence[bins==k]) } # plot plot(bison$d2f, bison$occurrence, pch = 20, type = &#39;p&#39;, col = &#39;gray&#39;, xlim=c(0,6000), ylim=c(0,1), xlab=&#39;Distance to forest&#39;, ylab=&#39;Bison occurrence&#39;) points(x_bar, y_bar, pch = 1) # add modelled mean response lines(newdat$d2f, y_new, lwd = 3, col = &quot;red&quot;) Figure 7.8: Left: Occurrence of bison as a function of forest distance with 1 indicating presence and 0 indicating absence. A logistic regression is fitted to these data that models the probability of occurrence. Note, due to the overlap of 1s and 0s to the left of the graph, the probability of occurrence comes out at only 0.6 for zero distance. Right: This can be better seen when plotting so called binned averages of the occurrence indicator against forest distance (open circles) after Gelman, Hill, and Vehtari (2020). Data from: Benjamin Bleyl. 7.5 Goodness of fit of GLMs Checking goodness of fit of GLMs works a little differently than for linear regression. Let’s look at the output of the log-linear model fit to the birds data: summary(birds_fit) ## ## Call: ## glm(formula = y ~ relief, family = poisson(link = &quot;log&quot;), data = birds) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -11.1980 -2.1407 -0.0021 2.1986 10.7204 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 4.665888 0.021577 216.24 &lt;2e-16 *** ## relief 0.339559 0.008382 40.51 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 2921.1 on 60 degrees of freedom ## Residual deviance: 1299.6 on 59 degrees of freedom ## AIC: 1739.7 ## ## Number of Fisher Scoring iterations: 4 The coefficient information can be interpreted as for linear regression, but the coefficients themselves are given at the scale of the linear predictor (Figure 7.5, right), prior to its transformation through the inverse log-link, the exponential function, in this case. So we need to transform the linear predictor using the exponential function to see the effects of the coefficients on the original scale of the response (Figure 7.5, left). For the mean response we can write: \\[y_i=\\exp\\left(4.67+0.34\\cdot relief_i\\right)=\\exp(4.67)\\cdot\\exp(0.34\\cdot relief_i)=107\\cdot 1.4^{relief_i}\\] So if \\(relief_i=0\\) then \\(y_i=107\\), the intercept (bottom-left corner of Figure 7.5, left), and for every relief increment of 1 we multiply that intercept with 1.4. Hence the exponential increase of the mean response in Figure 7.5, left. For example, increasing relief by five increments (to 5) leads to a number of bird species \\(y_i\\) of \\(107\\cdot 1.4^5=107\\cdot 5.4=578\\) (top-right corner of Figure 7.5, left). The residual deviance in this output requires more thought. It generalises the sum of squared errors (SSE) statistic of linear regression. In Chapter 6 we motivated minimising SSE by maximum likelihood theory and said that if residuals are normally distributed then minimising SSE yields the maximum likelihood estimate of the regression parameters. When using GLMs, when we don’t make the normality assumption anymore, we can’t use SSE either as a goodness of fit measure. Instead we use residual deviance, which includes SSE as a special case when the response distribution is normal. The residual deviance is defined as: \\[\\begin{equation} D=-2\\cdot\\left\\{logL\\left(\\boldsymbol{\\hat\\theta}|\\mathbf{y}\\right)-logL\\left(\\boldsymbol{\\hat\\theta^{sat}}|\\mathbf{y}\\right)\\right\\} \\tag{7.13} \\end{equation}\\] \\(logL\\left(\\boldsymbol{\\hat\\theta}|\\mathbf{y}\\right)\\) is the log-likelihood of the maximum likelihood estimate of the model parameters, \\(\\boldsymbol{\\hat\\theta}\\), which is compared by difference to the log-likelihood of the saturated model \\(logL\\left(\\boldsymbol{\\hat\\theta^{sat}}|\\mathbf{y}\\right)\\). Remember from multiple linear regression (Chapter 5) that the saturated model has one parameter per data point and hence gives a perfect fit. Comparing two models by their log-likelihood difference amounts to comparing the models by their likelihood ratio: \\[\\begin{equation} D=-2\\cdot\\log\\left\\{\\frac{L\\left(\\boldsymbol{\\hat\\theta}|\\mathbf{y}\\right)}{L\\left(\\boldsymbol{\\hat\\theta^{sat}}|\\mathbf{y}\\right)}\\right\\} \\tag{7.14} \\end{equation}\\] The factor “-2” in front of the deviance is there to scale the log-likelihood difference so that it follows asymptotically a Chi-squared sampling distribution. See, for example, McElreath (2020) and references therein; and compare our discussion of AIC in Chapter 5. The deviance residuals summarised at the top of the output measure the contribution of each data point to the residual deviance, analogues to squared errors in linear regression. The Null deviance is the log-likelihood difference of the Null model and the saturated model: \\[\\begin{equation} D=-2\\cdot\\left\\{logL\\left(\\boldsymbol{\\hat\\theta^0}|\\mathbf{y}\\right)-logL\\left(\\boldsymbol{\\hat\\theta^{sat}}|\\mathbf{y}\\right)\\right\\} \\tag{7.15} \\end{equation}\\] Remember from Chapter 5 that the Null model has the intercept only in the linear predictor, it’s the overall mean on the scale of the linear predictor (prior to transformation via any inverse link-function). We can then use the ratio of residual deviance to Null deviance as a goodness of fit measure analogues to the \\(r^2\\) statistic of linear regression (compare Chapter 3); this is called pseudo-\\(r^2\\): \\[\\begin{equation} \\text{pseudo-}r^2=1-\\frac{\\text{residual deviance}}{\\text{Null deviance}} \\tag{7.16} \\end{equation}\\] For the birds example this is: pseudo_r2 &lt;- 1 - birds_fit$deviance / birds_fit$null.deviance pseudo_r2 ## [1] 0.5551062 Finally, we need to look at the residuals as we did for linear regression, it’s just that the raw residuals are structured by design for non-normal response distributions, hence they require standardisation. To understand how the raw residuals are structured by design, consider the Poisson model (Figure 7.5, left): Here the residual variance increases with the mean response (Figure 7.9, left), so seeing this structure in the residuals is expected and doesn’t indicate a systematic error in the model. Consider further the binomial model (Figure 7.7, left): Here the residual variance peaks at 0.5 (Figure 7.9, right), which doesn’t indicate a systematic error either. # plot raw residuals of birds model # the raw residuals are called with the type = &#39;response&#39; argument in resid() plot(birds_fit$fitted.values, resid(birds_fit, type = &#39;response&#39;), pch = 19, type = &#39;p&#39;, xlim=c(100,400), ylim=c(-200,200), xlab=&#39;Number of bird species&#39;, ylab=&#39;Residuals&#39;) # plot raw residuals of survival model plot(survival_fit$fitted.values, resid(survival_fit, type = &#39;response&#39;), pch = 19, type = &#39;p&#39;, xlim=c(0,1), ylim=c(-0.15,0.15), xlab=&#39;Proportion&#39;, ylab=&#39;Residuals&#39;) Figure 7.9: Left: Raw residuals of the log-linear model fit to the birds data. The residual variance by design increases with increasing mean response. Right: Raw residuals of the logistic model fit to the survival data. The residual variance peaks at 0.5 by design. In order to correct for the change in residual variance that’s built into these models, a useful standardisation of the raw residuals is by the standard deviation of the prediction at each data point. This gives the so called Pearson residuals: \\[\\begin{equation} \\frac{y_i-\\hat y_i}{\\sqrt{Var\\left(\\hat y_i\\right)}} \\tag{7.17} \\end{equation}\\] Depending on the formula for the standard deviation \\(\\sqrt{Var\\left(\\hat y_i\\right)}\\), the residuals take on different forms for the different response distributions (Table 7.3). Table 7.3: Formula of the Pearson residuals for different response distributions, depending on their respective formula for the standard deviation. Response distribution Standardised residuals (Pearson) Normal \\(\\frac{y_i-\\hat y_i}{\\sigma}=\\frac{y_i-\\mu_i}{\\sigma}\\) Poisson \\(\\frac{y_i-\\hat y_i}{\\sqrt{\\hat y_i}}=\\frac{y_i-\\lambda_i}{\\sqrt{\\lambda_i}}\\) Binomial \\(\\frac{y_i-\\hat y_i}{\\sqrt{\\hat y_i\\cdot\\left[1-\\frac{\\hat y_i}{n_i}\\right]}}=\\frac{y_i-\\pi_i\\cdot n_i}{\\sqrt{\\pi_i\\cdot n_i\\cdot\\left[1-\\pi_i\\right]}}\\) For our examples, we plot the Pearson residuals in Figure 7.10. We see that some of the structure is removed, but not all, though this is hard to judge with so few data points. # plot Pearson residuals of birds model # this uses type = &#39;pearson&#39; in resid() plot(birds_fit$fitted.values, resid(birds_fit, type = &#39;pearson&#39;), pch = 19, type = &#39;p&#39;, xlim=c(100,400), ylim=c(-15,15), xlab=&#39;Number of bird species&#39;, ylab=&#39;Residuals&#39;) # plot Pearson residuals of survival model plot(survival_fit$fitted.values, resid(survival_fit, type = &#39;pearson&#39;), pch = 19, type = &#39;p&#39;, xlim=c(0,1), ylim=c(-1.5,1.5), xlab=&#39;Proportion&#39;, ylab=&#39;Residuals&#39;) Figure 7.10: Left: Pearson residuals of the log-linear model fit to the birds data. Compared to Figure 7.9, left, the increase in residual variance with increasing mean response is dampened. Right: Pearson residuals of the logistic model fit to the survival data. Compared to Figure 7.9, right, the peak in residual variance at 0.5 is attenuated. 7.6 Over-dispersion The residuals of the log-linear model of the birds data still show a pattern; after standardisation they still increase with increasing mean response (Figure 7.10, left). Remember that in the Poisson model the variance is modelled to increase 1:1 with the mean response (Figure 7.6). If we see the actual variance in our birds data increasing at a greater rate, then we say the data are over-dispersed. In such cases we might use the so called negative-binomial model which has two parameters: \\(\\mu\\), which is equal to the mean, and \\(\\theta\\), which modulates the variance in the form of \\(\\mu+\\frac{\\mu^2}{\\theta}\\), i.e. the variance is now a non-linear function of the the mean, and not equal to the mean as in the Poisson model. This allows for over-dispersion.22 The negative-binomial is available through the MASS package in R. Let’s see how it fits the birds data: library(&quot;MASS&quot;) # fit negative-binomial model with log-link to birds data birds_fit2 &lt;- glm.nb(y ~ relief, link = log, data = birds) # summarise regression summary(birds_fit2) ## ## Call: ## glm.nb(formula = y ~ relief, data = birds, link = log, init.theta = 11.71533906) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.31742 -0.59353 -0.05508 0.46728 2.12453 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 4.72453 0.08796 53.714 &lt; 2e-16 *** ## relief 0.31269 0.03893 8.032 9.59e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Negative Binomial(11.7153) family taken to be 1) ## ## Null deviance: 134.313 on 60 degrees of freedom ## Residual deviance: 61.519 on 59 degrees of freedom ## AIC: 681.88 ## ## Number of Fisher Scoring iterations: 1 ## ## ## Theta: 11.72 ## Std. Err.: 2.21 ## ## 2 x log-likelihood: -675.879 The parameters of the linear predictor are very similar to those of the Poisson model. Hence the modelled mean response looks very similar: # plot negative-binomial fit plot(birds$relief, birds$y, pch = 19, type = &#39;p&#39;, xlim=c(0,5), ylim=c(0,600), xlab=&#39;Average relief&#39;, ylab=&#39;Number of bird species&#39;) newdat &lt;- data.frame(relief=seq(0,5,0.01)) y_new &lt;- predict(birds_fit2, newdata=newdat, type = &quot;response&quot;) lines(newdat$relief, y_new, lwd = 3, col = &quot;red&quot;) But now we have the additional dispersion parameter \\(\\theta\\), which comes out at 11.72 for the birds data, which leads to a better coverage of the response distribution compared to the Poisson model: # plot overall response distribution for negative-binomial fit plot(h0, freq = FALSE, col = &quot;gray&quot;, ylim = c(0,0.02), main = &quot;overall negative-binomial model&quot;, xlab = &quot;Number of bird species&quot;, ylab = &quot;pdf&quot;) curve(( dnbinom(x, mu=predict(birds_fit2, newdata=data.frame(relief=0.5760), type = &quot;response&quot;), size=birds_fit2$theta) + dnbinom(x, mu=predict(birds_fit2, newdata=data.frame(relief=1.5690), type = &quot;response&quot;), size=birds_fit2$theta) + dnbinom(x, mu=predict(birds_fit2, newdata=data.frame(relief=2.4510), type = &quot;response&quot;), size=birds_fit2$theta) + dnbinom(x, mu=predict(birds_fit2, newdata=data.frame(relief=3.6090), type = &quot;response&quot;), size=birds_fit2$theta))/4, add=TRUE, col = &quot;red&quot;, type = &#39;b&#39;) AIC - indicative of predictive performance - is a lot better compared to the Poisson model, even if Pseudo-\\(r^2\\) is a little worse: birds_fit2$aic ## [1] 681.8786 pseudo_r2 &lt;- 1 - birds_fit2$deviance / birds_fit2$null.deviance pseudo_r2 ## [1] 0.541972 The residuals show some improvement, but not much: # plot Pearson residuals of negative-binomial fit plot(birds_fit2$fitted.values, resid(birds_fit2, type = &#39;pearson&#39;), pch = 19, type = &#39;p&#39;, xlim=c(100,400), ylim=c(-3,3), xlab=&#39;Number of bird species&#39;, ylab=&#39;Residuals&#39;) It seems we have reached the limit here of what we can achieve with just a single predictor. Further improvements should only be expected if we introduce additional predictors of the number of bird species. References "],
["multivariate.html", "Chapter 8 Multivariate methods 8.1 Cluster analysis 8.2 Principal Component Analysis (PCA) 8.3 Multivariate ANOVA (MANOVA) 8.4 Discriminant Function Analysis (DFA)", " Chapter 8 Multivariate methods Multivariate regression extends the case of multiple regression (one response variable/ multiple predictor variables) to the case of multiple response variables/ multiple predictor variables – predictors are continuous variables (though there is the same correspondence as between linear regression and ANOVA). Multivariate analysis of variance (MANOVA) (Chapter 8.3) extends the case of ANOVA (one response variable/ multiple predictor variables) to the case of multiple response variables/ multiple predictor variables – predictors are categorical variables (though MANOVA relates to multivariate regression just like ANOVA does to linear regression). Discriminant Function Analysis (DFA) (Chapter 8.4) is a classification method that tests how well multi-response observations discriminate between pre-determined groups, and can also be used to classify new observations into one of the groups. Principle Component Analysis (PCA) (Chapter 8.2), Factor Analysis (FA) and related methods aim at finding structure in a multivariate dataset, not deciding on response/predictor variables just yet. While PCA is trying to extract a reduced set of components that explain much of the variability in the original variables, FA is trying to explain correlations among the original variables (but both are related). PCA and FA are typically employed to pre-structure and simplify a problem by reducing its data dimensions, e.g. to reduce collinearity (compare Chapter 5). Cluster Analysis (Chapter 8.1) looks for groups (clusters) in a multivariate dataset. Objects (data points) belonging to the same group “resemble” each other (we will see what this means). Objects belonging to different groups are “dissimilar”. 8.1 Cluster analysis This section is based on material by Cornelius Senf.23 Cluster analysis looks for groups (clusters) in a multivariate dataset. Objects (data points) belonging to the same group “resemble” each other (we will see what this means). Objects belonging to different groups are “dissimilar”. There are three groups of methods: Partitioning the dataset into a number of clusters specified by the user, e.g. the kmeans algorithm Hierarchical, starting with each object (data point) as a separate cluster and then aggregating these step by step, ending up with a single cluster Divisive, starting with a single cluster of all objects (data points) and then splitting this up step by step until all objects are in different clusters Let’s illustrate the principles of these methods with the Iris dataset that is available from R, on which cluster analysis can be used to separate taxonomic groups. The dataset consists of a sample of Iris flowers for which the lengths and widths of their sepals and petals were measured. Sepals and petals are two different kinds of leaves in the flower. The question is: Can we separate clusters of flowers that are sufficiently different with respect to these four features? This then could form the basis of deriving taxonomic groups; indeed this is a typical approach in botany. To get a sense of the dataset let’s first plot a scatterplot matrix: # load Iris dataset data(iris) # scatterplot matrix plot(iris[,1:4]) We can already see at least two clusters. In some dimensions (petal length and width) they are more apart than in others (sepal length and width). Let’s formalise this analysis using the kmeans algorithm and afterwards look briefly what hierarchical methods do. 8.1.1 The kmeans algorithm The purpose of kmeans is to build clusters such that the distance of cluster objects (data points) to cluster centroids (vectors of means) is minimised. The algorithm proceeds though the following steps: Choose \\(k\\) random cluster centroids in the multivariate space Allocate each object to a cluster so that total intra-cluster sum of squares (Equation (8.1)) is minimised \\[\\begin{equation} \\sum_{j=1}^{k}\\sum_{i=1}^{n_j}\\lVert \\mathbf{y}_{ij}-\\boldsymbol{\\mu}_j\\rVert^2 \\tag{8.1} \\end{equation}\\] \\(\\boldsymbol{\\mu}_j\\) is the centroid of cluster \\(j=1,2,\\ldots,k\\), i.e. the vector of means across the data dimensions (here four). \\(\\mathbf{y}_{ij}\\) is data point \\(i=1,2,\\ldots,n_j\\) of cluster \\(j\\), i.e. a multivariate vector too. \\(\\lVert\\cdot\\rVert\\) symbolises the Euclidean distance. Re-calculate cluster centroids Repeat steps 2-3 until cluster centroids are not changing much anymore (by some chosen criterion) Often the Euclidean distance is used as a measure of (dis)similarity but others can be specified as well. We have to tell the algorithm how many clusters we want. Let’s use two to begin with (because that was our intuition earlier): # run kmeans algorithm on Iris data asking for 2 clusters iris_fit2 &lt;- kmeans(iris[,1:4], centers=2) # scatterplot matrix plot(iris[,1:4], col=iris_fit2$cluster) Two clusters didn’t seem enough to reproduce the separation we see visually. Let’s increase the number of clusters to three: # run kmeans algorithm on Iris data asking for 3 clusters iris_fit3 &lt;- kmeans(iris[,1:4], centers=3) # scatterplot matrix plot(iris[,1:4], col=iris_fit3$cluster) I think we would be happy with this visually. But are there perhaps even more clusters? When to stop? A useful stopping criterion is to look at the inflexion point where the total intra-cluster sum of squares (Equation (8.1)) does not change much anymore with increasing \\(k\\): # specify vector of clusters clusters &lt;- 1:10 # initialise corresponding vector of total intra-cluster sum of squares ticss &lt;- rep(NA, 10) # loop through vector of clusters for(i in clusters){ # run kmeans iris_fit &lt;- kmeans(iris[,1:4], centers=i) # collect total intra-cluster sum of squares ticss[i] &lt;- iris_fit$tot.withinss } # plot total intra-cluster sum of squares against clusters plot(clusters, ticss, pch = 19, type = &#39;b&#39;, xlab = &quot;Number of clusters&quot;, ylab = &quot;Total intra-cluster sum of squares&quot;) We can see that beyond three clusters the improvement in separation is minimal, so we would leave it at three. This, it turns out, matches almost perfectly the official taxonomic separation of the Iris dataset: # contingency table of cluster:species matches table(iris_fit3$cluster, iris$Species) ## ## setosa versicolor virginica ## 1 50 0 0 ## 2 0 2 36 ## 3 0 48 14 The separation of Iris versicolor and Iris virginica, however, is not perfect. 8.1.2 Hierarchical methods The principle of hierarchical methods is to start with each object as a separate cluster and then aggregate these step by step, ending up with a single cluster. Note, devisive methods are not covered here, but they work exactly the other way round. An example algorithm works as follows: Build dissimilarity matrix, i.e. a matrix of the distances of every object to every other object in the multivariate space, e.g. by Euclidean distance \\(\\lVert\\mathbf{y}_i-\\mathbf{y}_{i^*}\\rVert\\) Start with each object as a separate cluster Join the two most similar clusters, e.g. those that lead to minimum increase in total intra-cluster sum of squares after merging (Equation (8.1)); this is called Ward’s method Repeat step 3 until a single cluster is build The result is a dendrogram, whose “height” is the distance between clusters, e.g. in Ward’s method the increase in the intra-cluster sum of squares of the clusters being merged: # construct dissimilarity matrix for Iris data iris_dist &lt;- dist(iris[,1:4]) # run hierarchical clustering with Ward&#39;s method iris_fith &lt;- hclust(iris_dist, method=&quot;ward.D2&quot;) # dendrogram plot(iris_fith) Again we can see the three clearly separate clusters, beyond which any further separation is ambiguous. 8.2 Principal Component Analysis (PCA) To illustrate PCA I will use a dataset from (???), cited in Quinn and Keough (2002), that consists of stream chemistry measurements from 38 forested catchments in the Catskill Mountains, New York: # load stream chemistry data streams &lt;- read.table(&quot;data/streams.txt&quot;,header=T) head(streams) ## catchm_name max_elev sample_elev stream_length catchm_area NO3 TON TN NH4 ## 1 Santa_Cruz 1006 680 1680 23 24.2 5.6 29.9 0.8 ## 2 Colgate 1216 628 3912 462 25.4 4.9 30.3 1.4 ## 3 Halsey 1204 625 4032 297 29.7 4.4 33.0 0.8 ## 4 Batavia_Hill 1213 663 3072 399 22.1 6.1 28.3 1.4 ## 5 Windham_Ridg 1074 616 2520 207 13.1 5.7 17.6 0.6 ## 6 Silver_Sprin 1113 451 3120 348 27.5 3.0 30.8 1.1 ## DOC SO4 CL CA MG H ## 1 180.4 50.6 15.5 54.7 14.4 0.48 ## 2 108.8 55.4 16.4 58.4 17.0 0.24 ## 3 104.7 56.5 17.1 65.9 19.6 0.47 ## 4 84.5 57.5 16.8 59.5 19.5 0.23 ## 5 82.4 58.3 18.3 54.6 21.9 0.37 ## 6 86.6 63.0 15.7 68.5 22.4 0.17 The dataset first lists catchment characteristic: catchment name, maximum elevation in catchment, elevation where stream water sample was taken, stream length in catchment, catchment area. The stream chemistry variables are concentrations of: nitrate, total organic nitrogen, total nitrogen, ammonium, dissolved organic carbon, sulfate, chloride, calcium, magnesium, hydrogen. 8.2.1 From univariate normal to multivariate normal Like univariate methods, multivariate methods, too, rely on the normality assumption; the univariate normal distribution (Figure 8.1, left) is generalised to the multivariate normal distribution (Figure 8.1, right). Taking calcium concentration in the stream chemistry dataset as the example, the univariate normal model would be \\(N(\\mu,\\sigma)\\) with mean \\(\\mu=65.13\\) and variance \\(\\sigma^2=194.74\\) (Figure 8.1, left). Looking at calcium and magnesium concentration together, the multivariate normal model would be \\(MVN(\\boldsymbol\\mu,\\boldsymbol\\Sigma)\\) with centroid (vector of means) \\(\\boldsymbol\\mu=\\begin{pmatrix}65.13&amp;22.86\\end{pmatrix}^T\\) and variance-covariance matrix \\(\\boldsymbol\\Sigma=\\begin{pmatrix}194.74&amp;27.03\\\\27.03&amp;194.74\\end{pmatrix}\\) (Figure 8.1, right). Figure 8.1: Left: Histogram of calcium concentration measurements from stream chemistry dataset, with fitted normal distribution. The vertical line marks the mean. Right: Scatterplot of calcium against magnesium concentration measurements, with coloured countours of fitted multivariate normal distribution. The vertical and horizontal lines are the individual means, which intersect at the centroid. Data from: (???), cited in Quinn and Keough (2002). The variance-covariance matrix (or just covariance matrix) is a matrix of associations between variables. On its diagonal are the variances of the individual variables,24 on the off-diagonals are the covariances between two variables: \\[\\begin{equation} \\mathbf{C}=\\begin{pmatrix} \\frac{\\sum_{i=1}^{n}\\left(y_{i1}-\\bar y_1\\right)^2}{n-1} &amp; \\frac{\\sum_{i=1}^{n}\\left(y_{i2}-\\bar y_2\\right)\\cdot\\left(y_{i1}-\\bar y_1\\right)}{n-1} &amp; \\cdots &amp; \\frac{\\sum_{i=1}^{n}\\left(y_{ip}-\\bar y_p\\right)\\cdot\\left(y_{i1}-\\bar y_1\\right)}{n-1} \\\\ \\frac{\\sum_{i=1}^{n}\\left(y_{i1}-\\bar y_1\\right)\\cdot\\left(y_{i2}-\\bar y_2\\right)}{n-1} &amp; \\frac{\\sum_{i=1}^{n}\\left(y_{i2}-\\bar y_2\\right)^2}{n-1} &amp; \\cdots &amp; \\frac{\\sum_{i=1}^{n}\\left(y_{ip}-\\bar y_p\\right)\\cdot\\left(y_{i2}-\\bar y_2\\right)}{n-1} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\sum_{i=1}^{n}\\left(y_{i1}-\\bar y_1\\right)\\cdot\\left(y_{ip}-\\bar y_p\\right)}{n-1} &amp; \\frac{\\sum_{i=1}^{n}\\left(y_{i2}-\\bar y_2\\right)\\cdot\\left(y_{ip}-\\bar y_p\\right)}{n-1} &amp; \\cdots &amp; \\frac{\\sum_{i=1}^{n}\\left(y_{ip}-\\bar y_p\\right)^2}{n-1} \\end{pmatrix} \\tag{8.2} \\end{equation}\\] Normalising the variance-covariance terms in Equation (8.2) by the variances of the respective variables, e.g. \\(corr\\left(y_1,y_2\\right)=\\frac{cov\\left(y_1,y_2\\right)}{\\sqrt{\\sigma_{y_1}^2\\cdot\\sigma_{y_2}^2}}=\\frac{cov\\left(y_1,y_2\\right)}{\\sigma_{y_1}\\cdot\\sigma_{y_2}}\\), yields the correlation matrix: \\[\\begin{equation} \\mathbf{R}=\\begin{pmatrix} \\frac{\\sum_{i=1}^{n}\\left(y_{i1}-\\bar y_1\\right)^2}{\\sqrt{\\sum_{i=1}^{n}\\left(y_{i1}-\\bar y_1\\right)^2\\cdot\\sum_{i=1}^{n}\\left(y_{i1}-\\bar y_1\\right)^2}} &amp; \\frac{\\sum_{i=1}^{n}\\left(y_{i2}-\\bar y_2\\right)\\cdot\\left(y_{i1}-\\bar y_1\\right)}{\\sqrt{\\sum_{i=1}^{n}\\left(y_{i2}-\\bar y_2\\right)^2\\cdot\\sum_{i=1}^{n}\\left(y_{i1}-\\bar y_1\\right)^2}} &amp; \\cdots &amp; \\frac{\\sum_{i=1}^{n}\\left(y_{ip}-\\bar y_p\\right)\\cdot\\left(y_{i1}-\\bar y_1\\right)}{\\sqrt{\\sum_{i=1}^{n}\\left(y_{ip}-\\bar y_p\\right)^2\\cdot\\sum_{i=1}^{n}\\left(y_{i1}-\\bar y_1\\right)^2}} \\\\ \\frac{\\sum_{i=1}^{n}\\left(y_{i1}-\\bar y_1\\right)\\cdot\\left(y_{i2}-\\bar y_2\\right)}{\\sqrt{\\sum_{i=1}^{n}\\left(y_{i1}-\\bar y_1\\right)^2\\cdot\\sum_{i=1}^{n}\\left(y_{i2}-\\bar y_2\\right)^2}} &amp; \\frac{\\sum_{i=1}^{n}\\left(y_{i2}-\\bar y_2\\right)^2}{\\sqrt{\\sum_{i=1}^{n}\\left(y_{i2}-\\bar y_2\\right)^2\\cdot\\sum_{i=1}^{n}\\left(y_{i2}-\\bar y_2\\right)^2}} &amp; \\cdots &amp; \\frac{\\sum_{i=1}^{n}\\left(y_{ip}-\\bar y_p\\right)\\cdot\\left(y_{i2}-\\bar y_2\\right)}{\\sqrt{\\sum_{i=1}^{n}\\left(y_{ip}-\\bar y_p\\right)^2\\cdot\\sum_{i=1}^{n}\\left(y_{i2}-\\bar y_2\\right)^2}} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\sum_{i=1}^{n}\\left(y_{i1}-\\bar y_1\\right)\\cdot\\left(y_{ip}-\\bar y_p\\right)}{\\sqrt{\\sum_{i=1}^{n}\\left(y_{i1}-\\bar y_1\\right)^2\\cdot\\sum_{i=1}^{n}\\left(y_{ip}-\\bar y_p\\right)^2}} &amp; \\frac{\\sum_{i=1}^{n}\\left(y_{i2}-\\bar y_2\\right)\\cdot\\left(y_{ip}-\\bar y_p\\right)}{\\sqrt{\\sum_{i=1}^{n}\\left(y_{i2}-\\bar y_2\\right)^2\\cdot\\sum_{i=1}^{n}\\left(y_{ip}-\\bar y_p\\right)^2}} &amp; \\cdots &amp; \\frac{\\sum_{i=1}^{n}\\left(y_{ip}-\\bar y_p\\right)^2}{\\sqrt{\\sum_{i=1}^{n}\\left(y_{ip}-\\bar y_p\\right)^2\\cdot\\sum_{i=1}^{n}\\left(y_{ip}-\\bar y_p\\right)^2}} \\end{pmatrix} \\tag{8.2} \\end{equation}\\] Let’s look at the associations between the variables in the stream chemistry dataset: pairs(streams[,6:15], diag.panel = panel.hist) First of all we notice the non-normality DOC, Cl and H (at least; in principle this diagnosis is ambiguous). Hence we log-transform these three variables to make them adhere better to normality: streams_log &lt;- streams streams_log$DOC &lt;- log(streams$DOC) streams_log$CL &lt;- log(streams$CL) streams_log$H &lt;- log(streams$H) pairs(streams_log[,6:15], diag.panel = panel.hist) We then notice a positive correlation between NO3 and TN; most of the total nitrogen in streams actually comes in the form of nitrate. We notice a positive correlation between Ca and Mg; both are characteristic of alkaline waters and often appear together. We notice a positive correlation between Mg (and Ca) and SO4; magnesium (and calcium) often appear as Mg(Ca)-sulphate. And we notice a negative correlation between Ca and H; calcium is characteristic of alkaline waters, i.e. waters that have a high pH, hence a low hydrogen concentration. These associations are reflected in the covariance matrix: cov(streams_log[,6:15]) ## NO3 TON TN NH4 DOC SO4 ## NO3 74.1885064 -4.367083926 68.5690754 -1.21607397 0.26399923 -1.8439972 ## TON -4.3670839 1.636706970 -2.5259104 0.14816501 0.07963509 0.3585633 ## TN 68.5690754 -2.525910384 65.6039900 -0.97096728 0.30096702 0.9395590 ## NH4 -1.2160740 0.148165007 -0.9709673 0.53607397 -0.09426689 0.3710242 ## DOC 0.2639992 0.079635093 0.3009670 -0.09426689 0.11367492 -0.6113830 ## SO4 -1.8439972 0.358563300 0.9395590 0.37102418 -0.61138295 27.2433286 ## CL -1.2917322 0.060402549 -1.1991558 0.07313198 -0.01935573 0.4772130 ## CA 29.1343670 -2.879523471 29.0208179 0.57644381 -1.25321565 35.0845235 ## MG -14.7774538 2.627396871 -9.8611522 0.77961593 -0.60795805 19.8258464 ## H 0.3516809 -0.008264002 0.1717175 -0.10160741 0.09357589 -1.5999792 ## CL CA MG H ## NO3 -1.29173215 29.1343670 -14.7774538 0.351680867 ## TON 0.06040255 -2.8795235 2.6273969 -0.008264002 ## TN -1.19915582 29.0208179 -9.8611522 0.171717531 ## NH4 0.07313198 0.5764438 0.7796159 -0.101607407 ## DOC -0.01935573 -1.2532156 -0.6079580 0.093575891 ## SO4 0.47721304 35.0845235 19.8258464 -1.599979158 ## CL 0.12765675 0.8214436 1.0110406 -0.080456005 ## CA 0.82144357 194.7417710 27.0328307 -7.639475486 ## MG 1.01104057 27.0328307 26.2495306 -1.883364151 ## H -0.08045601 -7.6394755 -1.8833642 0.454192635 In particular, \\(cov(NO3,TN)=68.57\\), \\(cov(SO4,MG)=19.83\\) and \\(cov(CA,H)=-7.64\\). However, at the scale of covariances we cannot judge the strengths of the associations, because the magnitude of the covariance is determined by the diagonal variance terms of the respective variables. We need to normalise by those variances, i.e. look at the correlation matrix: cor(streams_log[,6:15]) ## NO3 TON TN NH4 DOC SO4 ## NO3 1.00000000 -0.396312489 0.98286722 -0.19283209 0.09090798 -0.04101681 ## TON -0.39631249 1.000000000 -0.24376275 0.15817863 0.18462338 0.05369703 ## TN 0.98286722 -0.243762753 1.00000000 -0.16372957 0.11021011 0.02222434 ## NH4 -0.19283209 0.158178634 -0.16372957 1.00000000 -0.38186908 0.09708671 ## DOC 0.09090798 0.184623379 0.11021011 -0.38186908 1.00000000 -0.34741688 ## SO4 -0.04101681 0.053697033 0.02222434 0.09708671 -0.34741688 1.00000000 ## CL -0.41974184 0.132144140 -0.41437028 0.27955874 -0.16067757 0.25589413 ## CA 0.24238610 -0.161289316 0.25675267 0.05641766 -0.26635688 0.48167703 ## MG -0.33486559 0.400847742 -0.23763027 0.20782968 -0.35194963 0.74138003 ## H 0.06058434 -0.009584841 0.03145788 -0.20591736 0.41182395 -0.45484581 ## CL CA MG H ## NO3 -0.4197418 0.24238610 -0.3348656 0.060584340 ## TON 0.1321441 -0.16128932 0.4008477 -0.009584841 ## TN -0.4143703 0.25675267 -0.2376303 0.031457881 ## NH4 0.2795587 0.05641766 0.2078297 -0.205917361 ## DOC -0.1606776 -0.26635688 -0.3519496 0.411823946 ## SO4 0.2558941 0.48167703 0.7413800 -0.454845809 ## CL 1.0000000 0.16475033 0.5523138 -0.334130816 ## CA 0.1647503 1.00000000 0.3780952 -0.812295299 ## MG 0.5523138 0.37809523 1.0000000 -0.545448165 ## H -0.3341308 -0.81229530 -0.5454482 1.000000000 Now the magnitude of the association is easier to see: \\(corr(NO3,TN)=0.98\\), \\(corr(SO4,MG)=0.74\\) and \\(corr(CA,H)=-0.81\\). 8.2.2 Linear combination of variables A first step in multivariate analyses (PCA, DFA, FA, …) is usually to centre the variables to zero mean: \\(y^*=y-\\mu\\) so that \\(\\mu^*=0\\) and \\(\\sigma^*=\\sigma\\). Compare Chapter 2. Then, the fundamental concept underlying multivariate analyses (PCA, DFA, FA, …) is to derive new linear combinations of the variables that summarise the variation in the original data set: \\[\\begin{equation} z_{ik}=u_{1k}\\cdot y_{i1}+u_{2k}\\cdot y_{i2}+\\ldots+u_{pk}\\cdot y_{ip} \\tag{8.3} \\end{equation}\\] \\(z_{ik}\\) is the value of the new variable \\(k\\) for object \\(i\\). The object can be a timestep or a spatial location or else. \\(u_{1k},\\ldots,u_{pk}\\) are the coefficients indicating how much each original variable contributes to the linear combination. This is the Eigenvector of the covariance matrix (more on this later). \\(y_{i1},\\ldots,y_{ip}\\) are the values of original variables \\(1,\\ldots,p\\) for object \\(i\\). The new variables are called, depending on the type of method, principal components, factors or discriminant functions. They are extracted so that the first explains most of the variance in the original variables, the second explains most of the remaining variance after the first has been extracted but is independent of (uncorrelated with) the first … and so on. The number of new variables \\(k\\) is the same as the number of original variables \\(p\\), although the variance is usually consolidated in the first few new variables. The unknown coefficients \\(u_{1k},\\ldots,u_{pk}\\) are determined via so called Eigenanalysis (see below). A graphical explanation of what PCA does is axis rotation. […] 8.2.3 Eigenanalysis The vector of coefficients \\(\\begin{pmatrix}u_{1k},\\ldots,u_{pk}\\end{pmatrix}^T\\) in Equation (8.3) is called the \\(k\\)th Eigenvector. This is found so that the following equation holds:25 \\[\\begin{equation} \\mathbf{C}\\cdot\\begin{pmatrix} u_{1k}\\\\u_{2k}\\\\\\vdots\\\\u_{pk} \\end{pmatrix}=\\lambda_k\\cdot\\begin{pmatrix} u_{1k}\\\\u_{2k}\\\\\\vdots\\\\u_{pk} \\end{pmatrix} \\tag{8.4} \\end{equation}\\] \\(\\mathbf{C}\\) is the covariance matrix and \\(\\lambda_1,\\ldots,\\lambda_k\\) are the so called Eigenvalues, which equal the amount of variance explained by each new variable. The sum of variances (Eigenvalues) of the new variables equals the sum of variances of the original variables. […] The Eigenanalysis can be carried out, i.e. the Eigenvectors and Eigenvalues determined, on the covariance matrix \\(\\mathbf{C}\\) or the correlation matrix \\(\\mathbf{R}\\) using Spectral Decomposition (Eigendecomposition) or on the data matrix (raw, centred or standardised) using Singular Value Decomposition, which is the more general method. We don’t go into the details of these techniques here. If the Eigenanalysis is carried out on the covariance matrix \\(\\mathbf{C}\\) then \\(\\sum_{j=1}^{k}\\lambda_j=Tr(\\mathbf{C})\\), i.e. the sum of the Eigenvalues is the trace of \\(\\mathbf{C}\\). The trace is defined as the sum of the diagonal elements of a matrix, i.e. here the sum of the variances of the original centred variables. This isappropriate when the variables are measured in comparable units and differences in variance make an important contribution to interpretation. If the Eigenanalysis is carried out on the correlation matrix \\(\\mathbf{R}\\) then \\(\\sum_{j=1}^{k}\\lambda_j=Tr(\\mathbf{R})\\), i.e. the sum of the variances of the original standardised variables: \\(y^*=\\frac{y-\\mu}{\\sigma}\\) so that \\(\\mu^*=0\\) and \\(\\sigma^*=1\\). Compare Chapter 2. This is necessary when variables are measured in very different units or scales, otherwise variables with large values/variances may dominate the results. In the stream chemistry example we best work with standardised data because variances are orders of magnitude different.26 The model for the new variables (Equation (8.3)) is: \\[\\begin{equation} z_{ik}=u_{1k}\\cdot\\left(NO_3\\right)_i+u_{2k}\\cdot\\left(TON\\right)_i+u_{3k}\\cdot\\left(TN\\right)_i+u_{4k}\\cdot\\left(NH_4\\right)_i+u_{5k}\\cdot\\left(\\log(DOC)\\right)_i+u_{6k}\\cdot\\left(SO_4\\right)_i+u_{7k}\\cdot\\left(\\log(Cl)\\right)_i+u_{8k}\\cdot\\left(Ca\\right)_i+u_{9k}\\cdot\\left(Mg\\right)_i+u_{10k}\\cdot\\left(\\log(H)\\right)_i \\tag{8.5} \\end{equation}\\] prcomp(streams_log[,6:15], scale=TRUE) ## Standard deviations (1, .., p=10): ## [1] 1.85038489 1.57254556 1.08217040 0.96516016 0.86381333 0.80396686 ## [7] 0.61930355 0.36160718 0.30237094 0.04783651 ## ## Rotation (n x k) = (10 x 10): ## PC1 PC2 PC3 PC4 PC5 PC6 ## NO3 -0.2608012 0.51882614 0.048948664 0.22991773 -0.02957754 -0.2391299 ## TON 0.1470633 -0.29927267 0.515134430 0.54375942 -0.11855680 0.3029571 ## TN -0.2284551 0.51019467 0.153554316 0.34323284 -0.02311043 -0.1993705 ## NH4 0.2279503 -0.07531476 -0.486775784 0.65372357 -0.26639758 -0.1033473 ## DOC -0.2882751 -0.14662430 0.562051058 -0.09028595 -0.43309048 -0.2021881 ## SO4 0.3684647 0.22497988 0.241732702 0.01627193 0.53819564 -0.1550440 ## CL 0.3578631 -0.15795736 -0.017738392 -0.16532689 -0.32510731 -0.7380443 ## CA 0.2811469 0.44581191 0.080590317 -0.18761722 -0.36044030 0.2312748 ## MG 0.4716303 0.01538667 0.300889286 0.10730721 0.23553548 -0.1420917 ## H -0.3972262 -0.28148573 0.005572391 0.15438867 0.38112577 -0.3424885 ## PC7 PC8 PC9 PC10 ## NO3 -0.16234834 0.05636563 -0.03009849 -0.720448525 ## TON -0.29797877 -0.28358340 -0.20552744 -0.102368378 ## TN -0.18049740 0.05424407 -0.04787331 0.684124042 ## NH4 0.43198173 0.06398237 0.07470189 -0.011423521 ## DOC 0.53976801 0.22174766 0.04692825 -0.013216360 ## SO4 0.48378154 -0.15731293 -0.42645336 -0.023891086 ## CL -0.29877601 -0.19877842 -0.19925916 0.021153361 ## CA 0.15884541 -0.57822152 0.37080339 0.006383739 ## MG -0.13127371 0.41741127 0.63806761 -0.031885844 ## H 0.09240295 -0.53606219 0.42495406 0.008249647 8.3 Multivariate ANOVA (MANOVA) 8.4 Discriminant Function Analysis (DFA) References "],
["solutions-to-exercises.html", "Solutions to exercises Answer to Q1 Answer to Q2", " Solutions to exercises Answer to Q1 Equations (3.3), (3.5) and (3.7) are linear models; the others are non-linear in their parameters. Note, that the variables of linear models can take non-linear forms (\\(x_1^3, x_1 \\cdot x_2, \\log x_1\\)) as long as the parameters are not implicated in this non-linearity. Answer to Q2 We have 6 main predictors, 5+4+3+2+1=15 2-way interactions, 20 3-way interactions, 15 4-way interactions, 6 5-way interactions and 1 6-way interaction, i.e. 63 possible predictors. Note the symmetry of this calculation: the number of possibilities of combining 2 variables (2-way interactions) is the same as the number of possibilities of leaving out 2 variables (4-way interactions). "],
["references.html", "References", " References "]
]
