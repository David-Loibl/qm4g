[
["index.html", "Quantitative Methods for Geographers Preface", " Quantitative Methods for Geographers Tobias Krueger 2020-11-20 Preface This is the script of the course ‘Quantitative Methods for Geographers’ run at the Geography Department of Humboldt-Universität zu Berlin. "],
["orga.html", "Chapter 1 Organisational matters 1.1 Motivating example 1.2 Topics 1.3 Format", " Chapter 1 Organisational matters Welcome to the course Quantitative Methods for Geographers (online edition), which consists of a seminar and a PC lab. In the seminar, theoretical input will be provided by myself up until January and by Tobia Lakes in the last four sessions in February. Part of this course is a PC lab run by Dirk Pflugmacher and David Loibl. 1.1 Motivating example Figure 1.1 shows the spatial variation of annual average precipitation and annual average temperature over Germany. Before reading on, reflect a minute on what might control this spatial variation. Figure 1.1: Maps of annual average precipitation (left) and annual average air temperature (right) over Germany from 1961 to 1990. Source: https://www.dwd.de/EN/climate_environment/climateatlas/climateatlas_node.html. The spatial variation of average precipitation and temperature in our climatic zone is controlled mainly by elevation and continentality: the higher up we go the more it rains over the year and the colder it gets; and the further East we go the drier it gets while a temperature effect is not visible. There are of course more nuanced effects such as aspect, but these are not so clearly visible in this figure. And this is precisely the goal of large parts of this course: To try and explain patterns in data (so called response variables) over space and time - such as rainfall and temperature - with other data (so called predictor variables). We do this using statistical inference methods. This includes separating dominant predictors from not so dominant predictors. It also includes making predictions with the relationships we find, e.g. for interpolating between data points to create maps like Figure 1.1. By the end of this course you will have learned the following: You have understood the concept of statistical inference using the linear model in depth. This includes linear regression with one or more predictors (multiple regression). It also includes mildly non-linear models and specific types of responses, dealt with by so called Generalised Linear Models. You have worked with metric as well as categorical predictors and mixtures of the two. You have learned metrics to compare and simplify models and evaluate their assumptions. You have understood the principles of extending these techniques to cases of more than one response variable (multivariate methods). You have learned specific techniques for working with spatial data. You can carry out the corresponding analyses in the software R. 1.2 Topics The timing of topics is shown in Table 1.1. Table 1.1: Schedule of Quantitative Methods for Geographers. Week Reading Monday seminar Wednesday PC lab 1 Organisational matters no class due to Dies Academicus Introduction to R 2 Mathematical preliminaries Introductions Data manipulation and import/export with R 3 The linear model Mathematical preliminaries Visualization and data manipulation with R 4 Categorical variables (ANOVA) and dummy coding The linear model Linear regression 5 Multiple linear regression Categorical variables (ANOVA) and dummy coding Hypothesis testing and ANOVA 6 Maximum Likelihood and outlook to Bayesian statistics Multiple linear regression Multiple linear regression 7 Generalized linear models I Maximum Likelihood and outlook to Bayesian statistics Outlook to machine learning 8 Generalized linear models II Generalized linear models I Generalized linear models I 9 Multivariate methods I Generalized linear models II Generalized linear models II 10 Multivariate methods II Multivariate methods I Principal Component Analysis 11 tbc Multivariate methods II Discriminant Function Analysis and model validation 12 tbc Understanding spatial data (Tobia) Spatial data and cluster analysis in R 13 tbc Point pattern analysis (Tobia) Point pattern analysis and spatial auto-correlation 14 tbc Spatial autocorrelation and interpolation (Tobia) Semivariogram analysis and kriging 15 tbc Spatial weights and linear modeling (Tobia) Spatial regression models 1.3 Format During this digital semester, the learning mode will be mainly reading. You are required to read a chapter of this script each week, which will then be discussed in a ZOOM session the following Monday 13:00-15:00 (see link on Moodle). The reading listed for each week in the table above is due the following week. I will provide you with guiding questions and small quizzes to guide your reading. You are required to post questions on the topics or aspects you would like me to focus on during the seminar in the Moodle Forum by each Friday. This way we know what to discuss each Monday and I will prepare some lecture-style input. We can also discuss questions that arise in the PC labs. Some questions might already be answered via the Forum. In the PC labs, Dirk and David will give you homework, which you need to submit via Moodle to pass the course. They will explain this in detail. The final exam is a project similar to an extended homework, in which you will be able to apply and expand on topics studied in class, using datasets provided in the research context of the Geography Department. The project will be done individually (other than in previous years) and each of you will get their own topic and data. We will allocate topics towards the end of the semester via Moodle. The project has to be submitted as a HTML document created by R Markdown towards the end of the semester break (deadline tbc). "],
["math.html", "Chapter 2 Mathematical preliminaries 2.1 Logarithm and exponentiation 2.2 Centring and standardisation 2.3 Derivatives 2.4 Matrix algebra 2.5 Exercises", " Chapter 2 Mathematical preliminaries In this chapter we get a few mathematical preliminaries out of the way that are important for later chapters. If you feel rusty on any of these then please read up on them elsewhere. 2.1 Logarithm and exponentiation The following is inspired by Gelman and Nolan (2002). Suppose you have an amoeba that takes one hour to divide (Figure 2.1), and then the two amoebas each divide in one more hour, and so forth. What is the equation of the number of amoebas, \\(y\\), as a function of time, \\(t\\) (in hours)? Figure 2.1: Amoeba dividing. Source: http://www.gutenberg.org/files/18451/18451-h/images/illus002.jpg. The equation is: \\[\\begin{equation} y=2^t \\tag{2.1} \\end{equation}\\] This is an exponential function with base 2 and exponent \\(t\\). Figure 2.2 shows two plots of this function. (Don’t worry, you will start to understand the R code better as you progress in the PC labs.) t &lt;- seq(1, 6) y &lt;- 2^t plot(t, y, pch = 19, type = &#39;b&#39;) plot(t, log(y), pch = 19, type = &#39;b&#39;) Figure 2.2: Left: Plot of Equation 2.1. Right: Plot of Equation 2.1 on logarithmic scale. The inverse of the exponential function is the logarithmic function: \\[\\begin{equation} log(y)=log(2^t)=t \\cdot log(2) \\tag{2.2} \\end{equation}\\] Since the logarithm of \\(y\\) is a linear function of \\(t\\) (Equation (2.2)), the right-hand side of Figure 2.2 (\\(y\\) on logarithmic scale) displays a straight line. Common bases of the logarithmic function are: \\[\\begin{equation} log_2\\left(2^t\\right)=lb\\left(2^t\\right)=t \\tag{2.3} \\end{equation}\\] This is called the binary logarithm (lb). \\[\\begin{equation} log_{10}\\left(10^t\\right)=lg\\left(10^t\\right)=t \\tag{2.4} \\end{equation}\\] This is called the common logarithm (lg). \\[\\begin{equation} log_e\\left(e^t\\right)=ln\\left(e^t\\right)=t \\tag{2.5} \\end{equation}\\] This is called the natural logarithm (ln) with \\(e \\approx 2.7183\\) being Euler’s constant. Note, programming often uses a different notation, which will also be used from now on in this course: \\[\\begin{equation} ln()=log() \\tag{2.6} \\end{equation}\\] \\[\\begin{equation} e^t=\\exp(t) \\tag{2.7} \\end{equation}\\] Basic rules for exponentiation are: \\[\\begin{equation} a^m \\cdot a^n=a^{m+n} \\tag{2.8} \\end{equation}\\] \\[\\begin{equation} a^n \\cdot b^n=(a \\cdot b)^n \\tag{2.9} \\end{equation}\\] \\[\\begin{equation} \\frac{a^m}{a^n}=a^{m-n} \\tag{2.10} \\end{equation}\\] \\[\\begin{equation} \\frac{a^n}{b^n}=\\left(\\frac{a}{b}\\right)^n \\tag{2.11} \\end{equation}\\] \\[\\begin{equation} \\left(a^m\\right)^n=a^{m \\cdot n} \\tag{2.12} \\end{equation}\\] At this point it is also useful to remind ourselves of the meaning of the sum and product symbols: \\[\\begin{equation} \\sum_{i=1}^{n}x_i=x_1+x_2+\\ldots+x_n \\tag{2.13} \\end{equation}\\] This signifies the sum of all \\(x_i\\) for \\(i\\) taking integer values from 1 to \\(n\\). \\[\\begin{equation} \\prod_{i=1}^{n}x_i=x_1 \\cdot x_2 \\cdot \\ldots \\cdot x_n \\tag{2.14} \\end{equation}\\] This signifies the product of all \\(x_i\\) for \\(i\\) taking integer values from 1 to \\(n\\). The basic rules of logarithm are: \\[\\begin{equation} log(u \\cdot v)=log(u)+log(v) \\tag{2.15} \\end{equation}\\] \\[\\begin{equation} log\\left(\\frac{u}{v}\\right)=log(u)-log(v) \\tag{2.16} \\end{equation}\\] \\[\\begin{equation} log\\left(u^r\\right)=r \\cdot log(u) \\tag{2.17} \\end{equation}\\] 2.2 Centring and standardisation Centring and standardisation are used to transform different datasets onto the same scale. We will need this for the multivariate methods in later sessions (Chapter 8). Centring means subtracting from every data point \\(y\\) the overall mean of the dataset \\(\\bar{y}\\): \\[\\begin{equation} y^*=y-\\bar{y} \\tag{2.18} \\end{equation}\\] This yields new data points \\(y^*\\) and a new mean \\(\\bar{y^*}=0\\) while the standard deviation of the transformed data remains the same: \\(s_{y^*}=s_y\\). Centring thus shifts the data histogram to be centred on zero, but does not change its shape (Figure 2.3). # draw random sample of size 1000 from normal distribution with mean 1 and standard deviation 2, i.e. y~N(1,2) y &lt;- rnorm(1000, mean = 1, sd = 2) # mean ybar &lt;- mean(y) # standard deviation s_y &lt;- sd(y) # histogram, raw hist(y, freq = FALSE, xlim = c(-10,10), ylim = c(0, 0.4), main = &quot;&quot;, xlab = &quot;y&quot;, ylab = &quot;relative frequency&quot;) lines(c(ybar, ybar), c(0,0.4), lwd = 3) # histogram, centred hist(y-ybar, freq = FALSE, xlim = c(-10,10), ylim = c(0, 0.4), main = &quot;&quot;, xlab = &quot;y*&quot;, ylab = &quot;relative frequency&quot;) lines(c(0, 0), c(0,0.4), lwd = 3) Figure 2.3: Histogram of dataset \\(y\\) (left) and centred dataset \\(y^*\\) (right). The vertical line represents the mean. Standardisation means subtracting from every data point \\(y\\) the overall mean of the dataset \\(\\bar{y}\\) and additionally dividing by the standard deviation \\(s_y\\): \\[\\begin{equation} y^*=\\frac{y-\\bar{y}}{s_y} \\tag{2.19} \\end{equation}\\] This yields new data points \\(y^*\\), a new mean \\(\\bar{y^*}=0\\) and a new standard deviation \\(s_{y^*}=1\\). Standardisation thus shifts the data histogram to be centred on zero and expands or contracts it to have unit standard deviation (Figure 2.4). # histogram, raw hist(y, freq = FALSE, xlim = c(-10,10), ylim = c(0, 0.4), main = &quot;&quot;, xlab = &quot;y&quot;, ylab = &quot;relative frequency&quot;) lines(c(ybar, ybar), c(0,0.4), lwd = 3) # histogram, standardised hist((y-ybar)/s_y, freq = FALSE, xlim = c(-10,10), ylim = c(0, 0.4), main = &quot;&quot;, xlab = &quot;y*&quot;, ylab = &quot;relative frequency&quot;) lines(c(0, 0), c(0,0.4), lwd = 3) Figure 2.4: Histogram of dataset \\(y\\) (left) and standardised dataset \\(y^*\\) (right). The vertical line represents the mean. If the original data \\(y\\) were normally distributed (like we set it up for the plots above) then standardisation would transform \\(y\\) to the scale of the standard normal distribution, i.e. a normal distribution with mean 0 and standard deviation 1. If you feel rusty on the normal distribution (or probability distributions in general) then Wikipedia is as good a source as any: https://en.wikipedia.org/wiki/Normal_distribution. 2.3 Derivatives The first derivative of a function \\(f(x)\\), written as \\(f&#39;(x)\\) or \\(\\frac{df(x)}{dx}\\), can be interpreted graphically as the slope of that function, i.e. the tangent line of a certain point of the function (Figure 2.5, left). Figure 2.5: Left: Tangent line of function \\(f(x)\\). Centre: Secant line of function \\(f(x)\\) between point \\(f\\left(x_0\\right)\\) and point \\(f\\left(x_0+h\\right)\\); the horizontal distance between these two points is \\(\\Delta x\\) and the vertical distance is \\(\\Delta f(x)\\). Right: Set of secant lines of function \\(f(x)\\) between point \\(f\\left(x_0\\right)\\) and point \\(f\\left(x_0+h\\right)\\) for progressively decreasing increments \\(h\\). Source: https://en.wikipedia.org/wiki/Derivative. Mathematically, the slope amounts to the limiting value of the ratio of the (vertical) increment of the function, \\(\\Delta f(x)\\), for an (horizontal) increment of \\(x\\), \\(\\Delta x\\), for \\(\\Delta x\\) approaching zero, \\(\\Delta x \\to 0\\): \\[\\begin{equation} f&#39;(x)=\\frac{df(x)}{dx}=\\lim_{\\Delta x \\to 0}\\frac{\\Delta f(x)}{\\Delta x} \\tag{2.20} \\end{equation}\\] This can be visualised as a secant line of the function between two points, \\(x_0\\) and \\(x_0+h\\) (Figure 2.5, centre), whose slope is: \\[\\begin{equation} \\frac{\\Delta f(x)}{\\Delta x}=\\frac{f\\left(x_0+h\\right)-f\\left(x_0\\right)}{\\left(x_0+h\\right)-\\left(x_0\\right)}=\\frac{f\\left(x_0+h\\right)-f\\left(x_0\\right)}{h} \\tag{2.21} \\end{equation}\\] As \\(h\\) approaches zero (Figure 2.5, right) we reach the limiting value of the slope at point \\(x_0\\), which is the first derivative: \\[\\begin{equation} \\frac{df(x)}{dx}=\\lim_{h \\to 0}\\frac{f\\left(x_0+h\\right)-f\\left(x_0\\right)}{h} \\tag{2.22} \\end{equation}\\] The first derivative is useful for finding minima, maxima and inflexion points of a function, because this is where the slope is zero, \\(\\frac{df(x)}{dx}=0\\) (Figure 2.6). The second derivative, measuring the curvature of the function, tells us whether these points are minima \\(\\left(\\frac{d^2f(x)}{dx^2}&gt;0\\right)\\), maxima \\(\\left(\\frac{d^2f(x)}{dx^2}&lt;0\\right)\\) or inflexion points \\(\\left(\\frac{d^2f(x)}{dx^2}=0\\right)\\), but often we already know that a function has only a single minimum or maximum and then we do not need the second derivative. Figure 2.6: Use of first and second derivative to determine minima, maxima and inflexion points of a function. Source: http://hyperphysics.phy-astr.gsu.edu/hbase/math/maxmin.html. The differentiation rules are listed below: If \\(y=f(t)=t^a\\) then \\(\\frac{dy}{dt}=a \\cdot t^{a-1}\\), i.e. multiplying the function with the exponent and reducing the exponent by one gives you the derivative of \\(f(x)\\). Constant factor rule: If \\(y=c \\cdot u(t)\\) then \\(\\frac{dy}{dt}=c \\cdot \\frac{du}{dt}\\). Sum rule: If \\(y=u(t) \\pm v(t)\\) then \\(\\frac{dy}{dt}=\\frac{du}{dt} \\pm \\frac{dv}{dt}\\). Product rule: If \\(y=u(t) \\cdot v(t)\\) then \\(\\frac{dy}{dt}=\\frac{du}{dt} \\cdot v+u \\cdot \\frac{dv}{dt}\\). Quotient rule: If \\(y=\\frac{u(t)}{v(t)}\\) then \\(\\frac{dy}{dt}=\\frac{\\left(\\frac{du}{dt} \\cdot v-u \\cdot \\frac{dv}{dt}\\right)}{v^2}\\). Chain rule: If \\(y=f[g(t)]\\) then \\(\\frac{dy}{dt}=\\frac{df[g]}{dg} \\cdot \\frac{dg}{dt}\\), i.e. “outer times inner derivative”. 2.4 Matrix algebra The following is based on Tabachnick and Fidell (2013). 2.4.1 Simple matrix operations Let \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) be matrices: \\[\\mathbf{A} = \\begin{pmatrix} a &amp; b &amp; c\\\\ d &amp; e &amp; f\\\\ g &amp; h &amp; i \\end{pmatrix} = \\begin{pmatrix} 3 &amp; 2 &amp; 4\\\\ 7 &amp; 5 &amp; 0\\\\ 1 &amp; 0 &amp; 8 \\end{pmatrix} \\] \\[\\mathbf{B} = \\begin{pmatrix} r &amp; s &amp; t\\\\ u &amp; v &amp; w\\\\ x &amp; y &amp; z \\end{pmatrix} = \\begin{pmatrix} 6 &amp; 1 &amp; 0\\\\ 2 &amp; 8 &amp; 7\\\\ 3 &amp; 4 &amp; 5 \\end{pmatrix} \\] Addition/subtraction of a constant to a matrix happens element-wise: \\[\\mathbf{A} + k = \\begin{pmatrix} a+k &amp; b+k &amp; c+k\\\\ d+k &amp; e+k &amp; f+k\\\\ g+k &amp; h+k &amp; i+k \\end{pmatrix} \\] \\[\\mathbf{A} - k = \\begin{pmatrix} a-k &amp; b-k &amp; c-k\\\\ d-k &amp; e-k &amp; f-k\\\\ g-k &amp; h-k &amp; i-k \\end{pmatrix} \\] Multiplication/division of a matrix by a constant also happens element-wise: \\[k \\cdot \\mathbf{A} = \\begin{pmatrix} k \\cdot a &amp; k \\cdot b &amp; k \\cdot c\\\\ k \\cdot d &amp; k \\cdot e &amp; k \\cdot f\\\\ k \\cdot g &amp; k \\cdot h &amp; k \\cdot i \\end{pmatrix} \\] \\[\\frac{1}{k} \\cdot \\mathbf{A} = \\begin{pmatrix} \\frac{1}{k} \\cdot a &amp; \\frac{1}{k} \\cdot b &amp; \\frac{1}{k} \\cdot c\\\\ \\frac{1}{k} \\cdot d &amp; \\frac{1}{k} \\cdot e &amp; \\frac{1}{k} \\cdot f\\\\ \\frac{1}{k} \\cdot g &amp; \\frac{1}{k} \\cdot h &amp; \\frac{1}{k} \\cdot i \\end{pmatrix} \\] Addition/subtraction of two matrices happens element-wise again: \\[\\mathbf{A} + \\mathbf{B} = \\begin{pmatrix} a+r &amp; b+s &amp; c+t\\\\ d+u &amp; e+v &amp; f+w\\\\ g+x &amp; h+y &amp; i+z \\end{pmatrix} \\] \\[\\mathbf{A} - \\mathbf{B} = \\begin{pmatrix} a-r &amp; b-s &amp; c-t\\\\ d-u &amp; e-v &amp; f-w\\\\ g-x &amp; h-y &amp; i-z \\end{pmatrix} \\] Finally, the so called transpose of a matrix refers to the mirroring of a matrix along its diagonal. Hence, the transpose of \\(\\mathbf{A}\\) is: \\[\\mathbf{A}&#39; = \\begin{pmatrix} a &amp; d &amp; g\\\\ b &amp; e &amp; h\\\\ c &amp; f &amp; i \\end{pmatrix} \\] 2.4.2 Matrix multiplication Now, the multiplication of two matrices is the only operation that is a bit complicated at first. It may be best to consider an example to work out the rules: \\[\\begin{eqnarray} \\mathbf{A} \\cdot \\mathbf{B}&amp;=&amp; \\begin{pmatrix} a &amp; b &amp; c\\\\ d &amp; e &amp; f\\\\ g &amp; h &amp; i \\end{pmatrix} \\cdot \\begin{pmatrix} r &amp; s &amp; t\\\\ u &amp; v &amp; w\\\\ x &amp; y &amp; z \\end{pmatrix}\\\\ &amp;=&amp;\\begin{pmatrix} a \\cdot r + b \\cdot u + c \\cdot x &amp; a \\cdot s + b \\cdot v + c \\cdot y &amp; a \\cdot t + b \\cdot w + c \\cdot z\\\\ d \\cdot r + e \\cdot u + f \\cdot x &amp; d \\cdot s + e \\cdot v + f \\cdot y &amp; d \\cdot t + e \\cdot w + f \\cdot z\\\\ g \\cdot r + h \\cdot u + i \\cdot x &amp; g \\cdot s + h \\cdot v + i \\cdot y &amp; g \\cdot t + h \\cdot w + i \\cdot z \\end{pmatrix}\\\\ &amp;=&amp;\\begin{pmatrix} 34 &amp; 35 &amp; 34\\\\ 52 &amp; 47 &amp; 35\\\\ 30 &amp; 33 &amp; 40 \\end{pmatrix} \\end{eqnarray}\\] The result of a matrix multiplication has as many rows as the 1st matrix and as many columns as the 2nd. For this to work, the number of columns of the 1st matrix must match the number of rows of the 2nd matrix. In our example, this does not matter as the matrices are square, i.e. they have as many rows as columns. To construct each cell of the results matrix, one row of the 1st matrix is combined with one column of the 2nd matrix. For cell (1,1) (top-left), for example, we combine the 1st row of matrix 1 (here \\(\\mathbf{A}\\)) and the 1st column of matrix 2 (here \\(\\mathbf{B}\\)). Moving to the right, for cell (1,2) (top-middle), we combine the 1st row of matrix 1 and the 2nd column of matrix 2. For cell (2,1) (middle-left), we combine the 2nd row of matrix 1 and the 1st column of matrix 2. And so on and so forth. The combination of the two respective vectors is the sum of the products of the vector elements paired in order. So for cell (1,1) in our example this is \\(a \\cdot r + b \\cdot u + c \\cdot x\\). Try and recreate the following example to get a feeling for the matrix multiplication rules. \\[\\begin{eqnarray} \\mathbf{B} \\cdot \\mathbf{A}&amp;=&amp; \\begin{pmatrix} r &amp; s &amp; t\\\\ u &amp; v &amp; w\\\\ x &amp; y &amp; z \\end{pmatrix} \\cdot \\begin{pmatrix} a &amp; b &amp; c\\\\ d &amp; e &amp; f\\\\ g &amp; h &amp; i \\end{pmatrix}\\\\ &amp;=&amp;\\begin{pmatrix} r \\cdot a + s \\cdot d + t \\cdot g &amp; r \\cdot b + s \\cdot e + t \\cdot h &amp; r \\cdot c + s \\cdot f + t \\cdot i\\\\ u \\cdot a + v \\cdot d + w \\cdot g &amp; u \\cdot b + v \\cdot e + w \\cdot h &amp; u \\cdot c + v \\cdot f + w \\cdot i\\\\ x \\cdot a + y \\cdot d + z \\cdot g &amp; x \\cdot b + y \\cdot e + z \\cdot h &amp; x \\cdot c + y \\cdot f + z \\cdot i \\end{pmatrix}\\\\ &amp;=&amp;\\begin{pmatrix} 25 &amp; 17 &amp; 24\\\\ 69 &amp; 44 &amp; 64\\\\ 42 &amp; 26 &amp; 52 \\end{pmatrix} \\end{eqnarray}\\] The point with this example is that \\(\\mathbf{A} \\cdot \\mathbf{B}\\) is not the same as \\(\\mathbf{B} \\cdot \\mathbf{A}\\). The order matters when multiplying matrices! Let’s consider two more example: \\[\\begin{eqnarray} \\mathbf{A} \\cdot \\mathbf{A}&amp;=&amp; \\begin{pmatrix} a &amp; b &amp; c\\\\ d &amp; e &amp; f\\\\ g &amp; h &amp; i \\end{pmatrix} \\cdot \\begin{pmatrix} a &amp; b &amp; c\\\\ d &amp; e &amp; f\\\\ g &amp; h &amp; i \\end{pmatrix}\\\\ &amp;=&amp;\\begin{pmatrix} a^2 + b \\cdot d + c \\cdot g &amp; a \\cdot b + b \\cdot e + c \\cdot h &amp; a \\cdot c + b \\cdot f + c \\cdot i\\\\ d \\cdot a + e \\cdot d + f \\cdot g &amp; d \\cdot b + e^2 + f \\cdot h &amp; d \\cdot c + e \\cdot f + f \\cdot i\\\\ g \\cdot a + h \\cdot d + i \\cdot g &amp; g \\cdot b + h \\cdot e + i \\cdot h &amp; g \\cdot c + h \\cdot f + i^2 \\end{pmatrix}\\\\ &amp;=&amp;\\begin{pmatrix} 27 &amp; 16 &amp; 44\\\\ 56 &amp; 39 &amp; 28\\\\ 11 &amp; 2 &amp; 68 \\end{pmatrix} \\end{eqnarray}\\] This matrix multiplied with itself, \\(\\mathbf{A} \\cdot \\mathbf{A}\\), is different to the same matrix multiplied with its transpose \\(\\mathbf{A} \\cdot \\mathbf{A}&#39;\\): \\[\\begin{eqnarray} \\mathbf{A} \\cdot \\mathbf{A}&#39;&amp;=&amp; \\begin{pmatrix} a &amp; b &amp; c\\\\ d &amp; e &amp; f\\\\ g &amp; h &amp; i \\end{pmatrix} \\cdot \\begin{pmatrix} a &amp; d &amp; g\\\\ b &amp; e &amp; h\\\\ c &amp; f &amp; i \\end{pmatrix}\\\\ &amp;=&amp;\\begin{pmatrix} a^2 + b^2 + c^2 &amp; a \\cdot d + b \\cdot e + c \\cdot f &amp; a \\cdot g + b \\cdot h + c \\cdot i\\\\ d \\cdot a + e \\cdot b + f \\cdot c &amp; d^2 + e^2 + f^2 &amp; d \\cdot g + e \\cdot h + f \\cdot i\\\\ g \\cdot a + h \\cdot b + i \\cdot c &amp; g \\cdot d + h \\cdot e + i \\cdot f &amp; g^2 + h^2 + i^2 \\end{pmatrix}\\\\ &amp;=&amp;\\begin{pmatrix} 29 &amp; 31 &amp; 35\\\\ 31 &amp; 74 &amp; 7\\\\ 35 &amp; 7 &amp; 65 \\end{pmatrix} \\end{eqnarray}\\] This last matrix is symmetrical, i.e. mirrored along its diagonal. Diagonal elements are so called sums of squares, off-diagonal elements are so called cross-products. This will be useful later on when working with variance-covariance matrices in Chapter 8. 2.4.3 Matrix division, inverse of a matrix, identity matrix Division of two matrices means multiplication of one matrix with the so called inverse of the other, here \\(\\mathbf{B}^{-1}\\): \\[\\frac{\\mathbf{A}}{\\mathbf{B}} = \\mathbf{A} \\cdot \\mathbf{B}^{-1}\\] The inverse of a matrix is different to the transpose. It is rather complicated to calculate, using in most cases numerical (and not analytical) techniques that we do not go into here. At the most general level, the inverse is found so that the following equation holds: \\[\\mathbf{A} \\cdot \\mathbf{A}^{-1} = \\mathbf{A}^{-1} \\cdot \\mathbf{A} = \\mathbf{I}\\] With \\(\\mathbf{I}\\) being the identity matrix, i.e. a matrix with diagonal elements 1 and off-diagonal elements 0: \\[\\mathbf{I} = \\begin{pmatrix} 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix} \\] 2.5 Exercises Exercise 1 Apply the rules in the script to the following equation to get rid of the product operator, and then simplify the resultant equation as much as you can: \\[\\begin{equation} \\prod_{i=1}^{n}\\frac{1}{\\sigma \\cdot \\sqrt{2 \\cdot \\pi}} \\cdot \\exp\\left(\\frac{\\left(y_i - \\beta_0 - \\beta_1 \\cdot x_i\\right)^2}{-2 \\cdot \\sigma^2}\\right)= \\tag{2.23} \\end{equation}\\] Exercise 2 Apply the differentiation rules in the script to take the derivative of the following equation: \\[\\begin{equation} \\frac{d\\sum_{i=1}^{n}\\left(y_i - \\beta_0 - \\beta_1 \\cdot x_i\\right)^2}{d\\beta_0}= \\tag{2.24} \\end{equation}\\] Exercise 3 Consider the following vectors and matrix: \\(y = \\begin{pmatrix} y_1\\\\ y_2\\\\ y_3 \\end{pmatrix}\\), \\(\\beta = \\begin{pmatrix} \\beta_0\\\\ \\beta_1\\\\ \\beta_2\\\\ \\beta_3 \\end{pmatrix}\\), \\(\\epsilon = \\begin{pmatrix} \\epsilon_1\\\\ \\epsilon_2\\\\ \\epsilon_3 \\end{pmatrix}\\) and \\(\\mathbf{X} = \\begin{pmatrix} 1 &amp; x_{11} &amp; x_{12} &amp; x_{13}\\\\ 1 &amp; x_{21} &amp; x_{22} &amp; x_{23}\\\\ 1 &amp; x_{31} &amp; x_{32} &amp; x_{33} \\end{pmatrix}\\). Now solve the following equation using matrix algebra: \\[y = \\mathbf{X} \\cdot \\beta + \\epsilon = \\] References "],
["linreg.html", "Chapter 3 Linear regression 3.1 Motivation 3.2 The linear model 3.3 Description versus prediction 3.4 Linear Regression 3.5 Significance of regression 3.6 Confidence in parameter estimates 3.7 Goodness of fit", " Chapter 3 Linear regression 3.1 Motivation The questions we wish to answer with linear regression are of the kind depicted in Figure 1.1: What drives spatial variation in annual average precipitation and annual average temperature? In the case of precipitation the drivers seem to be continentality and elevation, while temperature appears to be dominantly controlled by elevation only. Linear regression examines this question by modelling a response variable against one or more predictor variables, while the relationship between the two is linear in its parameters. 3.2 The linear model The most general from of a linear model is: \\[\\begin{equation} y = \\beta_0 + \\sum_{j=1}^{p}\\beta_j \\cdot x_j + \\epsilon \\tag{3.1} \\end{equation}\\] In this equation, \\(y\\) is the response variable (also called dependent or output variable), \\(x_j\\) are the predictor variables (also called independent, explanatory, input variables or covariates), \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) are the parameters and \\(\\epsilon\\) is the residual, i.e. that part of the response which remains unexplained by the predictors. In the case of one predictor, which has come to be known as linear regression, the linear model is: \\[\\begin{equation} y = \\beta_0 + \\beta_1 \\cdot x + \\epsilon \\tag{3.2} \\end{equation}\\] It can be visualised as a line, with \\(\\beta_0\\) being the intercept, where the line intersects the vertical axis \\((x=0)\\), and \\(\\beta_1\\) being the slope of the line (Figure 3.1). Note, the point \\(\\left(\\bar{x},\\bar{y}\\right)\\), the centroid of the data, always lies on the line. Figure 3.1: Linear model with one predictor variable (linear regression). Linear means linear in terms of the model parameters, not (necessarily) in terms of the predictor variables. With this in mind, consider the following five models. Which are linear models, which are non-linear models? (Q1)1 \\[\\begin{equation} y = \\beta_0 + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 + \\epsilon \\tag{3.3} \\end{equation}\\] \\[\\begin{equation} y = \\beta_0 + \\beta_1 \\cdot x_1^{\\beta_2} + \\epsilon \\tag{3.4} \\end{equation}\\] \\[\\begin{equation} y = \\beta_0 + \\beta_1 \\cdot x_1^3 + \\beta_2 \\cdot x_1 \\cdot x_2 + \\epsilon \\tag{3.5} \\end{equation}\\] \\[\\begin{equation} y = \\beta_0 + \\exp(\\beta_1 \\cdot x_1) + \\beta_2 \\cdot x_2 + \\epsilon \\tag{3.6} \\end{equation}\\] \\[\\begin{equation} y = \\beta_0 + \\beta_1 \\cdot \\log x_1 + \\beta_2 \\cdot x_2 + \\epsilon \\tag{3.7} \\end{equation}\\] We can also write the linear model equation with the data points explicitly indexed by \\(i\\) for \\(i=1, 2, \\ldots, n\\). We have omitted the index previously for ease of reading: \\[\\begin{equation} y_i = \\beta_0 + \\sum_{j=1}^{p}\\beta_j \\cdot x_{ij} + \\epsilon_i \\tag{3.8} \\end{equation}\\] These data points could be repeat measurements in time or in space. We can also write the model more compactly in a matrix format: \\[\\begin{equation} y = \\mathbf{X} \\cdot \\beta + \\epsilon \\tag{3.9} \\end{equation}\\] With \\(y = \\begin{pmatrix} y_1\\\\ y_2\\\\ y_3 \\end{pmatrix}\\), \\(\\beta = \\begin{pmatrix} \\beta_0\\\\ \\beta_1\\\\ \\beta_2\\\\ \\beta_3 \\end{pmatrix}\\), \\(\\epsilon = \\begin{pmatrix} \\epsilon_1\\\\ \\epsilon_2\\\\ \\epsilon_3 \\end{pmatrix}\\) and \\(\\mathbf{X} = \\begin{pmatrix} 1 &amp; x_{11} &amp; x_{12} &amp; x_{13}\\\\ 1 &amp; x_{21} &amp; x_{22} &amp; x_{23}\\\\ 1 &amp; x_{31} &amp; x_{32} &amp; x_{33} \\end{pmatrix}\\), the latter being the design matrix which summarises the predictor data. When we talk about the linear model, the response variable is always continuous, while the predictor variables can be continuous, categorical or mixed. In principle, each of these variants can be treated mathematically in the same way, e.g. all can be analysed using the lm() function in R. However, historically different names have been established for these variants, which are worth mentioning here to avoid confusion (Tables 3.1 and 3.2). Table 3.1: Historical names for the variants of the linear model, depending on whether the predictors are continuous, categorical or mixed. The response is always continuous. Continuouspredictors Categoricalpredictors Mixedpredictors Regression Analysis of variance(ANOVA) Analysis of covariance(ANCOVA) Table 3.2: Historical names for the regression, depending on whether we have one or more predictors and one or more responses. 1 predictor variable &gt;1 predictor variables 1 response variable Regression Multiple regression &gt;1 response variables Multivariate regression Multivariate multiple regression 3.3 Description versus prediction The primary purpose of regression analysis is the description (or explanation) of the data in terms of a general relationship pertaining to the population that these data are sampled from. Being a property of the population, this relationship should then also allow us to make predictions, but we need to be careful. Consider the relationship between year and world record time for the men’s mile depicted in Figure 3.2. When the predictor is time, as shown here, regression becomes a form of trend analysis, in this case of how the record time in the male competition decreased over the years. We will talk about the R code and output further below, here we’re just interested in the linear predictions. # load mile data from remote repository mile &lt;- read.csv(&quot;https://raw.githubusercontent.com/avehtari/ROS-Examples/master/Mile/data/mile.csv&quot;, header=TRUE) # fit linear model to data from 1st half of 20th century fit1 &lt;- lm(seconds ~ year, data = mile[mile$year&lt;1950,]) # extract information about parameter estimates coef(summary(fit1)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 912.2339944 67.90139506 13.434687 3.614623e-08 ## year -0.3438721 0.03509494 -9.798337 9.058700e-07 # fit linear model to complete dataset fit2 &lt;- lm(seconds ~ year, data = mile) coef(summary(fit2)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1006.8760057 21.5319332 46.76199 1.360809e-29 ## year -0.3930488 0.0109992 -35.73431 3.779773e-26 # plot fit for 1st half of 20th century plot(mile$year[mile$year&lt;1950], mile$seconds[mile$year&lt;1950], xlim = c(1900, 2000), ylim = c(200, 260), pch = 19, type = &#39;p&#39;, xlab = &quot;Year&quot;, ylab = &quot;World record, men&#39;s mile (seconds)&quot;) abline(coef(fit1), lwd = 3, col = &quot;red&quot;) # plot extrapolation to 2nd half of 20th century plot(mile$year, mile$seconds, xlim = c(1900, 2000), ylim = c(200, 260), pch = 19, type = &#39;p&#39;, xlab = &quot;Year&quot;, ylab = &quot;World record, men&#39;s mile (seconds)&quot;) abline(coef(fit1), lwd = 3, col = &quot;red&quot;) # plot all-data fit until 2050 plot(mile$year, mile$seconds, xlim = c(1900, 2050), ylim = c(200, 260), pch = 19, type = &#39;p&#39;, xlab = &quot;Year&quot;, ylab = &quot;World record, men&#39;s mile (seconds)&quot;) abline(coef(fit2), lwd = 3, col = &quot;red&quot;) Figure 3.2: Left: Trend of the world record for the men`s mile over the first half of the 20th century (description). Centre: Extrapolation of this trend over the 2nd half of the 20th century (prediction). Right: Extrapolation of the overall trend until the year 2050 (longer prediction). After: Wainer (2009) The world record for the men`s mile improved linearly over the first half of the 20th century (Figure 3.2, left). This trend provides a remarkably accurate fit for the second half of the century as well (Figure 3.2, centre). However, for how long can the world record continue to improve at the same rate (Figure 3.2, right)? This example clearly shows that the scope for prediction by regression lies within certain bounds, while highlighting the limits of these simple models for making distant predictions (e.g. in time and space). In the case of the world record we would expect the rate of improvement to decline with time, i.e. the world record to level off, which calls for a non-linear model. 3.4 Linear Regression Typically, regression problems are solved, i.e. the lines in Figures 3.1 and 3.2 are fitted to the data, by minimising the Sum of Squared Errors (SSE) between the regression line and the data points. This method has become known as Least Squares. Graphically, it means that in Figure 3.1 we try different lines with different intercepts \\(\\left(\\beta_0\\right)\\) and slopes \\(\\left(\\beta_1\\right)\\) and ultimately choose the one where the sum over all vertical distances \\(\\epsilon_i\\) squared is smallest. Mathematically, SSE is defined as: \\[\\begin{equation} SSE=\\sum_{i=1}^{n}\\left(\\epsilon_i\\right)^2=\\sum_{i=1}^{n}\\left(y_i-\\left(\\beta_0+\\beta_1 \\cdot x_i\\right)\\right)^2 \\tag{3.10} \\end{equation}\\] The terms \\(\\epsilon_i=y_i-\\left(\\beta_0+\\beta_1 \\cdot x_i\\right)\\) are called the residuals, i.e. that part of the variation in the data which the linear model cannot explain. In the case of linear regression, SSE can be minimised analytically, which is not the case for non-linear models, for example. Analytically, we find the minimum of SSE where its partial derivatives with respect to the two model parameters are both zero (compare Chapter 2): \\(\\frac{\\partial SSE}{\\partial \\beta_0}=0\\) and \\(\\frac{\\partial SSE}{\\partial \\beta_1}=0\\). Using the definition of SEE of Equation (3.10) we thus begin with a system of two differential equations: \\[\\begin{equation} \\frac{\\partial SSE}{\\partial \\beta_0}=-2 \\cdot \\sum_{i=1}^{n}\\left(y_i-\\beta_0-\\beta_1 \\cdot x_i\\right)=0 \\tag{3.11} \\end{equation}\\] \\[\\begin{equation} \\frac{\\partial SSE}{\\partial \\beta_1}=-2 \\cdot \\sum_{i=1}^{n}x_i \\cdot \\left(y_i-\\beta_0-\\beta_1 \\cdot x_i\\right)=0 \\tag{3.12} \\end{equation}\\] We have already calculated these derivatives in an exercise in Chapter 2 using the sum rule and the chain rule in particular. Since Equations (3.11) and (3.12) form a system of two differential equations with two unknowns (\\(\\beta_0\\) and \\(\\beta_1\\); the data points \\(x_i\\) and \\(y_i\\) are known) we can solve it exactly. First, we solve Equation (3.11) for \\(\\beta_0\\) (after dividing by -2): \\[\\begin{equation} \\sum_{i=1}^{n}y_i-n \\cdot \\beta_0-\\beta_1 \\cdot \\sum_{i=1}^{n}x_i=0 \\tag{3.13} \\end{equation}\\] \\[\\begin{equation} n \\cdot \\hat\\beta_0=\\sum_{i=1}^{n}y_i-\\hat\\beta_1 \\cdot \\sum_{i=1}^{n}x_i \\tag{3.14} \\end{equation}\\] \\[\\begin{equation} \\hat\\beta_0=\\bar{y}-\\hat\\beta_1 \\cdot \\bar{x} \\tag{3.15} \\end{equation}\\] Note, at some point we have renamed \\(\\beta_0\\) to \\(\\hat\\beta_0\\) and \\(\\beta_1\\) to \\(\\hat\\beta_1\\) to denote these as estimates. The parameter notation up to now has been general but as we approach actual numerical values for the data at hand we are using the “hat” symbol to signify that we are now calculating estimates of those general parameters for a given dataset. Second, we insert Equation (3.15) into Equation (3.12) (again after dividing by -2 and rearranging): \\[\\begin{equation} \\sum_{i=1}^{n}\\left(x_i \\cdot y_i-\\beta_0 \\cdot x_i-\\beta_1 \\cdot x_i^2\\right)=0 \\tag{3.16} \\end{equation}\\] \\[\\begin{equation} \\sum_{i=1}^{n}\\left(x_i \\cdot y_i-\\bar{y} \\cdot x_i+\\hat\\beta_1 \\cdot \\bar{x} \\cdot x_i-\\hat\\beta_1 \\cdot x_i^2\\right)=0 \\tag{3.17} \\end{equation}\\] Third, we solve Equation (3.17) for \\(\\beta_1\\): \\[\\begin{equation} \\sum_{i=1}^{n}\\left(x_i \\cdot y_i-\\bar{y} \\cdot x_i\\right)-\\hat\\beta_1 \\cdot \\sum_{i=1}^{n}\\left(x_i^2-\\bar{x} \\cdot x_i\\right)=0 \\tag{3.18} \\end{equation}\\] \\[\\begin{equation} \\hat\\beta_1=\\frac{\\sum_{i=1}^{n}\\left(x_i \\cdot y_i-\\bar{y} \\cdot x_i\\right)}{\\sum_{i=1}^{n}\\left(x_i^2-\\bar{x} \\cdot x_i\\right)} \\tag{3.19} \\end{equation}\\] Via a series of steps that I skip here, we arrive at: \\[\\begin{equation} \\hat\\beta_1=\\frac{SSXY}{SSX} \\tag{3.20} \\end{equation}\\] Where \\(SSX=\\sum_{i=1}^{n}\\left(x_i-\\bar{x}\\right)^2\\) and \\(SSXY=\\sum_{i=1}^{n}\\left(x_i-\\bar{x}\\right) \\cdot \\left(y_i-\\bar{y}\\right)\\). Note, analogously \\(SSY=\\sum_{i=1}^{n}\\left(y_i-\\bar{y}\\right)^2\\). Equation (3.20) is an exact solution for \\(\\hat\\beta_1\\). We then insert Equation (3.20) back into Equation (3.10) and have an exact solution for \\(\\hat\\beta_0\\). 3.5 Significance of regression Having estimates for the regression parameters we need to ask ourselves whether these estimates are statistically significant or could have arisen by chance from the (assumed) random process of sampling the data. We do this via Analysis of Variance (ANOVA), which begins by constructing the ANOVA table (Table 3.3). This is often done in the background in software like R and not actually looked at that much. Table 3.3: ANOVA table for linear regression. Source Sum ofsquares Degrees of freedom \\((df)\\) Mean squares F statistic \\(\\left(F_s\\right)\\) \\(\\Pr\\left(Z\\geq F_s\\right)\\) Regression \\(SSR=\\\\SSY-SSE\\) \\(1\\) \\(\\frac{SSR}{df_{SSR}}\\) \\(\\frac{\\frac{SSR}{df_{SSR}}}{s^2}\\) \\(1-F\\left(F_s,1,n-2\\right)\\) Error \\(SSE\\) \\(n-2\\) \\(\\frac{SSE}{df_{SSE}}=s^2\\) Total \\(SSY\\) \\(n-1\\) In the second column of Table 3.3, \\(SSY=\\sum_{i=1}^{n}\\left(y_i-\\bar{y}\\right)^2\\) is a measure of the total variance of the data, i.e. how much the data points are varying around the overall mean (Figure 3.3, left). \\(SSE=\\sum_{i=1}^{n}\\left(\\epsilon_i\\right)^2=\\sum_{i=1}^{n}\\left(y_i-\\left(\\beta_0+\\beta_1 \\cdot x_i\\right)\\right)^2\\) is a measure of the error variance, i.e. how much the data points are varying around the regression line (Figure 3.3, right). This is the variance not explained by the model. \\(SSR=SSY-SSE\\) then is a measure of the variance explained by the model. Figure 3.3: Variation of the data points around the mean, summarised by \\(SSY\\) (left), and around the regression line, summarised by \\(SSE\\) (right). The third column of Table 3.3 lists the so called degrees of freedom of the three variance terms, which can be understood as the number of free parameters for the respective term that is controlled by the (assumed) random process of sampling the data. It is the number of possibilities for the chance process to unfold. \\(SSY\\) requires one parameter \\(\\left(\\bar{y}\\right)\\) to be calculated from the data (see above). Hence the degrees of freedom are \\(n-1\\); if I know \\(\\bar{y}\\) then there are \\(n-1\\) data points left that can be generated by chance, the nth one I can calculate from all the others and \\(\\bar{y}\\). \\(SSE\\), in turn, requires two parameters (\\(\\beta_0\\) and \\(\\beta_1\\)) to be calculated from the data (Equations (3.15) and (3.20)). Hence the degrees of freedom are \\(n-2\\). The degrees of freedom of \\(SSR\\) then are just the difference between the former two; \\(df_{SSR}=df_{SSY}-df_{SSE}=1\\). The degrees of freedom are used to normalise the variance terms in the fourth column of Table 3.3, where \\(s^2\\) is called the error variance. In the fifth column of Table 3.3 we find the ratio of two variances; regression variance over error variance. Naturally, for a significant regression we want the regression variance (explained by the model) to be much larger than the error variance (unexplained by the model). This is an F-Test problem, testing whether the variance explained by the model is significantly different from the variance unexplained by the model. The ratio of the two variances serves as the F statistic \\(\\left(F_s\\right)\\). The sixth column of Table 3.3 then shows the p-value of the F-Test, i.e. the probability of getting \\(F_s\\) or a larger value (i.e. an even better model) by chance if the Null hypothesis \\(\\left(H_0\\right)\\)) is true. \\(H_0\\) here is that the two variances are equal. It can be shown mathematically that \\(F_s\\) follows an F-distribution with parameters \\(1\\) and \\(n-2\\) under the Null hypothesis (Figure 3.4). The red line in Figure 3.4 marks a particular value of \\(F_s\\) (between 10 and 11) and the corresponding value of the cumulative distribution function of the F-distribution \\(\\left(F\\left(F_s,1,n-2\\right)\\right)\\). The p-value is \\(\\Pr\\left(Z\\geq F_s\\right)=1-F\\left(F_s,1,n-2\\right)\\), i.e. the probability of getting this variance ratio or a greater one by chance (due to the random sampling process) even if the two variances are actually equal. Here this value is very small and hence we conclude that the regression is significant. Figure 3.4: Cumulative distribution function (CDF) of the F-distribution of the F statistic \\(\\left(F_s\\right)\\), with a particular value and corresponding value of the CDF marked in red. The correct interpretation of the p-value is a bit tricky. In the words of philosopher of science Ian Hacking (2001), if we have a p-value of say 0.01 this means “either the Null hypothesis is true, in which case something unusual happened by chance (probability 1%), or the Null hypothesis is false.” This means, strictly speaking, the p-value is not the probability of the Null hypothesis being true; it is the probability of the data to come about if the Null hypothesis were true. If this is a very low probability then we think this tells us something about the Null hypothesis (that perhaps we should reject it), but in a roundabout way. Note, in the case of linear regression, the Null model is \\(\\beta_1=0\\), i.e. \\(y=\\beta_0\\) with \\(\\hat\\beta_0=\\bar{y}\\), which means the overall mean is the best model summarising the data (Figure 3.3, left). 3.6 Confidence in parameter estimates Having established the statistical significance of the regression, we should look at the uncertainty around the parameter estimates. In classic linear regression this uncertainty is conceptualised as arising purely from the random sampling process; the data at hand are just one possibility of many, and in each alternative case the parameter estimates would have turned out slightly different. The linear model itself is assumed to be correct. The first step in establishing how confident we should be that the parameter estimates are correct is the calculation of standard errors. For \\(\\hat\\beta_0\\) this is (derivation not shown here): \\[\\begin{equation} s_{\\hat\\beta_0}=\\sqrt{\\frac{\\sum_{i=1}^{n}x_i^2}{n} \\cdot \\frac{s^2}{SSX}} \\tag{3.21} \\end{equation}\\] Breaking down this formula into its individual parts, we can see that the more data points \\(n\\) we have, the smaller the standard error, i.e. the more confidence we have in the estimate. Also, the larger the variation in \\(x\\) \\((SSX)\\) the smaller the standard error. Both effects make intuitive sense: the more data points we have and the more possibilities for \\(x\\) we have covered, the more we can be confident that we have not missed much in our random sample. Conversely, the larger the error variance \\(s^2\\), i.e. the smaller the explanatory power of our model, the larger the standard error. And, the more \\(x\\) data points we have away from zero, i.e. the greater \\(\\sum_{i=1}^{n}x_i^2\\), the smaller our confidence in the intercept (where \\(x=0\\)) and hence the standard error increases. The standard error for \\(\\hat\\beta_1\\) is: \\[\\begin{equation} s_{\\hat\\beta_1}=\\sqrt{\\frac{s^2}{SSX}} \\tag{3.22} \\end{equation}\\] The same interpretation applies, except there is no influence of the magnitude of the \\(x\\) data points. We can also establish a standard error for new predictions \\(\\hat y\\) for given new predictor values \\(\\hat x\\): \\[\\begin{equation} s_{\\hat y}=\\sqrt{s^2 \\cdot \\left(\\frac{1}{n}+\\frac{\\left(\\hat x-\\bar x\\right)^2}{SSX}\\right)} \\tag{3.23} \\end{equation}\\] The same interpretation applies again, except there now is an added term \\(\\left(\\hat x-\\bar x\\right)^2\\) which means the further the new \\(x\\) value is away from the centre of the original data (the training or calibration data) the greater the standard error of the new prediction, i.e. the lower the confidence in it being correct. Note, the formulae for the standard errors arise from the fundamental assumptions of linear regression, which will be covered below. This can be shown mathematically but is omitted here. From the standard errors we can calculate confidence intervals for the parameter estimates as follows: \\[\\begin{equation} \\Pr\\left(\\hat\\beta_0-t_{n-2;0.975} \\cdot s_{\\hat\\beta_0}\\leq \\beta_0\\leq \\hat\\beta_0+t_{n-2;0.975} \\cdot s_{\\hat\\beta_0}\\right)=0.95 \\tag{3.24} \\end{equation}\\] The symbol \\(\\Pr(\\cdot)\\) means probability. The symbol \\(t_{n-2;0.975}\\) stands for the 0.975-percentile of the t-distribution with \\(n-2\\) degrees of freedom. Equation (3.24) is the central 95% confidence interval, which is defined as the bounds in which the true parameter, here \\(\\beta_0\\), lies with a probability of 0.95. We can write the interval like this: \\[\\begin{equation} CI=\\left[\\hat\\beta_0-t_{n-2;0.975} \\cdot s_{\\hat\\beta_0};\\hat\\beta_0+t_{n-2;0.975} \\cdot s_{\\hat\\beta_0}\\right] \\tag{3.25} \\end{equation}\\] As can be seen, the confidence interval \\(CI\\) is symmetric around the parameter estimate \\(\\hat\\beta_0\\) and arises from a t-distribution with parameter \\(n-2\\) whose width is modulated by the standard error \\(s_{\\hat\\beta_0}\\). Note, the width of the t-distribution is also controlled by sample size, becoming narrower with increasing \\(n\\). The same formulae apply for \\(\\beta_1\\) and \\(y\\): \\[\\begin{equation} \\Pr\\left(\\hat\\beta_1-t_{n-2;0.975} \\cdot s_{\\hat\\beta_1}\\leq \\beta_1\\leq \\hat\\beta_1+t_{n-2;0.975} \\cdot s_{\\hat\\beta_1}\\right)=0.95 \\tag{3.26} \\end{equation}\\] \\[\\begin{equation} \\Pr\\left(\\hat y-t_{n-2;0.975} \\cdot s_{\\hat y}\\leq y\\leq \\hat y+t_{n-2;0.975} \\cdot s_{\\hat y}\\right)=0.95 \\tag{3.27} \\end{equation}\\] As with the p-values, we need to be clear about the meaning of probability here, which in classic statistics is predicated on the repeated sampling principle. The meaning of the 95% confidence interval then is that in an assumed infinite number of regression experiments the 95% confidence interval captures the true parameter value in 95% of the cases. Again, this is not a probability of the true parameter value lying within the confidence interval for any one experiment! The formulae for the confidence intervals (Equations (3.24), (3.26) and (3.27)) arise from the fundamental assumptions of linear regression; the residuals are independent identically distributed (iid) according to a normal distribution and the linear model is correct. Then it can be shown mathematically that \\(\\frac{\\hat\\beta_0-\\beta_0}{s_{\\hat\\beta_0}}\\), \\(\\frac{\\hat\\beta_1-\\beta_1}{s_{\\hat\\beta_1}}\\) and \\(\\frac{\\hat y-y}{s_{\\hat y}}\\) are \\(t_{n-2}\\)-distributed (t-distribution with \\(n-2\\) degrees of freedom). Since the central 95% confidence interval of an arbitrary \\(t_{n-2}\\)-distributed random variable \\(Z\\) is \\(\\Pr\\left(-t_{n-2;0.975}\\leq Z\\leq t_{n-2;0.975}\\right)=0.95\\) (Figure 3.5), we can substitute any of the aforementioned three terms for \\(Z\\) and rearrange to arrive at Equations (3.24), (3.26) and (3.27). Figure 3.5: Left: Probability density function (PDF) of a t-distributed random variable \\(Z\\), with central 95% confidence interval marked in red. 95% of the PDF lies between the two bounds, 2.5% lies left of the lower bound and 2.5% right of the upper bound. Right: Cumulative distribution function (CDF) of the same t-distributed random variable \\(Z\\). The upper bound of the 95% confidence interval is defined as \\(t_{n-2;0.975}\\), i.e. the 0.975-percentile of the distribution, while the lower bound is defined as \\(t_{n-2;0.025}\\), which is equivalent to \\(-t_{n-2;0.975}\\) due to the symmetry of the distribution. The t-distribution property of the parameter estimates can further be exploited to test each parameter estimate separately for its statistical significance. This becomes especially important for multiple regression problems where we have more than one possible predictor, not all of which will have a statistically significant effect. The significance of the parameter estimates is determined via a t-test. The Null hypothesis is that the true parameters are zero, i.e. the parameter estimates are not significant: \\[\\begin{equation} H_0:\\beta_0=0 \\tag{3.28} \\end{equation}\\] \\[\\begin{equation} H_0:\\beta_1=0 \\tag{3.29} \\end{equation}\\] This hypothesis is tested against the alternative hypothesis that the true parameters are different from zero, i.e. the parameter estimates are significant: \\[\\begin{equation} H_1:\\beta_0\\neq 0 \\tag{3.30} \\end{equation}\\] \\[\\begin{equation} H_1:\\beta_1\\neq 0 \\tag{3.31} \\end{equation}\\] The test statistics are: \\[\\begin{equation} t_s=\\frac{\\hat\\beta_0-0}{s_{\\hat\\beta_0}}\\sim t_{n-2} \\tag{3.32} \\end{equation}\\] \\[\\begin{equation} t_s=\\frac{\\hat\\beta_1-0}{s_{\\hat\\beta_1}}\\sim t_{n-2} \\tag{3.33} \\end{equation}\\] The “tilde” symbol \\((\\sim)\\) means the test statistics follow a certain distribution, here the t-distribution. This arises again from the regression assumptions noted above. The assumptions are the same as for the common t-test of means, except in the case of linear regression the residuals are assumed iid normal while in the case of means the actual data points \\(y\\) are assumed iid normal. Analogous to the common 2-sided t-test, the p-value is defined as: \\[\\begin{equation} 2 \\cdot \\Pr\\left(t&gt;|t_s|\\right)=2 \\cdot \\left(1-F_t\\left(|t_s|\\right)\\right) \\tag{3.34} \\end{equation}\\] The symbol \\(F_t\\left(|t_s|\\right)\\) signifies the value of the CDF of the t-distribution at the location of the absolute value of the test statistic (\\(|t_s|\\), Figure 3.6). With a significance level of say \\(\\alpha=0.05\\) we arrive at critical values of the test statistic \\(t_c=t_{n-2;0.975}\\) and \\(-t_c\\) beyond which we reject the Null hypothesis and call the parameter estimates significant (Figure 3.6). Figure 3.6: Schematic of the t-test of significance of parameter estimates. The test statistic follows a t-distribution under the Null hypothesis. The actual value of the test statistic \\(t_s\\) is marked in blue and mirrored at zero for the 2-sided test. The critical value of the test statistic \\(t_c\\), which we get from a significance level of \\(\\alpha=0.05\\), is marked in red; this too is mirrored for the 2-sided test. We reject the Null hypothesis if \\(|t_s|&gt;t_c\\), i.e. for values of \\(t_s\\) below \\(-t_c\\) and above \\(t_c\\), and then call this parameter estimate significant. We keep the Null hypothesis if \\(|t_s|\\leq t_c\\), i.e. for values of \\(t_s\\) between \\(-t_c\\) and \\(t_c\\), and then call this parameter estimate insignificant (for now). In the example shown the parameter estimate is insignificant. 3.7 Goodness of fit The final step in regression analysis is assessing the goodness of fit of the linear model. In the first instance this may be done through the coefficient of determination \\(\\left(r^2\\right)\\), which is defined as the proportion of variation (in y-direction) that is explained by the model: \\[\\begin{equation} r^2=\\frac{SSY-SSE}{SSY}=1-\\frac{SSE}{SSY} \\tag{3.35} \\end{equation}\\] As can be seen, when the model fails to explain more variation than the total variation around the mean, i.e. \\(SSE=SSY\\), then \\(r^2=0\\). Conversely, when the model fits the data perfectly, i.e. \\(SSE=0\\), then \\(r^2=1\\). Any value in between signifies varying levels of goodness of fit. This can be visualised again with Figure 3.3, with the left panel signifying \\(SSY\\) and the right panel \\(SSE\\). When it comes to comparing models of varying complexity (i.e. with more or less parameters) using \\(r^2\\), then penalising the metric by the number of model parameters makes sense since more complex models (more parameters) automatically lead to better fits, simply due to the greater degrees of freedom that more complex models have for fitting the data. This leads to the adjusted \\(r^2\\): \\[\\begin{equation} \\bar r^2=1-\\frac{\\frac{SSE}{df_{SSE}}}{\\frac{SSY}{df_{SSY}}}=1-\\frac{SSE}{SSY} \\cdot \\frac{df_{SSY}}{df_{SSE}} \\tag{3.36} \\end{equation}\\] The coefficient of determination alone, however, is insufficient for assessing goodness of fit. Consider the four datasets depicted in Figure 3.7, which together form the Anscombe (1973) dataset. # plot 4 individual datasets plot(anscombe$x1, anscombe$y1, xlim = c(0, 20), ylim = c(0, 14), pch = 19, type = &#39;p&#39;) plot(anscombe$x2, anscombe$y2, xlim = c(0, 20), ylim = c(0, 14), pch = 19, type = &#39;p&#39;) plot(anscombe$x3, anscombe$y3, xlim = c(0, 20), ylim = c(0, 14), pch = 19, type = &#39;p&#39;) plot(anscombe$x4, anscombe$y4, xlim = c(0, 20), ylim = c(0, 14), pch = 19, type = &#39;p&#39;) Figure 3.7: The four Anscombe (1973) datasets. The individual datasets have purposely been constructed to yield virtually the same parameter estimates and coefficients of determination, despite wildly different relationships between \\(x\\) and \\(y\\) (Figure 3.8): # perform individual regressions fit1 &lt;- lm(y1 ~ x1, data = anscombe) fit2 &lt;- lm(y2 ~ x2, data = anscombe) fit3 &lt;- lm(y3 ~ x3, data = anscombe) fit4 &lt;- lm(y4 ~ x4, data = anscombe) # extract information about parameter estimates and R2 coef(summary(fit1)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0000909 1.1247468 2.667348 0.025734051 ## x1 0.5000909 0.1179055 4.241455 0.002169629 summary(fit1)$r.squared ## [1] 0.6665425 coef(summary(fit2)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.000909 1.1253024 2.666758 0.025758941 ## x2 0.500000 0.1179637 4.238590 0.002178816 summary(fit2)$r.squared ## [1] 0.666242 coef(summary(fit3)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0024545 1.1244812 2.670080 0.025619109 ## x3 0.4997273 0.1178777 4.239372 0.002176305 summary(fit3)$r.squared ## [1] 0.666324 coef(summary(fit4)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0017273 1.1239211 2.670763 0.025590425 ## x4 0.4999091 0.1178189 4.243028 0.002164602 summary(fit4)$r.squared ## [1] 0.6667073 In these summary tables, “(Intercept)” stands for \\(\\beta_0\\), while “x1” to “x4” stand for \\(\\beta_1\\). The column “Estimate” gives \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\), the column “Std. Error” gives \\(s_{\\hat\\beta_0}\\) and \\(s_{\\hat\\beta_1}\\), the column “t value” gives the individual \\(t_s\\) and the column “Pr(&gt;|t|)” gives the respective p-value. # plot individual datasets with regression lines plot(anscombe$x1, anscombe$y1, xlim = c(0, 20), ylim = c(0, 14), pch = 19, type = &#39;p&#39;) abline(coef(fit1), lwd = 3, col = &quot;red&quot;) plot(anscombe$x2, anscombe$y2, xlim = c(0, 20), ylim = c(0, 14), pch = 19, type = &#39;p&#39;) abline(coef(fit2), lwd = 3, col = &quot;red&quot;) plot(anscombe$x3, anscombe$y3, xlim = c(0, 20), ylim = c(0, 14), pch = 19, type = &#39;p&#39;) abline(coef(fit3), lwd = 3, col = &quot;red&quot;) plot(anscombe$x4, anscombe$y4, xlim = c(0, 20), ylim = c(0, 14), pch = 19, type = &#39;p&#39;) abline(coef(fit4), lwd = 3, col = &quot;red&quot;) Figure 3.8: Regression analysis of the four Anscombe (1973) datasets, yielding virtually the same parameter estimates and coefficients of determination (see above), despite wildly different relationships between \\(x\\) and \\(y\\). The coefficient of determination is insensitive to these and similar systematic deviations from the regression line. But we can detect these deficiencies of the model by looking at plots like Figure 3.8, and more generally by performing residual diagnostics that check model assumptions. The fundamental assumptions of linear regression are: The residuals are independent, in which case there will be no serial correlation in the residual plot – this can be tested using the Durbin-Watson test The residuals are normally distributed – this can be visually assessed using the quantile-quantile plot (QQ plot) and the residual histogram, and can be tested using the Kolmogorov-Smirnov test and the Shapiro-Wilk test The variance is the same across residuals, i.e. residuals are homoscedastic, in which case there is no “fanning out” of the residuals If these assumptions are not met then we can resort to data transformation, weighted regression or Generalised Linear Models (this is the preferred option), which we will cover in chapter 7. A first useful diagnostic plot is of the residuals in series, i.e. by index \\(i\\), to see if there is a pattern due to the data collection process (Figure 3.9). For the Anscombe dataset, this detects the nonlinearity in dataset 2 (top-right) and the outlier in dataset 3 (bottom-left), compare Figure 3.8. # plot residuals against index plot(residuals(fit1), xlim = c(0, 12), ylim = c(-2, 2), pch = 19, type = &#39;p&#39;) abline(h = 0, lwd = 3, col = &quot;red&quot;) plot(residuals(fit2), xlim = c(0, 12), ylim = c(-2, 2), pch = 19, type = &#39;p&#39;) abline(h = 0, lwd = 3, col = &quot;red&quot;) plot(residuals(fit3), xlim = c(0, 12), ylim = c(-2, 4), pch = 19, type = &#39;p&#39;) abline(h = 0, lwd = 3, col = &quot;red&quot;) plot(residuals(fit4), xlim = c(0, 12), ylim = c(-2, 2), pch = 19, type = &#39;p&#39;) abline(h = 0, lwd = 3, col = &quot;red&quot;) Figure 3.9: Anscombe (1973) datasets. Plot of residuals in series, i.e. by index \\(i\\). We should also plot the residuals by predicted value of \\(y\\) to see if there is a pattern as a function of magnitude (Figure 3.10). For the Anscombe dataset, this emphasizes the non-linearity of dataset 2 (top-right) and the outlier in dataset 3 (bottom-left) and also detects the singular extreme point in dataset 4 (bottom-right). In sum, the independence and homoscedasticity assumptions seem to be violated in all datasets except dataset 1. This would have to be formally tested using the Durbin-Watson test, for example. # plot residuals against predicted value of y plot(fitted.values(fit1),residuals(fit1), xlim = c(0, 14), ylim = c(-2, 2), pch = 19, type = &#39;p&#39;) abline(h = 0, lwd = 3, col = &quot;red&quot;) plot(fitted.values(fit2),residuals(fit2), xlim = c(0, 14), ylim = c(-2, 2), pch = 19, type = &#39;p&#39;) abline(h = 0, lwd = 3, col = &quot;red&quot;) plot(fitted.values(fit3),residuals(fit3), xlim = c(0, 14), ylim = c(-2, 4), pch = 19, type = &#39;p&#39;) abline(h = 0, lwd = 3, col = &quot;red&quot;) plot(fitted.values(fit4),residuals(fit4), xlim = c(0, 14), ylim = c(-2, 2), pch = 19, type = &#39;p&#39;) abline(h = 0, lwd = 3, col = &quot;red&quot;) Figure 3.10: Anscombe (1973) datasets. Plot of residuals by predicted value of \\(y\\). The normality assumption can be assessed using the QQ plot (Figure 3.11). # QQ plots qqnorm(residuals(fit1), xlim = c(-4, 4), ylim = c(-4, 4)) qqline(residuals(fit1)) qqnorm(residuals(fit2), xlim = c(-4, 4), ylim = c(-4, 4)) qqline(residuals(fit2)) qqnorm(residuals(fit3), xlim = c(-4, 4), ylim = c(-4, 4)) qqline(residuals(fit3)) qqnorm(residuals(fit4), xlim = c(-4, 4), ylim = c(-4, 4)) qqline(residuals(fit4)) Figure 3.11: Anscombe (1973) datasets. Quantile-quantile plot (QQ plot) of residuals. In the QQ plot, every data point represents a certain quantile of the empirical distribution. This quantile (after standardisation) is plotted (vertical axis) against the value of that quantile expected under a standard normal distribution (horizontal axis). The resultant shapes say something about the distribution of the residuals (Figure 3.12), e.g. in case of a normal distribution they all fall on a straight line. In the Anscombe dataset, the only clearly non-normal dataset seems to be #3 (bottom-left). This would have to be formally tested using the Kolmogorov-Smirnov test or the Shapiro-Wilk test, for example. # 1) simulate normal data x_norm &lt;- rnorm(100, 0, 1) # QQ plot qqnorm(x_norm, xlim = c(-4, 4), ylim = c(-4, 4), main=&#39;Normal data&#39;) qqline(x_norm) # 2) simulate right-skewed data x_right &lt;- rlnorm(100, 0, 1) # QQ plot qqnorm(x_right, xlim = c(-4, 4), ylim = c(-2, 6), main=&#39;Right-skewed data&#39;) qqline(x_right) # 3) simulate left-skewed data x_left &lt;- -x_right # QQ plot qqnorm(x_left, xlim = c(-4, 4), ylim = c(-6, 2), main=&#39;Left-skewed data&#39;) qqline(x_left) # 4) simulate thick-tailed data x_thick &lt;- rcauchy(100, 0, 1) # QQ plot qqnorm(x_thick, xlim = c(-4, 4), ylim = c(-8, 8), main=&#39;Thick-tailed data&#39;) qqline(x_thick) Figure 3.12: Characteristic shapes of the QQ plot and what they mean for the residuals in our case. Note, I couldn’t find an easy distribution that has thinner tails than the normal, but such case would exhibit an S-shape, just like the thick-tailed variant mirrored on the diagonal line. The non-normality of dataset 3 becomes apparent also in the residual histograms (Figure 3.13). They also emphasize the outlier in dataset 3. Note, it is generally difficult to reject the hypothesis of normally distributed residuals with so few data points. # histograms of residuals hist(residuals(fit1), breaks = seq(-4,4,0.5)) hist(residuals(fit2), breaks = seq(-4,4,0.5)) hist(residuals(fit3), breaks = seq(-4,4,0.5)) hist(residuals(fit4), breaks = seq(-4,4,0.5)) Figure 3.13: Anscombe (1973) datasets. Quantile-quantile plot (QQ plot) of residuals. References "],
["categoricalvars.html", "Chapter 4 Categorical predictors", " Chapter 4 Categorical predictors The linear model where the predictor variables are categorical - so called factors - has come to be known as Analysis of Variance (ANOVA). In this chapter we first introduce ANOVA in a classic sense before showing how this is essentially a special case of the linear model. As an example we will look at a dataset of crop yields for different soil types from Crawley (2012) - see Figure 4.1 - asking the question: Does soil type significantly affect crop yield? # load yields data yields &lt;- read.table(&quot;data/yields.txt&quot;,header=T) # means per soil type mu_j &lt;- sapply(list(yields$sand,yields$clay,yields$loam),mean) # overall mean mu &lt;- mean(mu_j) # boxplots of yield per soil type boxplot(yields, ylim = c(0, 20), ylab = &quot;yield&quot;) abline(h = mu, lwd = 3, col = &quot;red&quot;) points(seq(1,3), mu_j, pch = 23, lwd = 3, col = &quot;red&quot;) Figure 4.1: Boxplots of yield per soil type (the factor) with individual and overall means marked in red. Data from: Crawley (2012) The factor in this example is “soil type”. The categories of a factor are called levels, groups or treatments depending on the experimental setting and the research field. In the yields example the factor levels are “sand”, “clay” and “loam”. The parameters of an ANOVA are called effects - more below. ANOVA with one factor is called one-way ANOVA. The typical question answered by ANOVA is: Are means across factor levels significantly different? This is tested by comparing the variation between levels (i.e. the overlap or not between boxplots in Figure 4.1) to the variation within levels (i.e. the size of the individual boxplots). In the case of one factor with two levels, ANOVA is equivalent to the familiar t-test. The Linear effects model formulation for ANOVA is: \\[\\begin{equation} y_{ji}=\\mu+a_j+\\epsilon_{ji} \\tag{4.1} \\end{equation}\\] With \\(j=1, 2, \\ldots, k\\) indexing the levels (e.g. sand, clay, loam), \\(i=1, 2, \\ldots, n_j\\) indexing the data points at level \\(j\\), \\(y_{ji}\\) being the \\(i\\)th observation of the response variable (e.g. yield) at level \\(j\\), \\(\\mu\\) being the overall mean of the response variable (Figure 4.2, left), \\(\\mu_j\\) being the mean of the response variable at level \\(j\\), \\(a_j=\\mu_j-\\mu\\) being the effect of level \\(j\\), and \\(\\epsilon_{ji}\\) being the residual error. So effectively the response variable for each level is predicted by its mean plus random noise (Figure 4.2, right): \\[\\begin{equation} y_{ji}=\\mu_j+\\epsilon_{ji} \\tag{4.2} \\end{equation}\\] Figure 4.2: Left: Variation of data points around overall mean \\(\\mu\\), summarised by \\(SSY\\). Right: Variation of data points around individual means \\(\\mu_1, \\mu_2, \\mu_3\\), summarised by \\(SSE\\). To see how the ANOVA model is essentially a linear model we look at what is called dummy coding of the categorical predictor (here soil type). This is the original data table: yields ## sand clay loam ## 1 6 17 13 ## 2 10 15 16 ## 3 8 3 9 ## 4 6 11 12 ## 5 14 14 15 ## 6 17 12 16 ## 7 9 12 17 ## 8 11 8 13 ## 9 7 10 18 ## 10 11 13 14 Now we expand this to a long table with dummy or indicator variables “clay” and “loam”: yields2 &lt;- data.frame(yield = c(yields$sand, yields$clay, yields$loam), clay = c(rep(0,10), rep(1,10), rep(0,10)), loam = c(rep(0,10), rep(0,10), rep(1,10))) yields2 ## yield clay loam ## 1 6 0 0 ## 2 10 0 0 ## 3 8 0 0 ## 4 6 0 0 ## 5 14 0 0 ## 6 17 0 0 ## 7 9 0 0 ## 8 11 0 0 ## 9 7 0 0 ## 10 11 0 0 ## 11 17 1 0 ## 12 15 1 0 ## 13 3 1 0 ## 14 11 1 0 ## 15 14 1 0 ## 16 12 1 0 ## 17 12 1 0 ## 18 8 1 0 ## 19 10 1 0 ## 20 13 1 0 ## 21 13 0 1 ## 22 16 0 1 ## 23 9 0 1 ## 24 12 0 1 ## 25 15 0 1 ## 26 16 0 1 ## 27 17 0 1 ## 28 13 0 1 ## 29 18 0 1 ## 30 14 0 1 When “clay” is 1 and “loam” is 0 then we are in the clay category, when “clay” is 0 and “loam” is 1 then we are in the loam category, and if both are 0 we are in the “sand” category. This is visualised in 3D in Figure 4.3. Figure 4.3: 3D representation of the dummy coding of the yields dataset. Now we can use the familiar linear model, but with two predictors, to represent ANOVA: \\[\\begin{equation} y_i=\\beta_0+\\beta_1\\cdot x_{i1}+\\beta_2\\cdot x_{i2}+\\epsilon_i \\tag{4.3} \\end{equation}\\] In this formulation, \\(x_1\\) is the indicator variable “clay” and \\(x_2\\) is the indicator variable “loam”. Both can only take values of 0 and 1, they are binary. Hence, looking at Figure 4.3, when both \\(x_1\\) and \\(x_2\\) are 0 then we are in the front corner of the plot where we see the yield data for “sand”. The model is then reduced to \\(y_i=\\beta_0+\\epsilon_i\\), with \\(\\beta_0\\) being the mean yield for “sand”. When \\(x_1=0\\) and \\(x_2=1\\) then we are in the left corner of Figure 4.3 where we see the “loam” yields. The model is then \\(y_i=\\beta_0+\\beta_2+\\epsilon_i\\) with \\(\\beta_0+\\beta_2\\) being the mean yield for “loam”. Finally, when \\(x_1=1\\) and \\(x_2=0\\) then we are in the right corner of Figure 4.3 where we see the “clay” yields. The model is then \\(y_i=\\beta_0+\\beta_1+\\epsilon_i\\) with \\(\\beta_0+\\beta_1\\) being the mean yield for “clay”. \\(\\beta_1\\) and \\(\\beta_2\\) are thus increments added to what’s called the base category (in this case “sand”) to arrive at the new means for “clay” and “loam”, respectively. This is symbolised by the red lines in Figure 4.3. We are basically modelling unique means for each factor level; Equations (4.3) and (4.2) are equivalent. If you remember, we already looked at an ANOVA table under linear regression (chapter 3). Let’s now see how this is essentially similar to the actual ANOVA, while noting some key differences. Table 4.1 shows the one-way ANOVA table. Table 4.1: One-way ANOVA table. Source Sum ofsquares Degrees of freedom \\((df)\\) Mean squares F statistic \\(\\left(F_s\\right)\\) \\(\\Pr\\left(Z\\geq F_s\\right)\\) Level \\(SSA=\\\\SSY-SSE\\) \\(k-1\\) \\(\\frac{SSA}{df_{SSA}}\\) \\(\\frac{\\frac{SSA}{df_{SSA}}}{s^2}\\) \\(1-F\\left(F_s,1,n-k\\right)\\) Error \\(SSE\\) \\(n-k\\) \\(\\frac{SSE}{df_{SSE}}=s^2\\) Total \\(SSY\\) \\(n-1\\) Compare this to Table 3.3. The essential differences are: The explained variation is now labeled “level” with the notation \\(SSA\\), instead of “regression” and \\(SSR\\). The number of parameters subtracted from the data degrees of freedom is now \\(k\\), the number of levels, instead of 2. The error variation is now calculated as \\(SSE=\\sum_{j=1}^{k}\\sum_{i=1}^{n_j}\\left(y_{ji}-\\bar y_j\\right)^2\\) (Figure 4.2, right), instead of \\(SSE=\\sum_{i=1}^{n}\\left(y_i-\\left(\\beta_0+\\beta_1\\cdot x_i\\right)\\right)^2\\). The total variation, however, is the same: \\(SSY=\\sum_{j=1}^{k}\\sum_{i=1}^{n_j}\\left(y_{ji}-\\bar y\\right)^2=\\sum_{i=1}^{n}\\left(y_i-\\bar y\\right)^2\\) (Figure 4.2, left). For comparison, a perfect model fit would look like Figure 4.4. Figure 4.4: Hypothetical perfect fit of an ANOVA model. Left: Variation of data points around overall mean, summarised by \\(SSY\\). Right: Variation of data points around individual means, summarised by \\(SSE\\). When we estimate the individual means of the factor levels, they come with a standard error just like the linear regression parameters. This is: \\[\\begin{equation} s_{\\mu_j}=\\sqrt{\\frac{s^2}{n_j}} \\tag{4.4} \\end{equation}\\] With \\(n_j\\) being 10 in our example, which does not need to be the same across the factor levels, and \\(s^2=\\frac{SSE}{n-k}\\), the error variance from the ANOVA table (Table 4.1). Let’s calculate the various means, effect sizes and corresponding errors again: # define constants n &lt;- 30 k &lt;- 3 n_j &lt;- 10 # means per soil type mu_j &lt;- sapply(list(yields$sand,yields$clay,yields$loam),mean) mu_j ## [1] 9.9 11.5 14.3 # overall mean mu &lt;- mean(mu_j) mu ## [1] 11.9 # effect size a_j &lt;- mu_j - mu a_j ## [1] -2.0 -0.4 2.4 # SSE SSE &lt;- sum(sapply(list(yields$sand,yields$clay,yields$loam),function (x) sum((x-mean(x))^2) )) SSE ## [1] 315.5 # error variance s2 &lt;- SSE/(n-k) s2 ## [1] 11.68519 # standard error of individual means # here the same across factor levels as n_j is homogeneous s_mu_j &lt;- sqrt(s2/n_j) s_mu_j ## [1] 1.08098 Finally, what we are really interested in are the differences between factor level means \\(\\mu_j\\) and whether they are statistically significant, i.e. large enough compared to the variation within each factor level. This is a t-test problem (comparing two means) for which we need the differences of means, let’s call them \\(\\Delta\\mu_j\\), and the standard errors of these differences: \\[\\begin{equation} s_{\\Delta\\mu_j}=\\sqrt{\\frac{2\\cdot s^2}{n_j}} \\tag{4.5} \\end{equation}\\] The t-test statistics then is: \\[\\begin{equation} t_s=\\frac{\\Delta\\mu_j}{s_{\\Delta\\mu_j}} \\tag{4.6} \\end{equation}\\] Compare the standard t-test where the test statistic is \\(t_s=\\frac{\\bar x_1-\\bar x_2}{\\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}}\\). Since in this dataset we have equal sample sizes across levels and homogeneous variance (homoscedasticity) is a fundamental assumption of ANOVA (as it is of linear regression), the standard t-test statistic simplifies to Equation (4.6). The corresponding p-value of the 2-sided test then is: \\[\\begin{equation} \\Pr\\left(|Z|\\geq t_s\\right)=2\\cdot\\left(1-t\\left(t_s,n&#39;-2\\right)\\right) \\tag{4.7} \\end{equation}\\] With \\(t\\left(t_s,n&#39;-2\\right)\\) being the value of the cumulative distribution function (cdf) of the t-distribution with parameter \\(n&#39;-2\\) at the location \\(t_s\\), and \\(n&#39;=20\\) in the yields example. Let’s calculate these t-tests “by hand” in R: # differences of means delta_mu_j &lt;- c(mu_j[2]-mu_j[1],mu_j[3]-mu_j[1],mu_j[3]-mu_j[2]) delta_mu_j ## [1] 1.6 4.4 2.8 # standard error of differences # here the same across factor levels as n_j is homogeneous s_delta_mu_j &lt;- sqrt(2*s2/n_j) s_delta_mu_j ## [1] 1.528737 # t-test statistics t_s &lt;- delta_mu_j/s_delta_mu_j t_s ## [1] 1.046616 2.878193 1.831577 # p-values pvalue &lt;- 2*(1-pt(t_s,20-2)) pvalue ## [1] 0.30912862 0.01000534 0.08361664 We conclude that the yields on sand and loam are just about significantly different at a conventional significance level of 0.01, with yields on sand being 4.4 units lower on average. The other yield differences are not statistically significant (compare Figure 4.1). References "],
["multiplelinreg.html", "Chapter 5 Multiple linear regression 5.1 Model selection 5.2 Collinearity 5.3 Overfitting 5.4 Information criteria 5.5 Mixed continuous-categorical predictors 5.6 General advise", " Chapter 5 Multiple linear regression The extension of linear regression to the case of more than one predictor - be they continuous or categorical or a mix of both - is called multiple linear regression. This means we go from the equation \\(y=\\beta_0+\\beta_1\\cdot x+\\epsilon\\) (Equation (3.2)) to the equation \\(y=\\beta_0+\\sum_{j=1}^{p}\\beta_j\\cdot x_j+\\epsilon\\) (Equation (3.1)). As an example we will look at a dataset of air quality, again from Crawley (2012) - see Figure 5.1 - asking the question: How is ground-level ozone concentration related to wind speed, air temperature and solar radiation intensity? A useful first thing to do is to plot what’s often called a scatterplot matrix (Figure 5.1). # load air quality data ozone &lt;- read.table(&quot;data/ozone.txt&quot;,header=T) # scatterplot matrix of ozone dataset # this requires running example(pairs) first so that the histogramms can be drawn on the diagonal # here this is done in the background pairs(ozone, diag.panel = panel.hist, lower.panel = panel.cor) Figure 5.1: Scatterplot matrix of ozone dataset: rad = solar radiation intensity; temp = air temperature; wind = wind speed; ozone = ground-level ozone concentration. The diagonal shows the histograms of the individual variables. The lower triangle shows the linear correlation coefficients, with font size proportional to size of correlation. Data from: Crawley (2012) On the diagonal you see the histograms of each variable. On the upper triangle you see scatterplots between two variables. On the lower triangle you see linear correlation coefficients, with font size proportional to size of correlation. From this we already see that “ozone” is correlated with all three other variables, but perhaps less so with “radiation”. We also see that the other variables are correlated, at least “wind” and “temperature”. The challenge we now face is typical for multiple regression - it is one of model selection: Which predictor variables to include? E.g. possibly \\(x_1=rad\\), \\(x_2=temp\\), \\(x_3=wind\\) Which interactions between variables to include? E.g. possibly \\(x_4=rad\\cdot temp\\), \\(x_5=rad\\cdot wind\\), \\(x_6=temp\\cdot wind\\), \\(x_7=rad\\cdot temp\\cdot wind\\) We haven’t talked about interactions so far - because we had just one predictor variable to think about - but interactions are quite an interesting element of regression models. Essentially, one predictor could modulate the effect that another predictor has on the response - we will discuss an example below in Chapter 5.5. Mathematically, this is achieved by adding the product of the two predictors as a predictor to the linear model, on top of the two individual predictors. In addition to such 2-way interactions, we can have 3-way interactions (three predictors multiplied) and so on and so forth, but these get increasingly complicated to interpret. In principle, we could also include higher-order terms, such as \\(x_8=rad^2\\), \\(x_9=temp^2\\), \\(x_{10}=wind^2\\) etc., in a regression model, as well as other predictor transformations. In practice, though, such choices will only be included if there is an process reason to include them. We then face two main problems in model selection: Collinearity of variables: Predictors may be correlated with each other, which complicates their estimation. Interactions (and higher-order terms) certainly introduce collinearity as we will see below. Overfitting: The more predictors (and hence parameters) we add the better we can fit the data; but with an increasing risk of fitting the noise and not just the signal in the data, which will lead to poor predictions. We discuss these points in turn in the next section. 5.1 Model selection Model selection often appeals to the Parsimony Principle or Occam’s Razor. It goes roughly like this: Given a set of models with “similar explanatory power”, the “simplest” of these shall be preferred. This is called a “philosophical razor”, i.e. a rule of thumb that narrows down the choices of possible explanation or action. It dates back to English Franciscan friar William of Ockham (also Occam; c. 1287-1347). His was a time of controversy within the church; and William of Ockham was one of those who advocated for a “simple” life (and by extension a poor and not a rich church, which got him into trouble).2 I believe this quest for simplicity carried over to his view on epistemology (how we know things) - hence Occam’s Razor. Anyhow, for us in statistical modelling Occam’s Razor roughly translates to the following guidelines (after Crawley (2012)): Prefer a model with \\(m-1\\) parameters to a model with \\(m\\) parameters Prefer a model with \\(k-1\\) explanatory variables to a model with \\(k\\) explanatory variables Prefer a linear model to a non-linear model Prefer a model without interactions to a model containing interactions between explanatory variables A model shall be simplified until it is minimal adequate To understand “minimal adequate”, consider Table 5.1. Table 5.1: Model complexity types in model selection. After: Crawley (2012). Saturated model Maximal model Minimal adequate model Null model One parameter for every data point Contains all (\\(p\\)) explanatory variables and interactions that might be of interest (many likely insignificant) Simplified model with \\(p&#39;\\) parameters (\\(0\\leq p&#39;\\leq p\\)) Just one parameter, the overall mean \\(\\bar y\\) Fit: perfect Fit: less than perfect Fit: less than maximal model, but not significantly so Fit: none, \\(SSE=SSY\\) Degrees of freedom: \\(0\\) Degrees of freedom: \\(n-p-1\\) Degrees of freedom: \\(n-p&#39;-1\\) Degrees of freedom: \\(n-1\\) Explanatory power: none Explanatory power: \\(r^2=1-\\frac{SSE}{SSY}\\) Explanatory power: \\(r^2=1-\\frac{SSE}{SSY}\\) Explanatory power: none At one end of the complexity spectrum of potential models is the so called saturated model. Is has one parameter for every data point and hence fits the data perfectly - this can be shown mathematically and is displayed in Figure 5.2 for a polynomial of \\((n-1)\\)th order. But this model has zero degrees of freedom, hence has no explanatory power; it fits the noise around the signal perfectly, which has no use in prediction - just imagine to use the \\((n-1)\\)th-order polynomial (or even lower-order ones) in Figure 5.2 for extrapolation. Figure 5.2: A dataset of nine data points is fitted by polynomials of varying order; poly1=1st-order to poly8=8th-order. The 8th-order polynomial (equation at top of figure) fits the data perfectly as it is a saturated model; it has as many parameters as data points. The Null model (intercept only) is just the mean of \\(y\\); equation at bottom right. At the other end of the complexity spectrum is what’s called the Null model. Its only parameter is the intercept \\(\\beta_0\\), whose best estimate minimising the sum of squared errors \\(SSE\\) is the mean of \\(y\\), i.e. \\(\\bar y\\) (see also Figure 5.2). The Null model doesn’t fit anything beyond \\(SSY\\), the variation around the mean, hence doesn’t explain any relations in the data. In between those polar opposites are the so called maximal model and the minimal adequate model. These terms are only loosely defined but mark the space of model complexity that we navigate in model selection. The maximal model contains all explanatory variables and interactions that might be of interest, of which many will likely turn out insignificant. It fits the data less than perfect - but that’s not the goal anyway given noise - and its explanatory power can be judged with \\(r^2\\), for example. I suggest that the maximal model be strongly informed by our underlying (theoretical) understanding of the relations to be modelled, and that predictors and interactions that don’t make any sense in relation to that understanding be excluded. This approach, of course, will limit our exposure to surprises, which we could learn a lot from, but aims at keeping the model selection problem manageable. The minimal adequate model then includes the subset of predictors of the maximal model that “really matter”, naturally compromising some goodness of fit of the maximal model, but not significantly so - that’s the trick. What “really matters” in that sense depends. Under the classic statistical paradigm, this has a lot to do with statistical significance (the p-values of the parameter estimates); we rarely include parameters that are insignificant.3 But since anything can be significant with enough data points, the cutoff at a significance level of say \\(\\alpha=0.01\\) seems arbitrary. But p-values can be taken as functions of the standard errors of the parameter estimates - this is simply what they are - and this is what they are useful for. In general we don’t want too many parameters in our models because this inflates their standard errors, making their interpretation essentially meaningless - so looking at standard errors (via p-values if you must) is important. But we want to be able to include the occasional parameter that has a large standard error (and is insignificant in a classic sense), simply because there might a mechanistic reason to do so, especially for prediction. The standard error of that parameter may be large because there is little information in the data at hand about the parameter, but that’s no reason to exclude it if we have reason to believe it is important. In this case the large standard error just helps to be honest about the capabilities of our model given the data at hand. So, in sum, let’s not be overly concerned about p-values during model selection. Instead, as we will see below (Chapter 5.4), it is a good idea to base model selection on information criteria as these approximate the models predictive performance. Model selection involves a lot of trial and error and personal judgement, but there are a few guidelines. In general, we can follow an up-ward or a down-ward selection of models. Up-ward model selection starts with a minimal set of predictors and sequentially adds more when this increases some information criterion or other measure. Down-ward model selection starts with the maximal model and sequentially simplifies this. Along each way there will be steps where we test several models - different routes to take for complication or simplification, respectively. Again, information criteria are crucial here. We can do this manually but automatic model selection algorithms are available in R - you will learn some of them in the PC-lab. But I would not trust them blindly - always confirm the result by testing the final model against some alternatives. 5.2 Collinearity Predictors are collinear (sometimes called multicollinear) when they are perfectly correlated, i.e. when a linear combination of them exists that equals zero for all the data (the estimated parameter values can compensate each other). The parameters then cannot be estimated uniquely (the estimates have standard errors of infinity) - they are said to be nonidentifiable. In practice, predictors are seldom perfectly correlated, so near-collinearity and poor identifiability are the issues to worry about. In the ozone dataset, we see weak collinearity between predictors, so nothing to worry about too much at this stage (Figure 5.1). Modelling interactions between predictors, however, introduces collinearity (Figure 5.3). # generate new dataset w interactions ozone2 &lt;- ozone ozone2$rad_temp &lt;- ozone$rad * ozone$temp ozone2$rad_wind &lt;- ozone$rad * ozone$wind ozone2$temp_wind &lt;- ozone$temp * ozone$wind ozone2$rad_temp_wind &lt;- ozone$rad * ozone$temp * ozone$wind pairs(ozone2, diag.panel = panel.hist, lower.panel = panel.cor) Figure 5.3: Scatterplot matrix of ozone dataset, including interactions. Interaction terms are generally correlated with the predictors interacting. Data from: Crawley (2012) We can live with mild levels of collinearity, for reasons discussed above, especially if including interactions, for example, is important for mechanistic reasons. However, if standard errors become so large as to make the parameter estimates essentially meaningless, we need to leave out some of the correlated predictors - even if we find them important from a mechanistic perspective. Another option is to transform the predictors by Principal Component Analysis (PCA; Chapter 8) into a set of new, uncorrelated predictors that combine the information of the original predictors. 5.3 Overfitting Overfitting occurs when we try to estimate too many parameters compared to the size of the dataset at hand. Then we will unduly fit the noise around the signal that interests us, from which there is nothing to be learned. We will also inflate standard errors as more and more parameters become less and less identifiable. This is illustrated with polynomials in Figure 5.2. To get an idea of how common this problem is we can look at another air quality dataset, also from Crawley (2012) (Figure 5.4). # load 2nd air quality data sulphur &lt;- read.table(&quot;data/sulphur.txt&quot;,header=T) pairs(sulphur, diag.panel = panel.hist, lower.panel = panel.cor) Figure 5.4: Scatterplot matrix of sulphur dataset: Pollution = sulphur dioxide concentration; Temp = air temperature; Industry = prevalence of industry; Population = population size; Wind = wind speed; Rain = rainfall; Wet-days = number of wet days. Data from: Crawley (2012) Here we have 41 data points and six possible main predictors. How many possible predictors can we have by including all possible interactions? (Q2)4 Once you’ve worked this out you see that we have to be really selective if we want to include interactions, because we approach the saturated model (Figure 5.2) very quickly. 5.4 Information criteria Information criteria help us select models because they approximate the models’ predictive performance, i.e. how well they would fit observations not included when fitting the models (“out-of-sample”). Information criteria penalise models for overfitting because overfitting makes predictive performance worse. Under the classical statistical paradigm, the preferred information criterion is arguably the Akaike Information Criterion (AIC):5 \\[\\begin{equation} AIC=-2\\cdot logL\\left(\\hat \\beta, \\sigma|y\\right)+2\\cdot p \\tag{5.1} \\end{equation}\\] \\(logL\\left(\\hat \\beta, \\hat \\sigma|y\\right)\\) is the log-likelihood function at the maximum likelihood estimate of the parameters \\(\\hat \\beta\\), with \\(\\hat \\sigma=\\sqrt{\\frac{SSE}{n-p}}\\), given the data \\(y\\). The maximum likelihood estimate is the Least Squares estimate for linear regression. We will learn more about the likelihood function and its relation to Least Squares in Chapter 6. \\(p\\) is the number of parameters in our model. Under the classic statistical paradigm, AIC is a reliable approximation of out-of-sample performance only when the likelihood function is approximately normal (which is a standard assumption anyway) and when the sample size is much greater than the number of parameters (McElreath 2020). So our estimate for predictive performance given by AIC is the log-likelihood of the “best” parameter estimates, penalised by the number of model parameters. We don’t have to worry about the factor “-2” in front of the log-likelihood,6 but need to get used to the fact that smaller (possibly negative) values indicate better models. Also note that AIC is not an absolute measure of model out-of-sample performance, as we don’t have an absolute benchmark like a true process; AIC only makes sense relatively when comparing two models. AIC is implemented in automated model selection algorithms, such as the step() function in R. Other information criteria exist but they are based on assumptions that, under the classic paradigm, are similar or even less realistic than those of AIC. Let’s analyse the ozone dataset now. We start up-ward with just the individual predictors. Prior to that we standardise the predictors (see Chapter 2). This brings all predictors onto the same scale and hence makes parameter estimates comparable. It also makes parameters easier to interpret. The intercept now is the ozone concentration when all predictors are at their mean values. And each parameter measures the change in ozone concentration when that predictor changes by one standard deviation. Standardising the predictors is common practice (Gelman, Hill, and Vehtari 2020). # standardise predictors ozone_std &lt;- ozone ozone_std$rad &lt;- (ozone$rad-mean(ozone$rad))/sd(ozone$rad) ozone_std$temp &lt;- (ozone$temp-mean(ozone$temp))/sd(ozone$temp) ozone_std$wind &lt;- (ozone$wind-mean(ozone$wind))/sd(ozone$wind) # multiple linear regression model with 3 predictors ozone_fit &lt;- lm(ozone ~ rad+temp+wind, data = ozone_std) # extract information about parameter estimates summary(ozone_fit) ## ## Call: ## lm(formula = ozone ~ rad + temp + wind, data = ozone_std) ## ## Residuals: ## Min 1Q Median 3Q Max ## -40.485 -14.210 -3.556 10.124 95.600 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42.099 2.010 20.949 &lt; 2e-16 *** ## rad 5.451 2.113 2.580 0.0112 * ## temp 15.736 2.415 6.516 2.43e-09 *** ## wind -11.879 2.327 -5.105 1.45e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 21.17 on 107 degrees of freedom ## Multiple R-squared: 0.6062, Adjusted R-squared: 0.5952 ## F-statistic: 54.91 on 3 and 107 DF, p-value: &lt; 2.2e-16 Before we interpret this, let’s look at the residuals: # residuals by index plot(residuals(ozone_fit), pch = 19, type = &#39;p&#39;) # residuals by modelled value plot(fitted.values(ozone_fit), residuals(ozone_fit), pch = 19, type = &#39;p&#39;) # residual histogram hist(residuals(ozone_fit)) # residual QQ-plot qqnorm(residuals(ozone_fit)) qqline(residuals(ozone_fit)) The residuals are heteroscedastic and right-skewed, which is something we might be able to fix by log-transforming the response. This is something to try in any case when the response varies over orders of magnitude (Gelman, Hill, and Vehtari 2020). # add log-transform of ozone to data.frame ozone_std$log_ozone &lt;- log(ozone_std$ozone) # fit log_ozone_fit &lt;- lm(log_ozone ~ rad+temp+wind, data = ozone_std) summary(log_ozone_fit) ## ## Call: ## lm(formula = log_ozone ~ rad + temp + wind, data = ozone_std) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.06212 -0.29968 -0.00223 0.30767 1.23572 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.41593 0.04826 70.775 &lt; 2e-16 *** ## rad 0.22922 0.05074 4.518 1.62e-05 *** ## temp 0.46852 0.05800 8.078 1.07e-12 *** ## wind -0.21922 0.05589 -3.922 0.000155 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5085 on 107 degrees of freedom ## Multiple R-squared: 0.6645, Adjusted R-squared: 0.6551 ## F-statistic: 70.65 on 3 and 107 DF, p-value: &lt; 2.2e-16 Log-transforming ozone stabilised the predictors and also increased \\(r^2\\) by 0.05. The relationships of ozone to the three predictors has apparently turned more linear on the log-scale. The residuals, too, conform better to the assumptions, except for one outlier at the very low end: plot(residuals(log_ozone_fit), pch = 19, type = &#39;p&#39;) plot(fitted.values(log_ozone_fit), residuals(log_ozone_fit), pch = 19, type = &#39;p&#39;) hist(residuals(log_ozone_fit)) qqnorm(residuals(log_ozone_fit)) qqline(residuals(log_ozone_fit)) Finally, the AIC of the log-ozone model got a lot better compared to the ozone model (remember that a smaller AIC indicates a better model): AIC(ozone_fit) ## [1] 998.6276 AIC(log_ozone_fit) ## [1] 170.7949 So we can proceed from this base model and see if adding interactions improves fit \\(\\left(r^2\\right)\\) and predictive performance (AIC). We start by adding the interaction of the largest effects (Gelman, Hill, and Vehtari 2020): log_ozone_fit2 &lt;- lm(log_ozone ~ rad*temp+wind, data = ozone_std) summary(log_ozone_fit2) ## ## Call: ## lm(formula = log_ozone ~ rad * temp + wind, data = ozone_std) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.18244 -0.30194 0.00665 0.31242 1.18972 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.40218 0.05061 67.229 &lt; 2e-16 *** ## rad 0.25245 0.05682 4.443 2.19e-05 *** ## temp 0.46918 0.05805 8.082 1.10e-12 *** ## wind -0.21934 0.05594 -3.921 0.000157 *** ## rad:temp 0.04717 0.05179 0.911 0.364443 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5089 on 106 degrees of freedom ## Multiple R-squared: 0.6671, Adjusted R-squared: 0.6546 ## F-statistic: 53.11 on 4 and 106 DF, p-value: &lt; 2.2e-16 AIC(log_ozone_fit2) ## [1] 171.9295 This interaction actually makes the predictive performance a little worse (greater AIC). Let’s try the next: log_ozone_fit3 &lt;- lm(log_ozone ~ rad+temp*wind, data = ozone_std) summary(log_ozone_fit3) ## ## Call: ## lm(formula = log_ozone ~ rad + temp * wind, data = ozone_std) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.98696 -0.32076 -0.05428 0.30238 1.19016 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.36697 0.05245 64.190 &lt; 2e-16 *** ## rad 0.23644 0.04998 4.731 6.93e-06 *** ## temp 0.46806 0.05700 8.211 5.70e-13 *** ## wind -0.21984 0.05493 -4.002 0.000117 *** ## temp:wind -0.09938 0.04545 -2.187 0.030967 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4997 on 106 degrees of freedom ## Multiple R-squared: 0.679, Adjusted R-squared: 0.6669 ## F-statistic: 56.05 on 4 and 106 DF, p-value: &lt; 2.2e-16 AIC(log_ozone_fit3) ## [1] 167.8976 Predictive performance is a little better than the base model (smaller AIC) and \\(r^2\\) increased by 0.02, even if the interaction comes out fairly uncertain. On to the last interaction: log_ozone_fit4 &lt;- lm(log_ozone ~ rad*wind+temp, data = ozone_std) summary(log_ozone_fit4) ## ## Call: ## lm(formula = log_ozone ~ rad * wind + temp, data = ozone_std) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.99855 -0.33075 -0.01627 0.26636 1.22283 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.40650 0.04837 70.423 &lt; 2e-16 *** ## rad 0.25227 0.05267 4.790 5.45e-06 *** ## wind -0.21686 0.05558 -3.902 0.000168 *** ## temp 0.46830 0.05765 8.123 8.93e-13 *** ## rad:wind -0.07466 0.04915 -1.519 0.131751 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5054 on 106 degrees of freedom ## Multiple R-squared: 0.6717, Adjusted R-squared: 0.6593 ## F-statistic: 54.21 on 4 and 106 DF, p-value: &lt; 2.2e-16 AIC(log_ozone_fit4) ## [1] 170.4048 This interaction is not estimated precisely and we only gain an improvement in \\(r^2\\) of 0.01 and a negligible gain in AIC. So I’m inclined to go with model 3 (all three predictors and “temp:wind” interaction). Due to the imprecision of the interaction parameter it doesn’t make sense to add more predictors at this stage. The residuals of model 3 still look ok: plot(residuals(log_ozone_fit3), pch = 19, type = &#39;p&#39;) plot(fitted.values(log_ozone_fit3), residuals(log_ozone_fit3), pch = 19, type = &#39;p&#39;) hist(residuals(log_ozone_fit3)) qqnorm(residuals(log_ozone_fit3)) qqline(residuals(log_ozone_fit3)) The model that we settled with is: \\[\\begin{equation} \\log(ozone)=3.4+0.2\\cdot rad_{std}+0.5\\cdot temp_{std}-0.2\\cdot wind_{std}-0.1\\cdot temp_{std}\\cdot wind_{std}+\\epsilon \\tag{5.2} \\end{equation}\\] It explains 68% of the variation in ozone concentration (at the log-scale, judged by \\(r^2\\)) - which is pretty good - and tells us the following: The most important driver (of those we had data for) of ozone concentration is air temperature, followed by radiation intensity and wind speed. We get this from comparing the size of the parameters of the different predictors, which we can only do if predictors are standardised. Air temperature and radiation intensity increase ozone concentrations, while wind speed decreases it. The negative interaction of air temperature and wind speed tells us that the air temperature effects is down-regulated with increasing wind speeds.7 We can thus group Equation (5.2) as follows: \\[\\begin{equation} \\log(ozone)=3.4+0.2\\cdot rad_{std}+\\left(0.5-0.1\\cdot wind_{std}\\right)\\cdot temp_{std}-0.2\\cdot wind_{std}+\\epsilon \\tag{5.3} \\end{equation}\\] This is a useful way of understanding interactions, which we will revisit now in the last section. 5.5 Mixed continuous-categorical predictors The special case of a mix of continuous and categorical predictors has got the special name of analysis of covariance (ANCOVA), though as we said before (chapter 4) it’s really just another variant of a linear model. For illustration we use an example from Gelman, Hill, and Vehtari (2020), modelling childrens’ IQ score by their mothers’ IQ score and whether or not the mothers have a high school degree (Figure 5.5). # load IQ data from remote repository iq &lt;- read.csv(&quot;https://raw.githubusercontent.com/avehtari/ROS-Examples/master/KidIQ/data/kidiq.csv&quot;, header=TRUE) # plot kid&#39;s IQ against mom&#39;s IQ # with symbols differentiated by whether or not the mom has a high school degree plot(iq$mom_iq, iq$kid_score, pch = c(1, 20)[iq$mom_hs+1], col = c(&quot;black&quot;, &quot;gray&quot;)[iq$mom_hs+1], xlab = &quot;Mom&#39;s IQ&quot;, ylab = &quot;Kid&#39;s IQ&quot;) Figure 5.5: Childrens’ IQ score against their mothers’ IQ score, with symbol and shading indicating whether or not the mothers have a high school degree (open black dots: no high school degree; closed grey dots: highschool degree. Data from: Gelman, Hill, and Vehtari (2020) We first centre the predictor “mom_iq” (the IQ score of the mothers) to make the corresponding parameter easier to interpret. Note, standardisation is not necessary here because there is no other continuous predictor to compare against, just a binary predictor.8 The binary predictor is “mom_hs”, with “1” indicating mother has a high school degree and “0” indicating mother hasn’t got one. # centre continuous predictor iq_cen &lt;- iq iq_cen$mom_iq &lt;- iq$mom_iq-mean(iq$mom_iq) We then fit all model variants with and without interaction all at once and compare them via AIC. Normally we would do this step by step as in the previous example and check residuals at every stage. I skip this here and only check residuals at the end because I’m more interested in showcasing the different model variants and what they mean mathematically and mechanistically. # fit Null model iq_fit0 &lt;- lm(kid_score ~ 1, data = iq_cen) coef(summary(iq_fit0)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 86.79724 0.9797444 88.59171 1.330993e-279 AIC(iq_fit0) ## [1] 3852.576 # fit common slope and intercept model iq_fit1 &lt;- lm(kid_score ~ mom_iq, data = iq_cen) coef(summary(iq_fit1)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 86.7972350 0.87680195 98.99298 5.127032e-299 ## mom_iq 0.6099746 0.05852092 10.42319 7.661950e-23 AIC(iq_fit1) ## [1] 3757.216 # fit individual Null models (2 intercepts) iq_fit2 &lt;- lm(kid_score ~ mom_hs, data = iq_cen) coef(summary(iq_fit2)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 77.54839 2.058612 37.670231 1.392224e-138 ## mom_hs 11.77126 2.322427 5.068516 5.956524e-07 AIC(iq_fit2) ## [1] 3829.506 # fit common slope model (2 intercepts) iq_fit3 &lt;- lm(kid_score ~ mom_iq+mom_hs, data = iq_cen) coef(summary(iq_fit3)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 82.122143 1.94370047 42.250411 2.435765e-155 ## mom_iq 0.563906 0.06057408 9.309362 6.609618e-19 ## mom_hs 5.950117 2.21181218 2.690155 7.419327e-03 AIC(iq_fit3) ## [1] 3751.989 # fit common intercept model (2 slopes) iq_fit4 &lt;- lm(kid_score ~ mom_iq+mom_iq:mom_hs, data = iq_cen) coef(summary(iq_fit4)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 87.7805823 0.8998447 97.550808 6.602618e-296 ## mom_iq 1.0549920 0.1288807 8.185802 3.085494e-15 ## mom_iq:mom_hs -0.5657798 0.1465785 -3.859909 1.307991e-04 AIC(iq_fit4) ## [1] 3744.467 # fit maximal model iq_fit5 &lt;- lm(kid_score ~ mom_iq*mom_hs, data = iq_cen) coef(summary(iq_fit5)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 85.4068999 2.2182219 38.50242 1.970160e-141 ## mom_iq 0.9688892 0.1483437 6.53138 1.843084e-10 ## mom_hs 2.8407569 2.4266700 1.17064 2.423919e-01 ## mom_iq:mom_hs -0.4842747 0.1622171 -2.98535 2.994237e-03 AIC(iq_fit5) ## [1] 3745.086 The most complex model (#5) comes out on top here according to AIC. Its residuals look ok too: plot(residuals(iq_fit5), pch = 19, type = &#39;p&#39;) plot(fitted.values(iq_fit5), residuals(iq_fit5), pch = 19, type = &#39;p&#39;) hist(residuals(iq_fit5)) qqnorm(residuals(iq_fit5)) qqline(residuals(iq_fit5)) Let’s plot the six variants to look at the meaning of interactions once more (Figure 5.6). # Null model plot(iq_cen$mom_iq, iq_cen$kid_score, pch = c(1, 20)[iq_cen$mom_hs+1], col = c(&quot;black&quot;, &quot;gray&quot;)[iq_cen$mom_hs+1], main =&quot;Null model&quot;, xlab = &quot;Mom&#39;s IQ (centred)&quot;, ylab = &quot;Kid&#39;s IQ&quot;) abline(h = coef(iq_fit0), lwd = 3, col = &quot;black&quot;) # common slope and intercept plot(iq_cen$mom_iq, iq_cen$kid_score, pch = c(1, 20)[iq_cen$mom_hs+1], col = c(&quot;black&quot;, &quot;gray&quot;)[iq_cen$mom_hs+1], main = &quot;common slope and intercept&quot;, xlab = &quot;Mom&#39;s IQ (centred)&quot;, ylab = &quot;Kid&#39;s IQ&quot;) abline(coef(iq_fit1), lwd = 3, col = &quot;black&quot;) # individual Null models plot(iq_cen$mom_iq, iq_cen$kid_score, pch = c(1, 20)[iq_cen$mom_hs+1], col = c(&quot;black&quot;, &quot;gray&quot;)[iq_cen$mom_hs+1], main = &quot;individual Null models&quot;, xlab = &quot;Mom&#39;s IQ (centred)&quot;, ylab = &quot;Kid&#39;s IQ&quot;) abline(h = coef(iq_fit2)[1], lwd = 3, col = &quot;black&quot;) abline(h = sum(coef(iq_fit2)), lwd = 3, col = &quot;gray&quot;) # common slope plot(iq_cen$mom_iq, iq_cen$kid_score, pch = c(1, 20)[iq_cen$mom_hs+1], col = c(&quot;black&quot;, &quot;gray&quot;)[iq_cen$mom_hs+1], main = &quot;common slope&quot;, xlab = &quot;Mom&#39;s IQ (centred)&quot;, ylab = &quot;Kid&#39;s IQ&quot;) abline(coef(iq_fit3)[c(1,2)], lwd = 3, col = &quot;black&quot;) abline(c(sum(coef(iq_fit3)[c(1,3)]),coef(iq_fit3)[2]), lwd = 3, col = &quot;gray&quot;) # common intercept plot(iq_cen$mom_iq, iq_cen$kid_score, pch = c(1, 20)[iq_cen$mom_hs+1], col = c(&quot;black&quot;, &quot;gray&quot;)[iq_cen$mom_hs+1], main = &quot;common intercept&quot;, xlab = &quot;Mom&#39;s IQ (centred)&quot;, ylab = &quot;Kid&#39;s IQ&quot;) abline(coef(iq_fit4)[c(1,2)], lwd = 3, col = &quot;black&quot;) abline(c(coef(iq_fit4)[1],sum(coef(iq_fit4)[c(2,3)])), lwd = 3, col = &quot;gray&quot;) # maximal model plot(iq_cen$mom_iq, iq_cen$kid_score, pch = c(1, 20)[iq_cen$mom_hs+1], col = c(&quot;black&quot;, &quot;gray&quot;)[iq_cen$mom_hs+1], main = &quot;maximal model&quot;, xlab = &quot;Mom&#39;s IQ (centred)&quot;, ylab = &quot;Kid&#39;s IQ&quot;) abline(coef(iq_fit5)[c(1,2)], lwd = 3, col = &quot;black&quot;) abline(c(sum(coef(iq_fit5)[c(1,3)]),sum(coef(iq_fit5)[c(2,4)])), lwd = 3, col = &quot;gray&quot;) Figure 5.6: Six model variants for the IQ dataset. Open black dots and black lines: mother has no high school degree. Closed grey dots and grey lines: mother has a highschool degree. Data from: Gelman, Hill, and Vehtari (2020). Mathematically, the Null model is: \\[\\begin{equation} IQ_{kid}=87+\\epsilon \\tag{5.4} \\end{equation}\\] I.e. the Null model is nothing more than the children’s mean IQ score, which is 87. The common slope and intercept model is: \\[\\begin{equation} IQ_{kid}=87+0.6\\cdot \\left(IQ_{mom}-\\hat{IQ}_{mom}\\right)+\\epsilon \\tag{5.5} \\end{equation}\\] So when mother’s IQ score is at its average then the child’s IQ score is again the overall average, 87, but increases or decreases by 0.6 for every unit increase or decrease in mother’s IQ score. The individual Null models formulation is: \\[\\begin{equation} IQ_{kid}=78+12\\cdot hs_{mom}+\\epsilon \\tag{5.6} \\end{equation}\\] These are two different intercepts now, the mean children’s IQ score for mothers without a high school degree (when \\(hs_{mom}=0\\)), which is 78, and the mean children’s IQ score for mothers with a high school degree (when \\(hs_{mom}=1\\)), which is 78+12=90. The common slope model is: \\[\\begin{equation} IQ_{kid}=82+6\\cdot hs_{mom}+0.6\\cdot \\left(IQ_{mom}-\\hat{IQ}_{mom}\\right)+\\epsilon \\tag{5.7} \\end{equation}\\] This model assumes different mean children’s IQ scores for mothers with and without high school degree, but the same dependence on the mothers’ IQ score, which again comes out at 0.6. The common intercept model now includes the interaction term, but not \\(hs_{mom}\\) as an individual predictor: \\[\\begin{equation} IQ_{kid}=88+1.1\\cdot \\left(IQ_{mom}-\\hat{IQ}_{mom}\\right)-0.6\\cdot hs_{mom}\\cdot \\left(IQ_{mom}-\\hat{IQ}_{mom}\\right)+\\epsilon \\tag{5.8} \\end{equation}\\] Regrouping leads to: \\[\\begin{equation} IQ_{kid}=88+\\left(1.1-0.6\\cdot hs_{mom}\\right)\\cdot \\left(IQ_{mom}-\\hat{IQ}_{mom}\\right)+\\epsilon \\tag{5.9} \\end{equation}\\] According to this model we embark from a common intercept but then follow different slopes depending on mother’s high school degree. When mother has a high school degree, the child’s IQ increases by 1.1-0.6=0.5 for very unit increase in mother’s IQ. When mother has no high school degree, then mother’s IQ effect is stronger, increasing the child’s IQ by 1.1 for very unit increase in mother’s IQ. Finally, the maximal model includes all predictors: \\[\\begin{equation} IQ_{kid}=85+3\\cdot hs_{mom}+1.0\\cdot \\left(IQ_{mom}-\\hat{IQ}_{mom}\\right)-0.5\\cdot hs_{mom}\\cdot \\left(IQ_{mom}-\\hat{IQ}_{mom}\\right)+\\epsilon \\tag{5.10} \\end{equation}\\] Regrouping leads to: \\[\\begin{equation} IQ_{kid}=85+3\\cdot hs_{mom}+\\left(1.0-0.5\\cdot hs_{mom}\\right)\\cdot \\left(IQ_{mom}-mean\\left(IQ_{mom}\\right)\\right)+\\epsilon \\tag{5.11} \\end{equation}\\] So positing two different average children’s IQ scores for mothers with and without high school degree, 88 and 85 respectively, and two different dependencies on mothers’ IQ score, 0.5 and 1.0 respectively. If we believe this model that turned out best according AIC, then there is a difference in average children’s IQ score depending on whether or not the children’s mothers have a high school degree (with high school degrees improving IQ scores by 3 units on average). What also matters is the mothers’ IQ score, with a positive relationship between mothers’ and children’s score. This relationship is stronger (mother’s IQ matters more) when mothers have no high school degree than when they have one. 5.6 General advise Let’s finish with some general advise for building regression models, taken from Gelman, Hill, and Vehtari (2020): Include all predictors that we consider important for mechanistic reasons. Consider combining several predictors into a “total score” by averaging or summation (this is something we haven’t looked at so far). If predictors have large effects, consider including their interactions. Use standard errors to get a sense of uncertainties in parameter estimates. Decide upon including or excluding predictors based on a combination of contextual understanding, data, and the uses to which the regression will be put: If the parameter of a predictor is estimated precisely (small standard error), then it generally makes sense to keep it in the model as it should improve predictions. If the standard error of a parameter is large and there is no good mechanistic reason for the predictor to be included, then it can make sense to remove it, as this can allow the other model parameters to be estimated more stably and can even reduce prediction errors. If a predictor is important for the problem at hand, then Gelman, Hill, and Vehtari (2020) generally recommend keeping it in, even if the estimate has a large standard error and is not “statistically significant”. In such settings we must acknowledge the resulting uncertainty and perhaps try to reduce it, e.g. by gathering more data. If a coefficient doesn’t make sense, then we should try to understand how this could happen. If the standard error is large, the estimate could be explainable from random variation. If the standard error is small, it can make sense to put more effort into understanding the coefficient. References "],
["mlbayes.html", "Chapter 6 Probabilistic underpinnings 6.1 Inference via Maximum Likelihood 6.2 Outlook: Bayesian inference", " Chapter 6 Probabilistic underpinnings In this chapter we will see how the assumptions of linear regression - which are needed for the quantification of uncertainty in our results - come about. We will first derive the Least Squares parameter estimators from Maximum Likelihood theory (the historically dominant approach) before giving an introduction to the more general approach of Bayesian statistics. The latter is the focus of my course Applied Statistical Modelling in the summer term and also features at a basic level in Risk and Uncertainty in Science and Policy (offered summers and winters). 6.1 Inference via Maximum Likelihood The so called likelihood of parameters conditional on some calibration data is defined as the probability of the data conditional on the parameters (and implicitly the model): \\[\\begin{equation} L(\\boldsymbol{\\theta}|\\mathbf{y})=\\Pr(\\mathbf{y}|\\boldsymbol{\\theta}) \\tag{6.1} \\end{equation}\\] \\(\\boldsymbol{\\theta}\\) is a vector of parameters, in the case of linear regression \\(\\boldsymbol{\\theta}=\\begin{pmatrix}\\beta_0 &amp; \\beta_1 &amp; \\sigma\\end{pmatrix}\\), and \\(\\mathbf{y}\\) is a vector of response data points \\(y_i\\). If all \\(y_i\\) are independent - here comes the first assumption of linear regression - then the joint probability in Equation (6.1) equals the product of the individual probabilities: \\[\\begin{equation} L(\\boldsymbol{\\theta}|\\mathbf{y})=\\prod_{i=1}^{n}\\Pr(y_i|\\boldsymbol{\\theta}) \\tag{6.2} \\end{equation}\\] This follows from the product rule of probability calculus. If we further assume the residuals of the linear model to be normally distributed then the likelihood is: \\[\\begin{equation} L(\\beta_0,\\beta_1,\\sigma|\\mathbf{y})=\\prod_{i=1}^{n}\\frac{1}{\\sigma\\cdot\\sqrt{2\\cdot\\pi}}\\cdot\\exp\\left(\\frac{\\left(y_i-\\beta_0-\\beta_1\\cdot x_i\\right)^2}{-2\\cdot\\sigma^2}\\right) \\tag{6.3} \\end{equation}\\] This means, the probability of individual data points to arise given certain parameter values, \\(\\Pr(y_i|\\boldsymbol{\\theta})\\), is \\(\\frac{1}{\\sigma\\cdot\\sqrt{2\\cdot\\pi}}\\cdot\\exp\\left(\\frac{\\left(y_i-\\beta_0-\\beta_1\\cdot x_i\\right)^2}{-2\\cdot\\sigma^2}\\right)\\). This is the formula of the probability density function (pdf) of the normal distribution, \\(\\frac{1}{\\sigma\\cdot\\sqrt{2\\cdot\\pi}}\\cdot\\exp\\left(\\frac{\\left(y_i-\\mu\\right)^2}{-2\\cdot\\sigma^2}\\right)\\), with \\(\\mu\\) being substituted with the linear predictor \\(\\beta_0+\\beta_1\\cdot x_i\\). In effect, we’re saying that the response data are normally distributed, with the mean represented by the linear model, i.e. not constant but changing as a function of the predictor \\(x_i\\): \\[\\begin{equation} y_i\\sim N\\left(\\beta_0+\\beta_1\\cdot x_i,\\sigma\\right) \\tag{6.4} \\end{equation}\\] Put differently, Equation (6.4) arises from combining the linear model \\(y_i=\\beta_0+\\beta_1\\cdot x_i+\\epsilon_i\\) with the normality assumption for the residuals \\(\\epsilon_i\\sim N(0,\\sigma)\\). Note, the mean of the residual distribution is zero because - based on our fundamental assumption that the model is correct - on average we expect no deviation from the regression line. Please spend some time understanding how the likelihood function is constructed - this is useful for understanding many advanced techniques later on. On our way to construct the maximum likelihood estimates, getting rid of the product operator in Equation (6.3) yields: \\[\\begin{equation} L(\\beta_0,\\beta_1,\\sigma|\\mathbf{y})=\\frac{1}{\\left(\\sigma\\cdot\\sqrt{2\\cdot\\pi}\\right)^n}\\cdot\\exp\\left(\\frac{-1}{2\\cdot\\sigma^2}\\cdot\\sum_{i=1}^{n}\\left(y_i-\\beta_0-\\beta_1\\cdot x_i\\right)^2\\right) \\tag{6.5} \\end{equation}\\] Compare exercises in chapter 2. The log-likelihood is often mathematically easier to handle, while locations of maxima (this is all about maximum likelihood) remain unchanged: \\[\\begin{equation} \\log L(\\beta_0,\\beta_1,\\sigma|\\mathbf{y})=\\log\\left(\\sigma^{-n}\\cdot (2\\cdot\\pi)^{-\\frac{n}{2}}\\right)-\\frac{1}{2\\cdot\\sigma^2}\\cdot\\sum_{i=1}^{n}\\left(y_i-\\beta_0-\\beta_1\\cdot x_i\\right)^2 \\tag{6.6} \\end{equation}\\] \\[\\begin{equation} \\log L(\\beta_0,\\beta_1,\\sigma|\\mathbf{y})=-n\\cdot \\log (\\sigma)-\\frac{n}{2}\\cdot\\log(2\\cdot\\pi)-\\frac{1}{2\\cdot\\sigma^2}\\cdot\\sum_{i=1}^{n}\\left(y_i-\\beta_0-\\beta_1\\cdot x_i\\right)^2 \\tag{6.7} \\end{equation}\\] Compare logarithm calculus of chapter 2. The maximum likelihood is where all partial derivatives with respect to the parameters are zero: \\(\\frac{\\partial\\log L}{\\partial \\beta_0}=0\\) and \\(\\frac{\\partial\\log L}{\\partial \\beta_1}=0\\) and \\(\\frac{\\partial\\log L}{\\partial \\sigma}=0\\). This yields: \\[\\begin{equation} \\frac{\\partial\\log L\\left(\\beta_0,\\beta_1,\\sigma\\right)}{\\partial \\beta_0}=\\frac{1}{\\sigma^2}\\cdot \\sum_{i=1}^{n}\\left(y_i-\\beta_0-\\beta_1 \\cdot x_i\\right)=0 \\tag{6.8} \\end{equation}\\] \\[\\begin{equation} \\frac{\\partial\\log L\\left(\\beta_0,\\beta_1,\\sigma\\right)}{\\partial \\beta_1}=\\frac{1}{\\sigma^2}\\cdot \\sum_{i=1}^{n}x_i\\cdot\\left(y_i-\\beta_0-\\beta_1 \\cdot x_i\\right)=0 \\tag{6.9} \\end{equation}\\] Hence, the maximum likelihood estimator for \\(\\beta_0\\) and \\(\\beta_1\\) under normal residuals is identical to the Least Squares parameter estimator (Equations (3.11) and (3.12) in chapter 3). For \\(\\sigma\\), we have: \\[\\begin{equation} \\frac{\\partial\\log L\\left(\\beta_0,\\beta_1,\\sigma\\right)}{\\partial \\sigma}=-\\frac{n}{\\sigma}+\\frac{1}{\\sigma^3}\\cdot\\sum_{i=1}^{n}\\left(y_i-\\beta_0-\\beta_1 \\cdot x_i\\right)^2=0 \\tag{6.10} \\end{equation}\\] \\[\\begin{equation} \\frac{\\partial\\log L\\left(\\beta_0,\\beta_1,\\sigma\\right)}{\\partial \\sigma}=-n\\cdot\\sigma^2+\\sum_{i=1}^{n}\\left(y_i-\\beta_0-\\beta_1 \\cdot x_i\\right)^2=0 \\tag{6.11} \\end{equation}\\] This yields the estimator: \\[\\begin{equation} \\sigma=\\sqrt{\\frac{SSE}{n}} \\tag{6.12} \\end{equation}\\] Note, the Least Squares estimator is \\(s=\\sqrt{\\frac{SSE}{df_{SSE}}}=\\sqrt{\\frac{SSE}{n-2}}\\), which doesn’t make much of a difference for large \\(n\\). Now it should be clear that the assumptions underpinning linear regression come from maximum likelihood theory; even if parameter estimators can be motivated via Least Squares, their standard errors, confidence intervals and significance tests rely on the assumptions that the residuals be independent and identically distributed according to a normal distribution (“iid normal”). In chapter 7 we will see how we can expand these assumptions, by making other distributional choices in Equation (6.4) as well as transformations of the linear model inside those distributions. We will effectively construct different likelihood functions - different formulations of \\(\\Pr(y_i|\\boldsymbol{\\theta})\\) in Equation (6.2) - motivated by our conceptualisation of the process that generates the response data at hand. 6.2 Outlook: Bayesian inference Bayesian statistics is based on a different philosophical understanding of probability than classic (so called frequentist) statistics, even if both share the same probability calculus. In frequentist statistics, probability is a long-run relative frequency. For example, if we toss a fair coin a thousand times then we will see approximately 500 heads and 500 tails; we say the probability of heads is \\(\\frac{500}{1000}=0.5\\). Of course 1000 tosses is not really enough to approach 0.5, so probability in this sense is defined mathematically as the limit when \\(n\\), the number of tosses in the example, goes to infinity. In Bayesian statistics9, probability is a degree of plausibility of a proposition, like that the coin will come up heads in our example. This degree of plausibility is informed by some observed (long-run) behaviour, like repeated tossing of the coin, but also other sources, like physical reasoning about the coin. In simple games of chance like coin tossing - processes that can be repeated a large number of times - it’s hard to see the philosophical difference between the two types of probability and the two types of statistics. Where the difference is clearer - and important - is in the uncertain information we construct around statistical estimates, i.e. confidence intervals in frequentist statistics. Most important, however, are the many cases where there is no long-run relative frequency at all (or we cannot observe it). Consider, for example, the probability of exceeding a 1.5 degree Celsius rise in global average temperature by the end of this century; this is not a frequentist probability but a Bayesian one - a degree of plausibility given some data, models and other information that go into these kinds of assessments. 6.2.1 Frequentist sampling distributions Now on to uncertainty estimates, where I said the differences between frequentist and Bayesian probability matters. Frequentist estimates like means, test statistics (t-test, F-test, …) and regression parameters all come with so called sampling distributions; probability density functions (PDFs) that describe the variation in those estimates if the estimation procedure were repeated an infinite number of times. These are PDFs in a long-run relative frequency sense. For the mean, if the population from which data \\(x\\) are a sample (of size \\(n\\)) is normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), we can show mathematically that the deviation of the estimate \\(\\hat \\mu=\\bar x\\) from the unknown \\(\\mu\\), scaled by the standard error \\(s_{\\hat \\mu}=\\frac{s}{\\sqrt{n}}\\) (with \\(s=\\sqrt{\\frac{1}{n-1}\\cdot\\sum_{i=1}^{n}\\left(x_i-\\bar x\\right)^2}\\) being the standard deviation of \\(x\\)) would follow a t-distribution in repeated sampling with parameter \\(n-1\\): \\[\\begin{equation} \\frac{\\hat \\mu-\\mu}{s_{\\hat \\mu}}\\sim t_{n-1} \\tag{6.13} \\end{equation}\\] For the t-test statistic, the sampling distribution is similar. If two samples came from normal populations with identical means, the Null hypothesis of the t-test, then the scaled difference between the two mean estimates would follow a t-distribution with parameter \\(n_1+n_2-2\\) in repeated sampling: \\[\\begin{equation} t_s=\\frac{\\hat \\mu_1-\\hat \\mu_2}{\\sqrt{s_{\\hat\\mu_1}^2+s_{\\hat\\mu_2}^2}}\\sim t_{n_1+n_2-2} \\tag{4.6} \\end{equation}\\] The F-test statistic, the ratio of two variance estimates \\(\\hat\\sigma_1\\) and \\(\\hat\\sigma_2\\), would follow a F-distribution10 in repeated sampling if the two samples came from normal populations with identical variances (the Null hypothesis of the F-test): \\[\\begin{equation} F_s=\\frac{\\hat \\sigma_1^2}{\\hat \\sigma_2^2}\\sim F_{n_1-1;n_2-1} \\tag{6.14} \\end{equation}\\] Finally, the regression parameter estimates too would vary around the true parameter value according to a t-distribution in repeated sampling if the residuals were normally distributed (see chapter 6.1): \\[\\begin{equation} \\frac{\\hat \\beta-\\beta}{s_{\\hat \\beta}}\\sim t_{n-2} \\tag{6.15} \\end{equation}\\] Based on these sampling distributions we can now construct confidence intervals (and p-values for tests), which we will only do here for the case of regression parameters, repeating what we did in Chapter 3: \\[\\begin{equation} \\Pr\\left(\\hat\\beta-t_{n-2;0.975} \\cdot s_{\\hat\\beta}\\leq \\beta\\leq \\hat\\beta+t_{n-2;0.975} \\cdot s_{\\hat\\beta}\\right)=0.95 \\tag{6.16} \\end{equation}\\] As mentioned in Chapter 3, this is the central interval in which the true parameter value \\(\\beta\\) lies with a probability of 0.95. But this is a frequentist probability, meaning that in an assumed infinite number of regression experiments the 95% confidence interval captures the true parameter value in 95% of the cases. It is some measure of confidence, but not a probability of the true parameter value lying within the confidence interval for any one experiment. This, by contrast, is what the Bayesian approach provides, as we will see next. 6.2.2 Bayesian posterior distributions The Bayesian approach gives us an actual probability density function (PDF) of the parameters \\(\\boldsymbol{\\theta}\\), i.e. degrees of plausibility for different values of these parameters. This is the so called posterior distribution \\(\\Pr(\\boldsymbol{\\theta}|\\mathbf{y})\\), i.e. the probability distribution of the parameters conditional on the data \\(\\mathbf{y}\\) at hand. Posterior here means “after seeing the data”. We get the posterior distribution from Bayes rule:11 \\[\\begin{equation} \\Pr(\\boldsymbol{\\theta}|\\mathbf{y})=\\frac{\\Pr(\\mathbf{y}|\\boldsymbol{\\theta})\\cdot\\Pr(\\boldsymbol{\\theta})}{\\int\\Pr(\\mathbf{y}|\\boldsymbol{\\theta})\\cdot\\Pr(\\boldsymbol{\\theta})\\;d\\boldsymbol{\\theta}} \\tag{6.17} \\end{equation}\\] Bayes rule involves the likelihood function \\(\\Pr(\\mathbf{y}|\\boldsymbol{\\theta})\\), which we already know from maximum likelihood estimation (Equation (6.1)). But this time the complete likelihood function is used, not just its maximum. Bayes rule also requires us to specify a probability distribution of the parameters unconditioned on the data, the so called prior distribution \\(\\Pr(\\boldsymbol{\\theta})\\). The denominator in Equation (6.17) can be viewed simply as a normalising constant and we don’t have to worry about it much. The likelihood function is the same that we would use in the frequentist approach - just that we use it fully here. So for a linear model with assumed iid normal residuals the likelihood function is Equation (6.3). We will see other choices in Chapter 7. The prior distribution is the only new choice and requires some thought. The prior is meant to capture our uncertainty about plausible parameter values before considering the data at hand. Ideally, this is informed by previous experience and can thus be “informative”, i.e. the PDF is narrowly centred on certain values. If we don’t have any clue about plausible parameter values then we might use an “uninformative” prior, e.g. a uniform distribution over the real line or some plausible range. For simple problems this gives the same results as maximum likelihood estimation, but with the different meaning of probability discussed above. Using uniform priors can, however, be numerically unstable. In practice, we will most likely resort to “weakly informative” priors in regression problems, e.g. wide normal distributions for the parameters centred on zero. Weakly informative means that we need moderately strong evidence in the data to pull the parameter estimates away from zero (no effect), which is an efficient measure against overfitting. What happens in Bayesian inference can be illustrated with Figure 6.1: The prior is effectively updated by the likelihood to yield the posterior. The likelihood thereby encodes the information in the data about plausible parameter values, mediated by our model of the data generation process, e.g. the linear model with iid normal residuals to stay with our example of linear regression. Figure 6.1: Bayesian updating: The prior PDF of a hypothetical parameter \\(\\theta\\) (green) is updated by the likelihood function (blue) to yield the posterior PDF (red). We see clearly how the posterior is a compromise between the prior and the likelihood. From the posterior, confidence interval-like metrics can be calculated, though these are called compatibility intervals in Bayesian statistics according to recent terminology.12 This is generally done numerically by sampling from the posterior - we will do this below. Once we’ve got our head round this it’s quite straightforward. And we have direct probabilistic estimates of the parameters, without having to invoke any sampling distributions.13 6.2.3 A Bayesian analysis of the yield dataset Let’s illustrate the Bayesian approach briefly for the yield dataset of Chapter 4. Implementation details, prior choices, model comparison and more complex models will be covered in Applied Statistical Modelling in the summer term. We use the brms package as the interface from R to the Bayesian inference engine Stan:14 # load brms package library(brms) # load yields data yields &lt;- read.table(&quot;data/yields.txt&quot;,header=T) # expand to a long variable &quot;yield&quot; and a index variable &quot;soiltype&quot; yields_long &lt;- data.frame(yield = c(yields$sand, yields$clay, yields$loam), soiltype = as.factor(c(rep(1,10), rep(2,10), rep(3,10)))) # fit linear model using brms with default priors yield_fit &lt;- brm(yield ~ 0 + soiltype, data = yields_long, family = gaussian(), silent = TRUE, refresh = 0) We have fitted the model using default priors so let’s check quickly which these are: # check default priors prior_summary(yield_fit) ## prior class coef group resp dpar nlpar bound source ## (flat) b default ## (flat) b soiltype1 (vectorized) ## (flat) b soiltype2 (vectorized) ## (flat) b soiltype3 (vectorized) ## student_t(3, 0, 4.4) sigma default This rather cryptic output tells us that brms has used flat, i.e. uniform, priors for the three parameters, which are the unique means for the three soil types (compare Chapter 4), and a t-distribution with 3 degrees of freedom, centred on 0 and scaled by 4.4, as prior for \\(\\sigma\\). Let’s look at the parameter estimates: # summarise posterior posterior_summary(yield_fit, pars = c(&#39;soiltype1&#39;,&#39;soiltype2&#39;,&#39;soiltype3&#39;,&#39;sigma&#39;)) ## Estimate Est.Error Q2.5 Q97.5 ## b_soiltype1 9.894787 1.126319 7.647740 12.098366 ## b_soiltype2 11.497368 1.084361 9.400974 13.704449 ## b_soiltype3 14.299465 1.130634 12.021777 16.493034 ## sigma 3.538722 0.509612 2.715191 4.691014 This output gives us the median of the posterior for each parameter (“Estimate”), the standard error of that estimate via the standard deviation of the posterior (“Est.Error”) and the central 95% compatibility interval between the bounds “Q2.5” and “Q97.5”. The central parameter estimates are essentially the same as in the frequentist approach (Chapter 4). The standard errors are slightly higher than the frequentist estimate of 1.08, and not homogeneous. The residual standard deviation, here \\(\\sigma\\), is slightly higher than the frequentist estimate of \\(\\sqrt{\\frac{SSE}{n-k}}=\\sqrt{11.7}=3.4\\). These are just summaries. To get a sense of the full posterior we need to extract the numerical samples of the posterior and then we can plot these as histograms, for example. Note, these are discrete approximations of the posterior PDFs, which already for moderately complex problems don’t exist in closed form, so working with samples is the most general and often the only option. # extract posterior samples s &lt;- posterior_samples(yield_fit, pars = c(&#39;soiltype1&#39;,&#39;soiltype2&#39;,&#39;soiltype3&#39;,&#39;sigma&#39;)) # plot parameter posteriors as histograms hist(s$b_soiltype1, freq = FALSE) hist(s$b_soiltype2, freq = FALSE) hist(s$b_soiltype3, freq = FALSE) The beauty now is that we can calculate a probability distribution of the average yield differences between soil types, without having to worry about the two-sample t-test we did back in Chapter 4 and its sampling distribution and all that. It’s very simple: # plot average yield differences between soil types as histograms hist(s$b_soiltype2-s$b_soiltype1, freq = FALSE) hist(s$b_soiltype3-s$b_soiltype1, freq = FALSE) hist(s$b_soiltype3-s$b_soiltype2, freq = FALSE) # express these differences as median and 95% compatibility interval quantile(s$b_soiltype2-s$b_soiltype1, probs = c(0.025, 0.5, 0.975)) ## 2.5% 50% 97.5% ## -1.450571 1.612319 4.764457 quantile(s$b_soiltype3-s$b_soiltype1, probs = c(0.025, 0.5, 0.975)) ## 2.5% 50% 97.5% ## 1.287159 4.360585 7.527257 quantile(s$b_soiltype3-s$b_soiltype2, probs = c(0.025, 0.5, 0.975)) ## 2.5% 50% 97.5% ## -0.3365286 2.8329556 5.8520960 We see that the only yield difference that is uniquely positive at the 95% compatibility level is that between soil types 3 and 1 - sand and loam. All other compatibility intervals overlap with zero meaning there are sizable probabilities of the differences going in either direction. In frequentist language we would call these differences insignificant, but looking at the full posterior distribution of the differences is much more useful than the binary split. All in all, this example shows how for simple models frequentist and Bayesian inference with uninformative priors yield essentially the same conclusions, although the Bayesian probabilities are much more intuitive to interpret and easier to post-process into any quantities of interest. Bayesian statistics is named after 18th century Presbyterian minister Thomas Bayes, who conducted a famous inferential experiment by applying what was later called Bayes rule. This type of inferential reasoning, however, predates Bayes and there were more influential figures since, but somehow the name stuck. Not even Bayes rule is anything special; it arises simply from rearranging the product rule of probability calculus.↩︎ These tests got their names from the sampling distributions of their test statistics.↩︎ As said previously, Bayes rule is just the rearranged product rule of basic probability calculus: \\(\\Pr(A,B)=\\Pr(A|B)\\cdot\\Pr(B)=\\Pr(B|A)\\cdot\\Pr(A)\\)↩︎ Gelman, Hill, and Vehtari (2020); alternative terms are “uncertainty intervals” or, somewhat outdated, “credible intervals”.↩︎ Note, another problem with frequentist statistics is that, even if sampling distributions may provide useful approximations of real-world uncertainties, already for moderately complex models there exist no closed-form sampling distributions. Here the Bayesian approach, and its numerical sampling, are much more general.↩︎ Other packages are available, most notably rstanarm, which is a little more intuitive but less comprehensive.↩︎ "],
["glms.html", "Chapter 7 Generalised Linear Models (GLMs)", " Chapter 7 Generalised Linear Models (GLMs) Under construction. "],
["multivariate.html", "Chapter 8 Multivariate methods 8.1 Cluster analysis 8.2 Principal Component Analysis (PCA) 8.3 Multivariate ANOVA (MANOVA) 8.4 Discriminant Function Analysis (DFA)", " Chapter 8 Multivariate methods 8.1 Cluster analysis 8.2 Principal Component Analysis (PCA) 8.3 Multivariate ANOVA (MANOVA) 8.4 Discriminant Function Analysis (DFA) "],
["solutions-to-exercises.html", "Solutions to exercises Answer to Q1 Answer to Q2", " Solutions to exercises Answer to Q1 Equations (3.3), (3.5) and (3.7) are linear models; the others are non-linear in their parameters. Note, that the variables of linear models can take non-linear forms (\\(x_1^3, x_1 \\cdot x_2, \\log x_1\\)) as long as the parameters are not implicated in this non-linearity. Answer to Q2 We have 6 main predictors, 5+4+3+2+1=15 2-way interactions, 20 3-way interactions, 15 4-way interactions, 6 5-way interactions and 1 6-way interaction, i.e. 63 possible predictors. Note the symmetry of this calculation: the number of possibilities of combining 2 variables (2-way interactions) is the same as the number of possibilities of leaving out 2 variables (4-way interactions). "],
["references.html", "References", " References "]
]
