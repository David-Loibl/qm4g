[
["index.html", "Quantitative Methods for Geographers Preface", " Quantitative Methods for Geographers Tobias Krueger 2020-10-28 Preface This is the script of the course ‘Quantitative Methods for Geographers’ run at the Geography Department of Humboldt-Universität zu Berlin. "],
["orga.html", "Chapter 1 Organisational matters 1.1 Motivating example 1.2 Topics 1.3 Format", " Chapter 1 Organisational matters Welcome to the course Quantitative Methods for Geographers (online edition), which consists of a seminar and a PC lab. In the seminar, theoretical input will be provided by myself up until January and by Tobia Lakes in the last four sessions in February. Part of this course is a PC lab run by Dirk Pflugmacher and David Loibl. 1.1 Motivating example Figure 1.1 shows the spatial variation of annual average precipitation and annual average temperature over Germany. Before reading on, reflect a minute on what might control this spatial variation. Figure 1.1: Maps of annual average precipitation (left) and annual average air temperature (right) over Germany from 1961 to 1990. Source: https://www.dwd.de/EN/climate_environment/climateatlas/climateatlas_node.html. The spatial variation of average precipitation and temperature in our climatic zone is controlled mainly by elevation and continentality: the higher up we go the more it rains over the year and the colder it gets; and the further East we go the drier it gets while a temperature effect is not visible. There are of course more nuanced effects such as aspect, but these are not so clearly visible in this figure. And this is precisely the goal of large parts of this course: To try and explain patterns in data (so called response variables) over space and time - such as rainfall and temperature - with other data (so called predictor variables). We do this using statistical inference methods. This includes separating dominant predictors from not so dominant predictors. It also includes making predictions with the relationships we find, e.g. for interpolating between data points to create maps like Figure 1.1. By the end of this course you will have learned the following: You have understood the concept of statistical inference using the linear model in depth. This includes linear regression with one or more predictors (multiple regression). It also includes mildly non-linear models and specific types of responses, dealt with by so called Generalised Linear Models. You have worked with metric as well as categorical predictors and mixtures of the two. You have learned metrics to compare and simplify models and evaluate their assumptions. You have understood the principles of extending these techniques to cases of more than one response variable (multivariate methods). You have learned specific techniques for working with spatial data. You can carry out the corresponding analyses in the software R. 1.2 Topics The timing of topics is shown in Table 1.1. Table 1.1: Schedule of Quantitative Methods for Geographers. Week Reading Monday seminar Wednesday PC lab 1 Organisational matters no class due to Dies Academicus Introduction to R 2 Mathematical preliminaries Introductions Data manipulation and import/export with R 3 The linear model Mathematical preliminaries Visualization and data manipulation with R 4 Categorical variables (ANOVA) and dummy coding The linear model Linear regression 5 Multiple linear regression Categorical variables (ANOVA) and dummy coding Hypothesis testing and ANOVA 6 Maximum Likelihood and outlook to Bayesian statistics Multiple linear regression Multiple linear regression 7 Generalized linear models I Maximum Likelihood and outlook to Bayesian statistics Outlook to machine learning 8 Generalized linear models II Generalized linear models I Generalized linear models I 9 Multivariate methods I Generalized linear models II Generalized linear models II 10 Multivariate methods II Multivariate methods I Principal Component Analysis 11 tbc Multivariate methods II Discriminant Function Analysis and model validation 12 tbc Understanding spatial data (Tobia) Spatial data and cluster analysis in R 13 tbc Point pattern analysis (Tobia) Point pattern analysis and spatial auto-correlation 14 tbc Spatial autocorrelation and interpolation (Tobia) Semivariogram analysis and kriging 15 tbc Spatial weights and linear modeling (Tobia) Spatial regression models 1.3 Format During this digital semester, the learning mode will be mainly reading. You are required to read a chapter of this script each week, which will then be discussed in a ZOOM session the following Monday 13:00-15:00 (see link on Moodle). The reading listed for each week in the table above is due the following week. I will provide you with guiding questions and small quizzes to guide your reading. You are required to post questions on the topics or aspects you would like me to focus on during the seminar in the Moodle Forum by each Friday. This way we know what to discuss each Monday and I will prepare some lecture-style input. We can also discuss questions that arise in the PC labs. Some questions might already be answered via the Forum. In the PC labs, Dirk and David will give you homework, which you need to submit via Moodle to pass the course. They will explain this in detail. The final exam is a project similar to an extended homework, in which you will be able to apply and expand on topics studied in class, using datasets provided in the research context of the Geography Department. The project will be done individually (other than in previous years) and each of you will get their own topic and data. We will allocate topics towards the end of the semester via Moodle. The project has to be submitted as a HTML document created by R Markdown towards the end of the semester break (deadline tbc). "],
["math.html", "Chapter 2 Mathematical preliminaries 2.1 Logarithm and exponentiation 2.2 Centring and standardisation 2.3 Derivatives 2.4 Matrix algebra 2.5 Exercises", " Chapter 2 Mathematical preliminaries In this chapter we get a few mathematical preliminaries out of the way that are important for later chapters. If you feel rusty on any of these then please read up on them elsewhere. 2.1 Logarithm and exponentiation The following is inspired by Gelman and Nolan (2002). Suppose you have an amoeba that takes one hour to divide (Figure 2.1), and then the two amoebas each divide in one more hour, and so forth. What is the equation of the number of amoebas, \\(y\\), as a function of time, \\(t\\) (in hours)? Figure 2.1: Amoeba dividing. Source: http://www.gutenberg.org/files/18451/18451-h/images/illus002.jpg. The equation is: \\[\\begin{equation} y=2^t \\tag{2.1} \\end{equation}\\] This is an exponential function with base 2 and exponent \\(t\\). Figure 2.2 shows two plots of this function. (Don’t worry, you will start to understand the R code better as you progress in the PC labs.) t &lt;- seq(1, 6) y &lt;- 2^t plot(t, y, pch = 19, type = &#39;b&#39;) plot(t, log(y), pch = 19, type = &#39;b&#39;) Figure 2.2: Left: Plot of Equation 2.1. Right: Plot of Equation 2.1 on logarithmic scale. The inverse of the exponential function is the logarithmic function: \\[\\begin{equation} log(y)=log(2^t)=t \\cdot log(2) \\tag{2.2} \\end{equation}\\] Since the logarithm of \\(y\\) is a linear function of \\(t\\) (Equation (2.2)), the right-hand side of Figure 2.2 (\\(y\\) on logarithmic scale) displays a straight line. Common bases of the logarithmic function are: \\[\\begin{equation} log_2\\left(2^t\\right)=lb\\left(2^t\\right)=t \\tag{2.3} \\end{equation}\\] This is called the binary logarithm (lb). \\[\\begin{equation} log_{10}\\left(10^t\\right)=lg\\left(10^t\\right)=t \\tag{2.4} \\end{equation}\\] This is called the common logarithm (lg). \\[\\begin{equation} log_e\\left(e^t\\right)=ln\\left(e^t\\right)=t \\tag{2.5} \\end{equation}\\] This is called the natural logarithm (ln) with \\(e \\approx 2.7183\\) being Euler’s constant. Note, programming often uses a different notation, which will also be used from now on in this course: \\[\\begin{equation} ln()=log() \\tag{2.6} \\end{equation}\\] \\[\\begin{equation} e^t=\\exp(t) \\tag{2.7} \\end{equation}\\] Basic rules for exponentiation are: \\[\\begin{equation} a^m \\cdot a^n=a^{m+n} \\tag{2.8} \\end{equation}\\] \\[\\begin{equation} a^n \\cdot b^n=(a \\cdot b)^n \\tag{2.9} \\end{equation}\\] \\[\\begin{equation} \\frac{a^m}{a^n}=a^{m-n} \\tag{2.10} \\end{equation}\\] \\[\\begin{equation} \\frac{a^n}{b^n}=\\left(\\frac{a}{b}\\right)^n \\tag{2.11} \\end{equation}\\] \\[\\begin{equation} \\left(a^m\\right)^n=a^{m \\cdot n} \\tag{2.12} \\end{equation}\\] At this point it is also useful to remind ourselves of the meaning of the sum and product symbols: \\[\\begin{equation} \\sum_{i=1}^{n}x_i=x_1+x_2+\\ldots+x_n \\tag{2.13} \\end{equation}\\] This signifies the sum of all \\(x_i\\) for \\(i\\) taking integer values from 1 to \\(n\\). \\[\\begin{equation} \\prod_{i=1}^{n}x_i=x_1 \\cdot x_2 \\cdot \\ldots \\cdot x_n \\tag{2.14} \\end{equation}\\] This signifies the product of all \\(x_i\\) for \\(i\\) taking integer values from 1 to \\(n\\). The basic rules of logarithm are: \\[\\begin{equation} log(u \\cdot v)=log(u)+log(v) \\tag{2.15} \\end{equation}\\] \\[\\begin{equation} log\\left(\\frac{u}{v}\\right)=log(u)-log(v) \\tag{2.16} \\end{equation}\\] \\[\\begin{equation} log\\left(u^r\\right)=r \\cdot log(u) \\tag{2.17} \\end{equation}\\] 2.2 Centring and standardisation Centring and standardisation are used to transform different datasets onto the same scale. We will need this for the multivariate methods in later sessions (Chapter 8). Centring means subtracting from every data point \\(y\\) the overall mean of the dataset \\(\\bar{y}\\): \\[\\begin{equation} y^*=y-\\bar{y} \\tag{2.18} \\end{equation}\\] This yields new data points \\(y^*\\) and a new mean \\(\\bar{y^*}=0\\) while the standard deviation of the transformed data remains the same: \\(s_{y^*}=s_y\\). Centring thus shifts the data histogram to be centred on zero, but does not change its shape (Figure 2.3). # draw random sample of size 1000 from normal distribution with mean 1 and standard deviation 2, i.e. y~N(1,2) y &lt;- rnorm(1000, mean = 1, sd = 2) # mean ybar &lt;- mean(y) # standard deviation s_y &lt;- sd(y) # histogram, raw hist(y, freq = FALSE, xlim = c(-10,10), ylim = c(0, 0.4), main = &quot;&quot;, xlab = &quot;y&quot;, ylab = &quot;relative frequency&quot;) lines(c(ybar, ybar), c(0,0.4), lwd = 3) # histogram, centred hist(y-ybar, freq = FALSE, xlim = c(-10,10), ylim = c(0, 0.4), main = &quot;&quot;, xlab = &quot;y*&quot;, ylab = &quot;relative frequency&quot;) lines(c(0, 0), c(0,0.4), lwd = 3) Figure 2.3: Histogram of dataset \\(y\\) (left) and centred dataset \\(y^*\\) (right). The vertical line represents the mean. Standardisation means subtracting from every data point \\(y\\) the overall mean of the dataset \\(\\bar{y}\\) and additionally dividing by the standard deviation \\(s_y\\): \\[\\begin{equation} y^*=\\frac{y-\\bar{y}}{s_y} \\tag{2.19} \\end{equation}\\] This yields new data points \\(y^*\\), a new mean \\(\\bar{y^*}=0\\) and a new standard deviation \\(s_{y^*}=1\\). Standardisation thus shifts the data histogram to be centred on zero and expands or contracts it to have unit standard deviation (Figure 2.4). # histogram, raw hist(y, freq = FALSE, xlim = c(-10,10), ylim = c(0, 0.4), main = &quot;&quot;, xlab = &quot;y&quot;, ylab = &quot;relative frequency&quot;) lines(c(ybar, ybar), c(0,0.4), lwd = 3) # histogram, standardised hist((y-ybar)/s_y, freq = FALSE, xlim = c(-10,10), ylim = c(0, 0.4), main = &quot;&quot;, xlab = &quot;y*&quot;, ylab = &quot;relative frequency&quot;) lines(c(0, 0), c(0,0.4), lwd = 3) Figure 2.4: Histogram of dataset \\(y\\) (left) and standardised dataset \\(y^*\\) (right). The vertical line represents the mean. If the original data \\(y\\) were normally distributed (like we set it up for the plots above) then standardisation would transform \\(y\\) to the scale of the standard normal distribution, i.e. a normal distribution with mean 0 and standard deviation 1. If you feel rusty on the normal distribution (or probability distributions in general) then Wikipedia is as good a source as any: https://en.wikipedia.org/wiki/Normal_distribution. 2.3 Derivatives The first derivative of a function \\(f(x)\\), written as \\(f&#39;(x)\\) or \\(\\frac{df(x)}{dx}\\), can be interpreted graphically as the slope of that function, i.e. the tangent line of a certain point of the function (Figure 2.5, left). Figure 2.5: Left: Tangent line of function \\(f(x)\\). Centre: Secant line of function \\(f(x)\\) between point \\(f\\left(x_0\\right)\\) and point \\(f\\left(x_0+h\\right)\\); the horizontal distance between these two points is \\(\\Delta x\\) and the vertical distance is \\(\\Delta f(x)\\). Right: Set of secant lines of function \\(f(x)\\) between point \\(f\\left(x_0\\right)\\) and point \\(f\\left(x_0+h\\right)\\) for progressively decreasing increments \\(h\\). Source: https://en.wikipedia.org/wiki/Derivative. Mathematically, the slope amounts to the limiting value of the ratio of the (vertical) increment of the function, \\(\\Delta f(x)\\), for an (horizontal) increment of \\(x\\), \\(\\Delta x\\), for \\(\\Delta x\\) approaching zero, \\(\\Delta x \\to 0\\): \\[\\begin{equation} f&#39;(x)=\\frac{df(x)}{dx}=\\lim_{\\Delta x \\to 0}\\frac{\\Delta f(x)}{\\Delta x} \\tag{2.20} \\end{equation}\\] This can be visualised as a secant line of the function between two points, \\(x_0\\) and \\(x_0+h\\) (Figure 2.5, centre), whose slope is: \\[\\begin{equation} \\frac{\\Delta f(x)}{\\Delta x}=\\frac{f\\left(x_0+h\\right)-f\\left(x_0\\right)}{\\left(x_0+h\\right)-\\left(x_0\\right)}=\\frac{f\\left(x_0+h\\right)-f\\left(x_0\\right)}{h} \\tag{2.21} \\end{equation}\\] As \\(h\\) approaches zero (Figure 2.5, right) we reach the limiting value of the slope at point \\(x_0\\), which is the first derivative: \\[\\begin{equation} \\frac{df(x)}{dx}=\\lim_{h \\to 0}\\frac{f\\left(x_0+h\\right)-f\\left(x_0\\right)}{h} \\tag{2.22} \\end{equation}\\] The first derivative is useful for finding minima, maxima and inflexion points of a function, because this is where the slope is zero, \\(\\frac{df(x)}{dx}=0\\) (Figure 2.6). The second derivative, measuring the curvature of the function, tells us whether these points are minima \\(\\left(\\frac{d^2f(x)}{dx^2}&gt;0\\right)\\), maxima \\(\\left(\\frac{d^2f(x)}{dx^2}&lt;0\\right)\\) or inflexion points \\(\\left(\\frac{d^2f(x)}{dx^2}=0\\right)\\), but often we already know that a function has only a single minimum or maximum and then we do not need the second derivative. Figure 2.6: Use of first and second derivative to determine minima, maxima and inflexion points of a function. Source: http://hyperphysics.phy-astr.gsu.edu/hbase/math/maxmin.html. The differentiation rules are listed below: If \\(y=f(t)=t^a\\) then \\(\\frac{dy}{dt}=a \\cdot t^{a-1}\\), i.e. multiplying the function with the exponent and reducing the exponent by one gives you the derivative of \\(f(x)\\). Constant factor rule: If \\(y=c \\cdot u(t)\\) then \\(\\frac{dy}{dt}=c \\cdot \\frac{du}{dt}\\). Sum rule: If \\(y=u(t) \\pm v(t)\\) then \\(\\frac{dy}{dt}=\\frac{du}{dt} \\pm \\frac{dv}{dt}\\). Product rule: If \\(y=u(t) \\cdot v(t)\\) then \\(\\frac{dy}{dt}=\\frac{du}{dt} \\cdot v+u \\cdot \\frac{dv}{dt}\\). Quotient rule: If \\(y=\\frac{u(t)}{v(t)}\\) then \\(\\frac{dy}{dt}=\\frac{\\left(\\frac{du}{dt} \\cdot v-u \\cdot \\frac{dv}{dt}\\right)}{v^2}\\). Chain rule: If \\(y=f[g(t)]\\) then \\(\\frac{dy}{dt}=\\frac{df[g]}{dg} \\cdot \\frac{dg}{dt}\\), i.e. “outer times inner derivative”. 2.4 Matrix algebra The following is based on Tabachnick and Fidell (2013). 2.4.1 Simple matrix operations Let \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) be matrices: \\[\\mathbf{A} = \\begin{pmatrix} a &amp; b &amp; c\\\\ d &amp; e &amp; f\\\\ g &amp; h &amp; i \\end{pmatrix} = \\begin{pmatrix} 3 &amp; 2 &amp; 4\\\\ 7 &amp; 5 &amp; 0\\\\ 1 &amp; 0 &amp; 8 \\end{pmatrix} \\] \\[\\mathbf{B} = \\begin{pmatrix} r &amp; s &amp; t\\\\ u &amp; v &amp; w\\\\ x &amp; y &amp; z \\end{pmatrix} = \\begin{pmatrix} 6 &amp; 1 &amp; 0\\\\ 2 &amp; 8 &amp; 7\\\\ 3 &amp; 4 &amp; 5 \\end{pmatrix} \\] Addition/subtraction of a constant to a matrix happens element-wise: \\[\\mathbf{A} + k = \\begin{pmatrix} a+k &amp; b+k &amp; c+k\\\\ d+k &amp; e+k &amp; f+k\\\\ g+k &amp; h+k &amp; i+k \\end{pmatrix} \\] \\[\\mathbf{A} - k = \\begin{pmatrix} a-k &amp; b-k &amp; c-k\\\\ d-k &amp; e-k &amp; f-k\\\\ g-k &amp; h-k &amp; i-k \\end{pmatrix} \\] Multiplication/division of a matrix by a constant also happens element-wise: \\[k \\cdot \\mathbf{A} = \\begin{pmatrix} k \\cdot a &amp; k \\cdot b &amp; k \\cdot c\\\\ k \\cdot d &amp; k \\cdot e &amp; k \\cdot f\\\\ k \\cdot g &amp; k \\cdot h &amp; k \\cdot i \\end{pmatrix} \\] \\[\\frac{1}{k} \\cdot \\mathbf{A} = \\begin{pmatrix} \\frac{1}{k} \\cdot a &amp; \\frac{1}{k} \\cdot b &amp; \\frac{1}{k} \\cdot c\\\\ \\frac{1}{k} \\cdot d &amp; \\frac{1}{k} \\cdot e &amp; \\frac{1}{k} \\cdot f\\\\ \\frac{1}{k} \\cdot g &amp; \\frac{1}{k} \\cdot h &amp; \\frac{1}{k} \\cdot i \\end{pmatrix} \\] Addition/subtraction of two matrices happens element-wise again: \\[\\mathbf{A} + \\mathbf{B} = \\begin{pmatrix} a+r &amp; b+s &amp; c+t\\\\ d+u &amp; e+v &amp; f+w\\\\ g+x &amp; h+y &amp; i+z \\end{pmatrix} \\] \\[\\mathbf{A} - \\mathbf{B} = \\begin{pmatrix} a-r &amp; b-s &amp; c-t\\\\ d-u &amp; e-v &amp; f-w\\\\ g-x &amp; h-y &amp; i-z \\end{pmatrix} \\] Finally, the so called transpose of a matrix refers to the mirroring of a matrix along its diagonal. Hence, the transpose of \\(\\mathbf{A}\\) is: \\[\\mathbf{A}&#39; = \\begin{pmatrix} a &amp; d &amp; g\\\\ b &amp; e &amp; h\\\\ c &amp; f &amp; i \\end{pmatrix} \\] 2.4.2 Matrix multiplication Now, the multiplication of two matrices is the only operation that is a bit complicated at first. It may be best to consider an example to work out the rules: \\[\\begin{eqnarray} \\mathbf{A} \\cdot \\mathbf{B}&amp;=&amp; \\begin{pmatrix} a &amp; b &amp; c\\\\ d &amp; e &amp; f\\\\ g &amp; h &amp; i \\end{pmatrix} \\cdot \\begin{pmatrix} r &amp; s &amp; t\\\\ u &amp; v &amp; w\\\\ x &amp; y &amp; z \\end{pmatrix}\\\\ &amp;=&amp;\\begin{pmatrix} a \\cdot r + b \\cdot u + c \\cdot x &amp; a \\cdot s + b \\cdot v + c \\cdot y &amp; a \\cdot t + b \\cdot w + c \\cdot z\\\\ d \\cdot r + e \\cdot u + f \\cdot x &amp; d \\cdot s + e \\cdot v + f \\cdot y &amp; d \\cdot t + e \\cdot w + f \\cdot z\\\\ g \\cdot r + h \\cdot u + i \\cdot x &amp; g \\cdot s + h \\cdot v + i \\cdot y &amp; g \\cdot t + h \\cdot w + i \\cdot z \\end{pmatrix}\\\\ &amp;=&amp;\\begin{pmatrix} 34 &amp; 35 &amp; 34\\\\ 52 &amp; 47 &amp; 35\\\\ 30 &amp; 33 &amp; 40 \\end{pmatrix} \\end{eqnarray}\\] The result of a matrix multiplication has as many rows as the 1st matrix and as many columns as the 2nd. For this to work, the number of columns of the 1st matrix must match the number of rows of the 2nd matrix. In our example, this does not matter as the matrices are square, i.e. they have as many rows as columns. To construct each cell of the results matrix, one row of the 1st matrix is combined with one column of the 2nd matrix. For cell (1,1) (top-left), for example, we combine the 1st row of matrix 1 (here \\(\\mathbf{A}\\)) and the 1st column of matrix 2 (here \\(\\mathbf{B}\\)). Moving to the right, for cell (1,2) (top-middle), we combine the 1st row of matrix 1 and the 2nd column of matrix 2. For cell (2,1) (middle-left), we combine the 2nd row of matrix 1 and the 1st column of matrix 2. And so on and so forth. The combination of the two respective vectors is the sum of the products of the vector elements paired in order. So for cell (1,1) in our example this is \\(a \\cdot r + b \\cdot u + c \\cdot x\\). Try and recreate the following example to get a feeling for the matrix multiplication rules. \\[\\begin{eqnarray} \\mathbf{B} \\cdot \\mathbf{A}&amp;=&amp; \\begin{pmatrix} r &amp; s &amp; t\\\\ u &amp; v &amp; w\\\\ x &amp; y &amp; z \\end{pmatrix} \\cdot \\begin{pmatrix} a &amp; b &amp; c\\\\ d &amp; e &amp; f\\\\ g &amp; h &amp; i \\end{pmatrix}\\\\ &amp;=&amp;\\begin{pmatrix} r \\cdot a + s \\cdot d + t \\cdot g &amp; r \\cdot b + s \\cdot e + t \\cdot h &amp; r \\cdot c + s \\cdot f + t \\cdot i\\\\ u \\cdot a + v \\cdot d + w \\cdot g &amp; u \\cdot b + v \\cdot e + w \\cdot h &amp; u \\cdot c + v \\cdot f + w \\cdot i\\\\ x \\cdot a + y \\cdot d + z \\cdot g &amp; x \\cdot b + y \\cdot e + z \\cdot h &amp; x \\cdot c + y \\cdot f + z \\cdot i \\end{pmatrix}\\\\ &amp;=&amp;\\begin{pmatrix} 25 &amp; 17 &amp; 24\\\\ 69 &amp; 44 &amp; 64\\\\ 42 &amp; 26 &amp; 52 \\end{pmatrix} \\end{eqnarray}\\] The point with this example is that \\(\\mathbf{A} \\cdot \\mathbf{B}\\) is not the same as \\(\\mathbf{B} \\cdot \\mathbf{A}\\). The order matters when multiplying matrices! Let’s consider two more example: \\[\\begin{eqnarray} \\mathbf{A} \\cdot \\mathbf{A}&amp;=&amp; \\begin{pmatrix} a &amp; b &amp; c\\\\ d &amp; e &amp; f\\\\ g &amp; h &amp; i \\end{pmatrix} \\cdot \\begin{pmatrix} a &amp; b &amp; c\\\\ d &amp; e &amp; f\\\\ g &amp; h &amp; i \\end{pmatrix}\\\\ &amp;=&amp;\\begin{pmatrix} a^2 + b \\cdot d + c \\cdot g &amp; a \\cdot b + b \\cdot e + c \\cdot h &amp; a \\cdot c + b \\cdot f + c \\cdot i\\\\ d \\cdot a + e \\cdot d + f \\cdot g &amp; d \\cdot b + e^2 + f \\cdot h &amp; d \\cdot c + e \\cdot f + f \\cdot i\\\\ g \\cdot a + h \\cdot d + i \\cdot g &amp; g \\cdot b + h \\cdot e + i \\cdot h &amp; g \\cdot c + h \\cdot f + i^2 \\end{pmatrix}\\\\ &amp;=&amp;\\begin{pmatrix} 27 &amp; 16 &amp; 44\\\\ 56 &amp; 39 &amp; 28\\\\ 11 &amp; 2 &amp; 68 \\end{pmatrix} \\end{eqnarray}\\] This matrix multiplied with itself, \\(\\mathbf{A} \\cdot \\mathbf{A}\\), is different to the same matrix multiplied with its transpose \\(\\mathbf{A} \\cdot \\mathbf{A}&#39;\\): \\[\\begin{eqnarray} \\mathbf{A} \\cdot \\mathbf{A}&#39;&amp;=&amp; \\begin{pmatrix} a &amp; b &amp; c\\\\ d &amp; e &amp; f\\\\ g &amp; h &amp; i \\end{pmatrix} \\cdot \\begin{pmatrix} a &amp; d &amp; g\\\\ b &amp; e &amp; h\\\\ c &amp; f &amp; i \\end{pmatrix}\\\\ &amp;=&amp;\\begin{pmatrix} a^2 + b^2 + c^2 &amp; a \\cdot d + b \\cdot e + c \\cdot f &amp; a \\cdot g + b \\cdot h + c \\cdot i\\\\ d \\cdot a + e \\cdot b + f \\cdot c &amp; d^2 + e^2 + f^2 &amp; d \\cdot g + e \\cdot h + f \\cdot i\\\\ g \\cdot a + h \\cdot b + i \\cdot c &amp; g \\cdot d + h \\cdot e + i \\cdot f &amp; g^2 + h^2 + i^2 \\end{pmatrix}\\\\ &amp;=&amp;\\begin{pmatrix} 29 &amp; 31 &amp; 35\\\\ 31 &amp; 74 &amp; 7\\\\ 35 &amp; 7 &amp; 65 \\end{pmatrix} \\end{eqnarray}\\] This last matrix is symmetrical, i.e. mirrored along its diagonal. Diagonal elements are so called sums of squares, off-diagonal elements are so called cross-products. This will be useful later on when working with variance-covariance matrices in Chapter 8. 2.4.3 Matrix division, inverse of a matrix, identity matrix Division of two matrices means multiplication of one matrix with the so called inverse of the other, here \\(\\mathbf{B}^{-1}\\): \\[\\frac{\\mathbf{A}}{\\mathbf{B}} = \\mathbf{A} \\cdot \\mathbf{B}^{-1}\\] The inverse of a matrix is different to the transpose. It is rather complicated to calculate, using in most cases numerical (and not analytical) techniques that we do not go into here. At the most general level, the inverse is found so that the following equation holds: \\[\\mathbf{A} \\cdot \\mathbf{A}^{-1} = \\mathbf{A}^{-1} \\cdot \\mathbf{A} = \\mathbf{I}\\] With \\(\\mathbf{I}\\) being the identity matrix, i.e. a matrix with diagonal elements 1 and off-diagonal elements 0: \\[\\mathbf{I} = \\begin{pmatrix} 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix} \\] 2.5 Exercises Exercise 1 Apply the rules in the script to the following equation to get rid of the product operator, and then simplify the resultant equation as much as you can: \\[\\begin{equation} \\prod_{i=1}^{n}\\frac{1}{\\sigma \\cdot \\sqrt{2 \\cdot \\pi}} \\cdot \\exp\\left(\\frac{\\left(y_i - \\beta_0 - \\beta_1 \\cdot x_i\\right)^2}{-2 \\cdot \\sigma^2}\\right)= \\tag{2.23} \\end{equation}\\] Exercise 2 Apply the differentiation rules in the script to take the derivative of the following equation: \\[\\begin{equation} \\frac{d\\sum_{i=1}^{n}\\left(y_i - \\beta_0 - \\beta_1 \\cdot x_i\\right)^2}{d\\beta_0}= \\tag{2.24} \\end{equation}\\] Exercise 3 Consider the following vectors and matrix: \\(y = \\begin{pmatrix} y_1\\\\ y_2\\\\ y_3 \\end{pmatrix}\\), \\(\\beta = \\begin{pmatrix} \\beta_0\\\\ \\beta_1\\\\ \\beta_2\\\\ \\beta_3 \\end{pmatrix}\\), \\(\\epsilon = \\begin{pmatrix} \\epsilon_1\\\\ \\epsilon_2\\\\ \\epsilon_3 \\end{pmatrix}\\) and \\(\\mathbf{X} = \\begin{pmatrix} 1 &amp; x_{11} &amp; x_{12} &amp; x_{13}\\\\ 1 &amp; x_{21} &amp; x_{22} &amp; x_{23}\\\\ 1 &amp; x_{31} &amp; x_{32} &amp; x_{33} \\end{pmatrix}\\). Now solve the following equation using matrix algebra: \\[y = \\mathbf{X} \\cdot \\beta + \\epsilon = \\] References "],
["lin-reg.html", "Chapter 3 Linear regression 3.1 Motivation 3.2 The linear model 3.3 Description versus prediction 3.4 Linear Regression 3.5 Significance of regression 3.6 Confidence in parameter estimates 3.7 Goodness of fit", " Chapter 3 Linear regression 3.1 Motivation The questions we wish to answer with linear regression are of the kind depicted in Figure 1.1: What drives spatial variation in annual average precipitation and annual average temperature? In the case of precipitation the drivers seem to be continentality and elevation, while temperature appears to be dominantly controlled by elevation only. Linear regression examines this question by modelling a response variable against one or more predictor variables, while the relationship between the two is linear in its parameters. 3.2 The linear model The most general from of a linear model is: \\[\\begin{equation} y = \\beta_0 + \\sum_{j=1}^{p}\\beta_j \\cdot x_j + \\epsilon \\tag{3.1} \\end{equation}\\] In this equation, \\(y\\) is the response variable (also called dependent or output variable), \\(x_j\\) are the predictor variables (also called independent, explanatory, input variables or covariates), \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) are the parameters and \\(\\epsilon\\) is the residual, i.e. that part of the response which remains unexplained by the predictors. In the case of one predictor, which has come to be known as linear regression, the linear model is: \\[\\begin{equation} y = \\beta_0 + \\beta_1 \\cdot x + \\epsilon \\tag{3.2} \\end{equation}\\] It can be visualised as a line, with \\(\\beta_0\\) being the intercept, where the line intersects the vertical axis \\((x=0)\\), and \\(\\beta_1\\) being the slope of the line (Figure 3.1). Note, the point \\(\\left(\\bar{x},\\bar{y}\\right)\\), the centroid of the data, always lies on the line. Figure 3.1: Linear model with one predictor variable (linear regression). Linear means linear in terms of the model parameters, not (necessarily) in terms of the predictor variables. With this in mind, consider the following five models. Which are linear models, which are non-linear models? (Q1)1 \\[\\begin{equation} y = \\beta_0 + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 + \\epsilon \\tag{3.3} \\end{equation}\\] \\[\\begin{equation} y = \\beta_0 + \\beta_1 \\cdot x_1^{\\beta_2} + \\epsilon \\tag{3.4} \\end{equation}\\] \\[\\begin{equation} y = \\beta_0 + \\beta_1 \\cdot x_1^3 + \\beta_2 \\cdot x_1 \\cdot x_2 + \\epsilon \\tag{3.5} \\end{equation}\\] \\[\\begin{equation} y = \\beta_0 + \\exp(\\beta_1 \\cdot x_1) + \\beta_2 \\cdot x_2 + \\epsilon \\tag{3.6} \\end{equation}\\] \\[\\begin{equation} y = \\beta_0 + \\beta_1 \\cdot \\log x_1 + \\beta_2 \\cdot x_2 + \\epsilon \\tag{3.7} \\end{equation}\\] We can also write the linear model equation with the data points explicitly indexed by \\(i\\) for \\(i=1, 2, \\ldots, n\\). We have omitted the index previously for ease of reading: \\[\\begin{equation} y_i = \\beta_0 + \\sum_{j=1}^{p}\\beta_j \\cdot x_{ij} + \\epsilon_i \\tag{3.8} \\end{equation}\\] These data points could be repeat measurements in time or in space. We can also write the model more compactly in a matrix format: \\[\\begin{equation} y = \\mathbf{X} \\cdot \\beta + \\epsilon \\tag{3.9} \\end{equation}\\] With \\(y = \\begin{pmatrix} y_1\\\\ y_2\\\\ y_3 \\end{pmatrix}\\), \\(\\beta = \\begin{pmatrix} \\beta_0\\\\ \\beta_1\\\\ \\beta_2\\\\ \\beta_3 \\end{pmatrix}\\), \\(\\epsilon = \\begin{pmatrix} \\epsilon_1\\\\ \\epsilon_2\\\\ \\epsilon_3 \\end{pmatrix}\\) and \\(\\mathbf{X} = \\begin{pmatrix} 1 &amp; x_{11} &amp; x_{12} &amp; x_{13}\\\\ 1 &amp; x_{21} &amp; x_{22} &amp; x_{23}\\\\ 1 &amp; x_{31} &amp; x_{32} &amp; x_{33} \\end{pmatrix}\\), the latter being the design matrix which summarises the predictor data. When we talk about the linear model, the response variable is always continuous, while the predictor variables can be continuous, categorical or mixed. In principle, each of these variants can be treated mathematically in the same way, e.g. all can be analysed using the lm() function in R. However, historically different names have been established for these variants, which are worth mentioning here to avoid confusion (Tables 3.1 and 3.2). Table 3.1: Historical names for the variants of the linear model, depending on whether the predictors are continuous, categorical or mixed. The response is always continuous. Continuouspredictors Categoricalpredictors Mixedpredictors Regression Analysis of variance(ANOVA) Analysis of covariance(ANCOVA) Table 3.2: Historical names for the regression, depending on whether we have one or more predictors and one or more responses. 1 predictor variable &gt;1 predictor variables 1 response variable Regression Multiple regression &gt;1 response variables Multivariate regression Multivariate multiple regression 3.3 Description versus prediction The primary purpose of regression analysis is the description (or explanation) of the data in terms of a general relationship pertaining to the population that these data are sampled from. Being a property of the population, this relationship should then also allow us to make predictions, but we need to be careful. Consider the relationship between year and world record time for the men’s mile depicted in Figure 3.2. When the predictor is time, as shown here, regression becomes a form of trend analysis, in this case of how the record time in the male competition decreased over the years. We will talk about the R code and output further below, here we’re just interested in the linear predictions. # load mile data from remote repository dat &lt;- read.csv(&quot;https://raw.githubusercontent.com/avehtari/ROS-Examples/master/Mile/data/mile.csv&quot;, header=TRUE) # fit linear model to data from 1st half of 20th century fit1 &lt;- lm(seconds ~ year, data = dat[dat$year&lt;1950,]) # extract information about parameter estimates coef(summary(fit1)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 912.2339944 67.90139506 13.434687 3.614623e-08 ## year -0.3438721 0.03509494 -9.798337 9.058700e-07 # fit linear model to complete dataset fit2 &lt;- lm(seconds ~ year, data = dat) coef(summary(fit2)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1006.8760057 21.5319332 46.76199 1.360809e-29 ## year -0.3930488 0.0109992 -35.73431 3.779773e-26 # plot fit for 1st half of 20th century plot(dat$year[dat$year&lt;1950], dat$seconds[dat$year&lt;1950], xlim = c(1900, 2000), ylim = c(200, 260), pch = 19, type = &#39;p&#39;, xlab = &quot;Year&quot;, ylab = &quot;World record, men&#39;s mile (seconds)&quot;) abline(coef(fit1), lwd = 3, col = &quot;red&quot;) # plot extrapolation to 2nd half of 20th century plot(dat$year, dat$seconds, xlim = c(1900, 2000), ylim = c(200, 260), pch = 19, type = &#39;p&#39;, xlab = &quot;Year&quot;, ylab = &quot;World record, men&#39;s mile (seconds)&quot;) abline(coef(fit1), lwd = 3, col = &quot;red&quot;) # plot all-data fit until 2050 plot(dat$year, dat$seconds, xlim = c(1900, 2050), ylim = c(200, 260), pch = 19, type = &#39;p&#39;, xlab = &quot;Year&quot;, ylab = &quot;World record, men&#39;s mile (seconds)&quot;) abline(coef(fit2), lwd = 3, col = &quot;red&quot;) Figure 3.2: Left: Trend of the world record for the men`s mile over the first half of the 20th century (description). Centre: Extrapolation of this trend over the 2nd half of the 20th century (prediction). Right: Extrapolation of the overall trend until the year 2050 (longer prediction). After: Wainer (2009) The world record for the men`s mile improved linearly over the first half of the 20th century (Figure 3.2, left). This trend provides a remarkably accurate fit for the second half of the century as well (Figure 3.2, centre). However, for how long can the world record continue to improve at the same rate (Figure 3.2, right)? This example clearly shows that the scope for prediction by regression lies within certain bounds, while highlighting the limits of these simple models for making distant predictions (e.g. in time and space). In the case of the world record we would expect the rate of improvement to decline with time, i.e. the world record to level off, which calls for a non-linear model. 3.4 Linear Regression Typically, regression problems are solved, i.e. the lines in Figures 3.1 and 3.2 are fitted to the data, by minimising the Sum of Squared Errors (SSE) between the regression line and the data points. This method has become known as Least Squares. Graphically, it means that in Figure 3.1 we try different lines with different intercepts \\(\\left(\\beta_0\\right)\\) and slopes \\(\\left(\\beta_1\\right)\\) and ultimately choose the one where the sum over all vertical distances \\(\\epsilon_i\\) squared is smallest. Mathematically, SSE is defined as: \\[\\begin{equation} SSE=\\sum_{i=1}^{n}\\left(\\epsilon_i\\right)^2=\\sum_{i=1}^{n}\\left(y_i-\\left(\\beta_0+\\beta_1 \\cdot x_i\\right)\\right)^2 \\tag{3.10} \\end{equation}\\] The terms \\(\\epsilon_i=y_i-\\left(\\beta_0+\\beta_1 \\cdot x_i\\right)\\) are called the residuals, i.e. that part of the variation in the data which the linear model cannot explain. In the case of linear regression, SSE can be minimised analytically, which is not the case for non-linear models, for example. Analytically, we find the minimum of SSE where its partial derivatives with respect to the two model parameters are both zero (compare Chapter 2): \\(\\frac{\\partial SSE}{\\partial \\beta_0}=0\\) and \\(\\frac{\\partial SSE}{\\partial \\beta_1}=0\\). Using the definition of SEE of Equation (3.10) we thus begin with a system of two differential equations: \\[\\begin{equation} \\frac{\\partial SSE}{\\partial \\beta_0}=-2 \\cdot \\sum_{i=1}^{n}\\left(y_i-\\beta_0-\\beta_1 \\cdot x_i\\right)=0 \\tag{3.11} \\end{equation}\\] \\[\\begin{equation} \\frac{\\partial SSE}{\\partial \\beta_1}=-2 \\cdot \\sum_{i=1}^{n}x_i \\cdot \\left(y_i-\\beta_0-\\beta_1 \\cdot x_i\\right)=0 \\tag{3.12} \\end{equation}\\] We have already calculated these derivatives in an exercise in Chapter 2 using the sum rule and the chain rule in particular. Since Equations (3.11) and (3.12) form a system of two differential equations with two unknowns (\\(\\beta_0\\) and \\(\\beta_1\\); the data points \\(x_i\\) and \\(y_i\\) are known) we can solve it exactly. First, we solve Equation (3.11) for \\(\\beta_0\\) (after dividing by -2): \\[\\begin{equation} \\sum_{i=1}^{n}y_i-n \\cdot \\beta_0-\\beta_1 \\cdot \\sum_{i=1}^{n}x_i=0 \\tag{3.13} \\end{equation}\\] \\[\\begin{equation} n \\cdot \\hat\\beta_0=\\sum_{i=1}^{n}y_i-\\hat\\beta_1 \\cdot \\sum_{i=1}^{n}x_i \\tag{3.14} \\end{equation}\\] \\[\\begin{equation} \\hat\\beta_0=\\bar{y}-\\hat\\beta_1 \\cdot \\bar{x} \\tag{3.15} \\end{equation}\\] Note, at some point we have renamed \\(\\beta_0\\) to \\(\\hat\\beta_0\\) and \\(\\beta_1\\) to \\(\\hat\\beta_1\\) to denote these as estimates. The parameter notation up to now has been general but as we approach actual numerical values for the data at hand we are using the “hat” symbol to signify that we are now calculating estimates of those general parameters for a given dataset. Second, we insert Equation (3.15) into Equation (3.12) (again after dividing by -2 and rearranging): \\[\\begin{equation} \\sum_{i=1}^{n}\\left(x_i \\cdot y_i-\\beta_0 \\cdot x_i-\\beta_1 \\cdot x_i^2\\right)=0 \\tag{3.16} \\end{equation}\\] \\[\\begin{equation} \\sum_{i=1}^{n}\\left(x_i \\cdot y_i-\\bar{y} \\cdot x_i+\\hat\\beta_1 \\cdot \\bar{x} \\cdot x_i-\\hat\\beta_1 \\cdot x_i^2\\right)=0 \\tag{3.17} \\end{equation}\\] Third, we solve Equation (3.17) for \\(\\beta_1\\): \\[\\begin{equation} \\sum_{i=1}^{n}\\left(x_i \\cdot y_i-\\bar{y} \\cdot x_i\\right)-\\hat\\beta_1 \\cdot \\sum_{i=1}^{n}\\left(x_i^2-\\bar{x} \\cdot x_i\\right)=0 \\tag{3.18} \\end{equation}\\] \\[\\begin{equation} \\hat\\beta_1=\\frac{\\sum_{i=1}^{n}\\left(x_i \\cdot y_i-\\bar{y} \\cdot x_i\\right)}{\\sum_{i=1}^{n}\\left(x_i^2-\\bar{x} \\cdot x_i\\right)} \\tag{3.19} \\end{equation}\\] Via a series of steps that I skip here, we arrive at: \\[\\begin{equation} \\hat\\beta_1=\\frac{SSXY}{SSX} \\tag{3.20} \\end{equation}\\] Where \\(SSX=\\sum_{i=1}^{n}\\left(x_i-\\bar{x}\\right)^2\\) and \\(SSXY=\\sum_{i=1}^{n}\\left(x_i-\\bar{x}\\right) \\cdot \\left(y_i-\\bar{y}\\right)\\). Note, analogously \\(SSY=\\sum_{i=1}^{n}\\left(y_i-\\bar{y}\\right)^2\\). Equation (3.20) is an exact solution for \\(\\hat\\beta_1\\). We then insert Equation (3.20) back into Equation (3.10) and have an exact solution for \\(\\hat\\beta_0\\). 3.5 Significance of regression Having estimates for the regression parameters we need to ask ourselves whether these estimates are statistically significant or could have arisen by chance from the (assumed) random process of sampling the data. We do this via Analysis of Variance (ANOVA), which begins by constructing the ANOVA table (Table 3.3). This is often done in the background in software like R and not actually looked at that much. Table 3.3: ANOVA table for linear regression. Source Sum ofsquares Degrees of freedom \\((df)\\) Mean squares F statistic \\(\\left(F_s\\right)\\) \\(\\Pr\\left(Z\\geq F_s\\right)\\) Regression \\(SSR=\\\\SSY-SSE\\) \\(1\\) \\(\\frac{SSR}{df_{SSR}}\\) \\(\\frac{\\frac{SSR}{df_{SSR}}}{s^2}\\) \\(1-F\\left(F_s,1,n-2\\right)\\) Error \\(SSE\\) \\(n-2\\) \\(\\frac{SSE}{df_{SSE}}=s^2\\) Total \\(SSY\\) \\(n-1\\) In the second column of Table 3.3, \\(SSY=\\sum_{i=1}^{n}\\left(y_i-\\bar{y}\\right)^2\\) is a measure of the total variance of the data, i.e. how much the data points are varying around the overall mean (Figure 3.3, left). \\(SSE=\\sum_{i=1}^{n}\\left(\\epsilon_i\\right)^2=\\sum_{i=1}^{n}\\left(y_i-\\left(\\beta_0+\\beta_1 \\cdot x_i\\right)\\right)^2\\) is a measure of the error variance, i.e. how much the data points are varying around the regression line (Figure 3.3, right). This is the variance not explained by the model. \\(SSR=SSY-SSE\\) then is a measure of the variance explained by the model. Figure 3.3: Variation of the data points around the mean, summarised by \\(SSY\\) (left), and around the regression line, summarised by \\(SSE\\) (right). The third column of Table 3.3 lists the so called degrees of freedom of the three variance terms, which can be understood as the number of free parameters for the respective term that is controlled by the (assumed) random process of sampling the data. It is the number of possibilities for the chance process to unfold. \\(SSY\\) requires one parameter \\(\\left(\\bar{y}\\right)\\) to be calculated from the data (see above). Hence the degrees of freedom are \\(n-1\\); if I know \\(\\bar{y}\\) then there are \\(n-1\\) data points left that can be generated by chance, the nth one I can calculate from all the others and \\(\\bar{y}\\). \\(SSE\\), in turn, requires two parameters (\\(\\beta_0\\) and \\(\\beta_1\\)) to be calculated from the data (Equations (3.15) and (3.20)). Hence the degrees of freedom are \\(n-2\\). The degrees of freedom of \\(SSR\\) then are just the difference between the former two; \\(df_{SSR}=df_{SSY}-df_{SSE}=1\\). The degrees of freedom are used to normalise the variance terms in the fourth column of Table 3.3, where \\(s^2\\) is called the error variance. In the fifth column of Table 3.3 we find the ratio of two variances; regression variance over error variance. Naturally, for a significant regression we want the regression variance (explained by the model) to be much larger than the error variance (unexplained by the model). This is an F-Test problem, testing whether the variance explained by the model is significantly different from the variance unexplained by the model. The ratio of the two variances serves as the F statistic \\(\\left(F_s\\right)\\). The sixth column of Table 3.3 then shows the p-value of the F-Test, i.e. the probability of getting \\(F_s\\) or a larger value (i.e. an even better model) by chance if the Null hypothesis \\(\\left(H_0\\right)\\)) is true. \\(H_0\\) here is that the two variances are equal. It can be shown mathematically that \\(F_s\\) follows an F-distribution with parameters \\(1\\) and \\(n-2\\) under the Null hypothesis (Figure 3.4). The red line in Figure 3.4 marks a particular value of \\(F_s\\) (between 10 and 11) and the corresponding value of the cumulative distribution function of the F-distribution \\(\\left(F\\left(F_s,1,n-2\\right)\\right)\\). The p-value is \\(\\Pr\\left(Z\\geq F_s\\right)=1-F\\left(F_s,1,n-2\\right)\\), i.e. the probability of getting this variance ratio or a greater one by chance (due to the random sampling process) even if the two variances are actually equal. Here this value is very small and hence we conclude that the regression is significant. Figure 3.4: Cumulative distribution function (CDF) of the F-distribution of the F statistic \\(\\left(F_s\\right)\\), with a particular value and corresponding value of the CDF marked in red. The correct interpretation of the p-value is a bit tricky. In the words of philosopher of science Ian Hacking (2001), if we have a p-value of say 0.01 this means “either the Null hypothesis is true, in which case something unusual happened by chance (probability 1%), or the Null hypothesis is false.” This means, strictly speaking, the p-value is not the probability of the Null hypothesis being true; it is the probability of the data to come about if the Null hypothesis were true. If this is a very low probability then we think this tells us something about the Null hypothesis (that perhaps we should reject it), but in a roundabout way. Note, in the case of linear regression, the Null model is \\(\\beta_1=0\\), i.e. \\(y=\\beta_0\\) with \\(\\hat\\beta_0=\\bar{y}\\), which means the overall mean is the best model summarising the data (Figure 3.3, left). 3.6 Confidence in parameter estimates Having established the statistical significance of the regression, we should look at the uncertainty around the parameter estimates. In classic linear regression this uncertainty is conceptualised as arising purely from the random sampling process; the data at hand are just one possibility of many, and in each alternative case the parameter estimates would have turned out slightly different. The linear model itself is assumed to be correct. The first step in establishing how confident we should be that the parameter estimates are correct is the calculation of standard errors. For \\(\\hat\\beta_0\\) this is (derivation not shown here): \\[\\begin{equation} s_{\\hat\\beta_0}=\\sqrt{\\frac{\\sum_{i=1}^{n}x_i^2}{n} \\cdot \\frac{s^2}{SSX}} \\tag{3.21} \\end{equation}\\] Breaking down this formula into its individual parts, we can see that the more data points \\(n\\) we have, the smaller the standard error, i.e. the more confidence we have in the estimate. Also, the larger the variation in \\(x\\) \\((SSX)\\) the smaller the standard error. Both effects make intuitive sense: the more data points we have and the more possibilities for \\(x\\) we have covered, the more we can be confident that we have not missed much in our random sample. Conversely, the larger the error variance \\(s^2\\), i.e. the smaller the explanatory power of our model, the larger the standard error. And, the more \\(x\\) data points we have away from zero, i.e. the greater \\(\\sum_{i=1}^{n}x_i^2\\), the smaller our confidence in the intercept (where \\(x=0\\)) and hence the standard error increases. The standard error for \\(\\hat\\beta_1\\) is: \\[\\begin{equation} s_{\\hat\\beta_1}=\\sqrt{\\frac{s^2}{SSX}} \\tag{3.22} \\end{equation}\\] The same interpretation applies, except there is no influence of the magnitude of the \\(x\\) data points. We can also establish a standard error for new predictions \\(\\hat y\\) for given new predictor values \\(\\hat x\\): \\[\\begin{equation} s_{\\hat y}=\\sqrt{s^2 \\cdot \\left(\\frac{1}{n}+\\frac{\\left(\\hat x-\\bar x\\right)^2}{SSX}\\right)} \\tag{3.23} \\end{equation}\\] The same interpretation applies again, except there now is an added term \\(\\left(\\hat x-\\bar x\\right)^2\\) which means the further the new \\(x\\) value is away from the centre of the original data (the training or calibration data) the greater the standard error of the new prediction, i.e. the lower the confidence in it being correct. Note, the formulae for the standard errors arise from the fundamental assumptions of linear regression, which will be covered below. This can be shown mathematically but is omitted here. From the standard errors we can calculate confidence intervals for the parameter estimates as follows: \\[\\begin{equation} \\Pr\\left(\\hat\\beta_0-t_{n-2;0.975} \\cdot s_{\\hat\\beta_0}\\leq \\beta_0\\leq \\hat\\beta_0+t_{n-2;0.975} \\cdot s_{\\hat\\beta_0}\\right)=0.95 \\tag{3.24} \\end{equation}\\] The symbol \\(\\Pr(\\cdot)\\) means probability. The symbol \\(t_{n-2;0.975}\\) stands for the 0.975-percentile of the t-distribution with \\(n-2\\) degrees of freedom. Equation (3.24) is the central 95% confidence interval, which is defined as the bounds in which the true parameter, here \\(\\beta_0\\), lies with a probability of 0.95. We can write the interval like this: \\[\\begin{equation} CI=\\left[\\hat\\beta_0-t_{n-2;0.975} \\cdot s_{\\hat\\beta_0};\\hat\\beta_0+t_{n-2;0.975} \\cdot s_{\\hat\\beta_0}\\right] \\tag{3.25} \\end{equation}\\] As can be seen, the confidence interval \\(CI\\) is symmetric around the parameter estimate \\(\\hat\\beta_0\\) and arises from a t-distribution with parameter \\(n-2\\) whose width is modulated by the standard error \\(s_{\\hat\\beta_0}\\). Note, the width of the t-distribution is also controlled by sample size, becoming narrower with increasing \\(n\\). The same formulae apply for \\(\\beta_1\\) and \\(y\\): \\[\\begin{equation} \\Pr\\left(\\hat\\beta_1-t_{n-2;0.975} \\cdot s_{\\hat\\beta_1}\\leq \\beta_1\\leq \\hat\\beta_1+t_{n-2;0.975} \\cdot s_{\\hat\\beta_1}\\right)=0.95 \\tag{3.26} \\end{equation}\\] \\[\\begin{equation} \\Pr\\left(\\hat y-t_{n-2;0.975} \\cdot s_{\\hat y}\\leq y\\leq \\hat y+t_{n-2;0.975} \\cdot s_{\\hat y}\\right)=0.95 \\tag{3.27} \\end{equation}\\] As with the p-values, we need to be clear about the meaning of probability here, which in classic statistics is predicated on the repeated sampling principle. The meaning of the 95% confidence interval then is that in an assumed infinite number of regression experiments the 95% confidence interval captures the true parameter value in 95% of the cases. Again, this is not a probability of the true parameter value lying within the confidence interval for any one experiment! The formulae for the confidence intervals (Equations (3.24), (3.26) and (3.27)) arise from the fundamental assumptions of linear regression; the residuals are independent identically distributed (iid) according to a normal distribution and the linear model is correct. Then it can be shown mathematically that \\(\\frac{\\hat\\beta_0-\\beta_0}{s_{\\hat\\beta_0}}\\), \\(\\frac{\\hat\\beta_1-\\beta_1}{s_{\\hat\\beta_1}}\\) and \\(\\frac{\\hat y-y}{s_{\\hat y}}\\) are \\(t_{n-2}\\)-distributed (t-distribution with \\(n-2\\) degrees of freedom). Since the central 95% confidence interval of an arbitrary \\(t_{n-2}\\)-distributed random variable \\(Z\\) is \\(\\Pr\\left(-t_{n-2;0.975}\\leq Z\\leq t_{n-2;0.975}\\right)=0.95\\) (Figure 3.5), we can substitute any of the aforementioned three terms for \\(Z\\) and rearrange to arrive at Equations (3.24), (3.26) and (3.27). Figure 3.5: Left: Probability density function (PDF) of a t-distributed random variable \\(Z\\), with central 95% confidence interval marked in red. 95% of the PDF lies between the two bounds, 2.5% lies left of the lower bound and 2.5% right of the upper bound. Right: Cumulative distribution function (CDF) of the same t-distributed random variable \\(Z\\). The upper bound of the 95% confidence interval is defined as \\(t_{n-2;0.975}\\), i.e. the 0.975-percentile of the distribution, while the lower bound is defined as \\(t_{n-2;0.025}\\), which is equivalent to \\(-t_{n-2;0.975}\\) due to the symmetry of the distribution. The t-distribution property of the parameter estimates can further be exploited to test each parameter estimate separately for its statistical significance. This becomes especially important for multiple regression problems where we have more than one possible predictor, not all of which will have a statistically significant effect. The significance of the parameter estimates is determined via a t-test. The Null hypothesis is that the true parameters are zero, i.e. the parameter estimates are not significant: \\[\\begin{equation} H_0:\\beta_0=0 \\tag{3.28} \\end{equation}\\] \\[\\begin{equation} H_0:\\beta_1=0 \\tag{3.29} \\end{equation}\\] This hypothesis is tested against the alternative hypothesis that the true parameters are different from zero, i.e. the parameter estimates are significant: \\[\\begin{equation} H_1:\\beta_0\\neq 0 \\tag{3.30} \\end{equation}\\] \\[\\begin{equation} H_1:\\beta_1\\neq 0 \\tag{3.31} \\end{equation}\\] The test statistics are: \\[\\begin{equation} t_s=\\frac{\\hat\\beta_0-0}{s_{\\hat\\beta_0}}\\sim t_{n-2} \\tag{3.32} \\end{equation}\\] \\[\\begin{equation} t_s=\\frac{\\hat\\beta_1-0}{s_{\\hat\\beta_1}}\\sim t_{n-2} \\tag{3.33} \\end{equation}\\] The “tilde” symbol \\((\\sim)\\) means the test statistics follow a certain distribution, here the t-distribution. This arises again from the regression assumptions noted above. The assumptions are the same as for the common t-test of means, except in the case of linear regression the residuals are assumed iid normal while in the case of means the actual data points \\(y\\) are assumed iid normal. Analogous to the common 2-sided t-test, the p-value is defined as: \\[\\begin{equation} 2 \\cdot \\Pr\\left(t&gt;|t_s|\\right)=2 \\cdot \\left(1-F_t\\left(|t_s|\\right)\\right) \\tag{3.34} \\end{equation}\\] The symbol \\(F_t\\left(|t_s|\\right)\\) signifies the value of the CDF of the t-distribution at the location of the absolute value of the test statistic (\\(|t_s|\\), Figure 3.6). With a significance level of say \\(\\alpha=0.05\\) we arrive at critical values of the test statistic \\(t_c=t_{n-2;0.975}\\) and \\(-t_c\\) beyond which we reject the Null hypothesis and call the parameter estimates significant (Figure 3.6). Figure 3.6: Schematic of the t-test of significance of parameter estimates. The test statistic follows a t-distribution under the Null hypothesis. The actual value of the test statistic \\(t_s\\) is marked in blue and mirrored at zero for the 2-sided test. The critical value of the test statistic \\(t_c\\), which we get from a significance level of \\(\\alpha=0.05\\), is marked in red; this too is mirrored for the 2-sided test. We reject the Null hypothesis if \\(|t_s|&gt;t_c\\), i.e. for values of \\(t_s\\) below \\(-t_c\\) and above \\(t_c\\), and then call this parameter estimate significant. We keep the Null hypothesis if \\(|t_s|\\leq t_c\\), i.e. for values of \\(t_s\\) between \\(-t_c\\) and \\(t_c\\), and then call this parameter estimate insignificant (for now). In the example shown the parameter estimate is insignificant. 3.7 Goodness of fit The final step in regression analysis is assessing the goodness of fit of the linear model. In the first instance this may be done through the coefficient of determination \\(\\left(r^2\\right)\\), which is defined as the proportion of variation (in y-direction) that is explained by the model: \\[\\begin{equation} r^2=\\frac{SSY-SSE}{SSY}=1-\\frac{SSE}{SSY} \\tag{3.35} \\end{equation}\\] As can be seen, when the model fails to explain more variation than the total variation around the mean, i.e. \\(SSE=SSY\\), then \\(r^2=0\\). Conversely, when the model fits the data perfectly, i.e. \\(SSE=0\\), then \\(r^2=1\\). Any value in between signifies varying levels of goodness of fit. This can be visualised again with Figure 3.3, with the left panel signifying \\(SSY\\) and the right panel \\(SSE\\). When it comes to comparing models of varying complexity (i.e. with more or less parameters) using \\(r^2\\), then penalising the metric by the number of model parameters makes sense since more complex models (more parameters) automatically lead to better fits, simply due to the greater degrees of freedom that more complex models have for fitting the data. This leads to the adjusted \\(r^2\\): \\[\\begin{equation} \\bar r^2=1-\\frac{\\frac{SSE}{df_{SSE}}}{\\frac{SSY}{df_{SSY}}}=1-\\frac{SSE}{SSY} \\cdot \\frac{df_{SSY}}{df_{SSE}} \\tag{3.36} \\end{equation}\\] The coefficient of determination alone, however, is insufficient for assessing goodness of fit. Consider the four datasets depicted in Figure 3.7, which together form the Anscombe (1973) dataset. # load Anscombe dataset dat &lt;- anscombe # plot 4 individual datasets plot(dat$x1, dat$y1, xlim = c(0, 20), ylim = c(0, 14), pch = 19, type = &#39;p&#39;) plot(dat$x2, dat$y2, xlim = c(0, 20), ylim = c(0, 14), pch = 19, type = &#39;p&#39;) plot(dat$x3, dat$y3, xlim = c(0, 20), ylim = c(0, 14), pch = 19, type = &#39;p&#39;) plot(dat$x4, dat$y4, xlim = c(0, 20), ylim = c(0, 14), pch = 19, type = &#39;p&#39;) Figure 3.7: The four Anscombe (1973) datasets. The individual datasets have purposely been constructed to yield virtually the same parameter estimates and coefficients of determination, despite wildly different relationships between \\(x\\) and \\(y\\) (Figure 3.8): # perform individual regressions fit1 &lt;- lm(y1 ~ x1, data = dat) fit2 &lt;- lm(y2 ~ x2, data = dat) fit3 &lt;- lm(y3 ~ x3, data = dat) fit4 &lt;- lm(y4 ~ x4, data = dat) # extract information about parameter estimates and R2 coef(summary(fit1)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0000909 1.1247468 2.667348 0.025734051 ## x1 0.5000909 0.1179055 4.241455 0.002169629 summary(fit1)$r.squared ## [1] 0.6665425 coef(summary(fit2)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.000909 1.1253024 2.666758 0.025758941 ## x2 0.500000 0.1179637 4.238590 0.002178816 summary(fit2)$r.squared ## [1] 0.666242 coef(summary(fit3)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0024545 1.1244812 2.670080 0.025619109 ## x3 0.4997273 0.1178777 4.239372 0.002176305 summary(fit3)$r.squared ## [1] 0.666324 coef(summary(fit4)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.0017273 1.1239211 2.670763 0.025590425 ## x4 0.4999091 0.1178189 4.243028 0.002164602 summary(fit4)$r.squared ## [1] 0.6667073 In these summary tables, “(Intercept)” stands for \\(\\beta_0\\), while “x1” to “x4” stand for \\(\\beta_1\\). The column “Estimate” gives \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\), the column “Std. Error” gives \\(s_{\\hat\\beta_0}\\) and \\(s_{\\hat\\beta_1}\\), the column “t value” gives the individual \\(t_s\\) and the column “Pr(&gt;|t|)” gives the respective p-value. # plot individual datasets with regression lines plot(dat$x1, dat$y1, xlim = c(0, 20), ylim = c(0, 14), pch = 19, type = &#39;p&#39;) abline(coef(fit1), lwd = 3, col = &quot;red&quot;) plot(dat$x2, dat$y2, xlim = c(0, 20), ylim = c(0, 14), pch = 19, type = &#39;p&#39;) abline(coef(fit2), lwd = 3, col = &quot;red&quot;) plot(dat$x3, dat$y3, xlim = c(0, 20), ylim = c(0, 14), pch = 19, type = &#39;p&#39;) abline(coef(fit3), lwd = 3, col = &quot;red&quot;) plot(dat$x4, dat$y4, xlim = c(0, 20), ylim = c(0, 14), pch = 19, type = &#39;p&#39;) abline(coef(fit4), lwd = 3, col = &quot;red&quot;) Figure 3.8: Regression analysis of the four Anscombe (1973) datasets, yielding virtually the same parameter estimates and coefficients of determination (see above), despite wildly different relationships between \\(x\\) and \\(y\\). The coefficient of determination is insensitive to these and similar systematic deviations from the regression line. But we can detect these deficiencies of the model by looking at plots like Figure 3.8, and more generally by performing residual diagnostics that check model assumptions. The fundamental assumptions of linear regression are: The residuals are independent, in which case there will be no serial correlation in the residual plot – this can be tested using the Durbin-Watson test The residuals are normally distributed – this can be visually assessed using the quantile-quantile plot (QQ plot) and the residual histogram, and can be tested using the Kolmogorov-Smirnov test and the Shapiro-Wilk test The variance is the same across residuals, i.e. residuals are homoscedastic, in which case there is no “fanning out” of the residuals If these assumptions are not met then we can resort to data transformation, weighted regression or Generalised Linear Models (this is the preferred option), which we will cover in chapter 7. A first useful diagnostic plot is of the residuals in series, i.e. by index \\(i\\), to see if there is a pattern due to the data collection process (Figure 3.9). For the Anscombe dataset, this detects the nonlinearity in dataset 2 (top-right) and the outlier in dataset 3 (bottom-left), compare Figure 3.8. # plot residuals against index plot(residuals(fit1), xlim = c(0, 12), ylim = c(-2, 2), pch = 19, type = &#39;p&#39;) abline(h = 0, lwd = 3, col = &quot;red&quot;) plot(residuals(fit2), xlim = c(0, 12), ylim = c(-2, 2), pch = 19, type = &#39;p&#39;) abline(h = 0, lwd = 3, col = &quot;red&quot;) plot(residuals(fit3), xlim = c(0, 12), ylim = c(-2, 4), pch = 19, type = &#39;p&#39;) abline(h = 0, lwd = 3, col = &quot;red&quot;) plot(residuals(fit4), xlim = c(0, 12), ylim = c(-2, 2), pch = 19, type = &#39;p&#39;) abline(h = 0, lwd = 3, col = &quot;red&quot;) Figure 3.9: Anscombe (1973) datasets. Plot of residuals in series, i.e. by index \\(i\\). We should also plot the residuals by predicted value of \\(y\\) to see if there is a pattern as a function of magnitude (Figure 3.10). For the Anscombe dataset, this emphasizes the non-linearity of dataset 2 (top-right) and the outlier in dataset 3 (bottom-left) and also detects the singular extreme point in dataset 4 (bottom-right). In sum, the independence and homoscedasticity assumptions seem to be violated in all datasets except dataset 1. This would have to be formally tested using the Durbin-Watson test, for example. # plot residuals against predicted value of y plot(fitted.values(fit1),residuals(fit1), xlim = c(0, 14), ylim = c(-2, 2), pch = 19, type = &#39;p&#39;) abline(h = 0, lwd = 3, col = &quot;red&quot;) plot(fitted.values(fit2),residuals(fit2), xlim = c(0, 14), ylim = c(-2, 2), pch = 19, type = &#39;p&#39;) abline(h = 0, lwd = 3, col = &quot;red&quot;) plot(fitted.values(fit3),residuals(fit3), xlim = c(0, 14), ylim = c(-2, 4), pch = 19, type = &#39;p&#39;) abline(h = 0, lwd = 3, col = &quot;red&quot;) plot(fitted.values(fit4),residuals(fit4), xlim = c(0, 14), ylim = c(-2, 2), pch = 19, type = &#39;p&#39;) abline(h = 0, lwd = 3, col = &quot;red&quot;) Figure 3.10: Anscombe (1973) datasets. Plot of residuals by predicted value of \\(y\\). The normality assumption can be assessed using the QQ plot (Figure 3.11). # QQ plots qqnorm(residuals(fit1), xlim = c(-4, 4), ylim = c(-4, 4)) qqline(residuals(fit1)) qqnorm(residuals(fit2), xlim = c(-4, 4), ylim = c(-4, 4)) qqline(residuals(fit2)) qqnorm(residuals(fit3), xlim = c(-4, 4), ylim = c(-4, 4)) qqline(residuals(fit3)) qqnorm(residuals(fit4), xlim = c(-4, 4), ylim = c(-4, 4)) qqline(residuals(fit4)) Figure 3.11: Anscombe (1973) datasets. Quantile-quantile plot (QQ plot) of residuals. In the QQ plot, every data point represents a certain quantile of the empirical distribution. This quantile (after standardisation) is plotted (vertical axis) against the value of that quantile expected under a standard normal distribution (horizontal axis). The resultant shapes say something about the distribution of the residuals (Figure 3.12), e.g. in case of a normal distribution they all fall on a straight line. In the Anscombe dataset, the only clearly non-normal dataset seems to be #3 (bottom-left). This would have to be formally tested using the Kolmogorov-Smirnov test or the Shapiro-Wilk test, for example. Figure 3.12: Characteristic shapes of the QQ plot and what they mean for the residuals in our case. Source: https://condor.depaul.edu/sjost/it223/documents/normal-plot.htm. The non-normality of dataset 3 becomes apparent also in the residual histograms (Figure 3.13). They also emphasize the outlier in dataset 3. Note, it is generally difficult to reject the hypothesis of normally distributed residuals with so few data points. # histograms of residuals hist(residuals(fit1), breaks = seq(-4,4,0.5)) hist(residuals(fit2), breaks = seq(-4,4,0.5)) hist(residuals(fit3), breaks = seq(-4,4,0.5)) hist(residuals(fit4), breaks = seq(-4,4,0.5)) Figure 3.13: Anscombe (1973) datasets. Quantile-quantile plot (QQ plot) of residuals. References "],
["categorical-vars.html", "Chapter 4 Categorical variables", " Chapter 4 Categorical variables Under construction. "],
["multiple-lin-reg.html", "Chapter 5 Multiple linear regression", " Chapter 5 Multiple linear regression Under construction. "],
["ml-bayes.html", "Chapter 6 Probabilistic underpinnings", " Chapter 6 Probabilistic underpinnings Under construction. "],
["glms.html", "Chapter 7 Generalised Linear Models (GLMs)", " Chapter 7 Generalised Linear Models (GLMs) Under construction. "],
["multivariate.html", "Chapter 8 Multivariate methods 8.1 Cluster analysis 8.2 Principal Component Analysis (PCA) 8.3 Multivariate ANOVA (MANOVA) 8.4 Discriminant Function Analysis (DFA)", " Chapter 8 Multivariate methods 8.1 Cluster analysis 8.2 Principal Component Analysis (PCA) 8.3 Multivariate ANOVA (MANOVA) 8.4 Discriminant Function Analysis (DFA) "],
["solutions-to-exercises.html", "Solutions to exercises Answer to Q1", " Solutions to exercises Answer to Q1 Equations (3.3), (3.5) and (3.7) are linear models; the others are non-linear in their parameters. Note, that the variables of linear models can take non-linear forms (\\(x_1^3, x_1 \\cdot x_2, \\log x_1\\)) as long as the parameters are not implicated in this non-linearity. "],
["references.html", "References", " References "]
]
