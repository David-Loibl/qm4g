[
["orga.html", "Quantitative Methods for Geographers Chapter 1 Organisational matters 1.1 Motivatiing example 1.2 Topics 1.3 Format", " Quantitative Methods for Geographers Tobias Krueger 2020-10-06 Chapter 1 Organisational matters Welcome to the course Quantitative Methods for Geographers (online edition), which consists of a seminar and a PC lab. In the seminar, theoretical input will be provided by myself up until January and by Tobia Lakes in the last four sessions in February. Part of this course is a PC lab run by Dirk Pflugmacher and David Loibl. 1.1 Motivatiing example Figure 1.1 shows the spatial variation of annual average precipitation and annual average temperature over Germany. Before reading on, reflect a minute on what might control this spatial variation. Figure 1.1: Maps of annual average precipitation (left) and annual average air temperature (right) over Germany from 1961 to 1990. Source: www.dwd.de/klimaatlas. The spatial variation of average precipitation and temperature in our climatic zone is controlled mainly by elevation and continentality: the higher up we go the more it rains over the year and the colder it gets; and the further east we go the drier it gets while a temperature effect is not visible. There are of course more nuanced effects such as aspect, but these are not so clearly visible in this graph. And this is exactly the goal of large parts of this course: To try and explain patterns we see in data (so called response variables) - in space (like here) and in time - with other data (so called predictor variables) using statistical inference methods. This includes separating dominant predictors from not so dominant predictors. It also includes making predictions with the relationships we find, e.g. for interpolating between data points to create maps like Figure 1.1. By the end of this course you will have learned the following: You have understood the concept of statistical inference using the linear model in depth. This includes linear regression with one or more predictors (multiple regression). It also includes mildly non-linear models and specific types of responses, dealt with by so called Generalised Linear Models. You have worked with metric as well as categorical predictors and mixtures of the two. You have learned metrics to compare and simplify models and evaluate their assumptions. You have understood the principles of extending these techniques to cases of more than one response variable (multivariate methods). You have learned specific techniques for working with spatial data. You can carry out the corresponding analyses in the software R. 1.2 Topics The timing of topics is shown in Table 1.1. Table 1.1: Schedule of Quantitative Methods for Geographers. Week Reading Monday seminar Wednesday PC lab 1 Organisational matters no class due to Dies Academicus Introduction to R 2 Mathematical preliminaries Introductions Data manipulation and import/export with R 3 The linear model Mathematical preliminaries Visualization and data manipulation with R 4 Categorical variables (ANOVA) and dummy coding The linear model Linear regression 5 Multiple linear regression Categorical variables (ANOVA) and dummy coding Hypothesis testing and ANOVA 6 Maximum Likelihood and outlook to Bayesian statistics Multiple linear regression Multiple linear regression 7 Generalized linear models I Maximum Likelihood and outlook to Bayesian statistics Outlook to machine learning 8 Generalized linear models II Generalized linear models I Generalized linear models I 9 Multivariate methods I Generalized linear models II Generalized linear models II 10 Multivariate methods II Multivariate methods I Principal Component Analysis 11 tbc Multivariate methods II Discriminant Function Analysis and model validation 12 tbc Understanding spatial data (Tobia) Spatial data and cluster analysis in R 13 tbc Point pattern analysis (Tobia) Point pattern analysis and spatial auto-correlation 14 tbc Spatial autocorrelation and interpolation (Tobia) Semivariogram analysis and kriging 15 tbc Spatial weights and linear modeling (Tobia) Spatial regression models 1.3 Format During this digital semester, the learning mode will be mainly reading. You are required to read a chapter of this script each week, which will then be discussed in a ZOOM session the following Monday 13:00-15:00 (see link on Moodle). I will give you guiding questions and little quizzes to guide your reading. And you are required to post questions on the topics or aspects that I should deepen in the Moodle Forum by each Friday. This way we know what to discuss each Monday and I will prepare some lecture-style input. We can also discuss questions that arise in the PC labs. Some questions might already be answered via the Forum. In the PC labs, Dirk and David will give you homework, which you need to submit via Moodle to pass the course. They will explain this in detail. The final exam is a project similar to an extended homework in which course topics are applied and extended on a dataset from the research context of the Geography Department. Each of will get their own topic and data. We will allocate topics towards the end of the semester via Moodle. The project has to be submitted as a HTML document created by R Markdown towards the end of the semester break (deadline tbc). "],
["math.html", "Chapter 2 Mathematical preliminaries 2.1 Logarithm and exponentiation 2.2 Centring and standardisation 2.3 Derivatives 2.4 Matrix algebra 2.5 Exercises", " Chapter 2 Mathematical preliminaries In this chapter we get a few mathematical preliminaries out of the way that are important for later chapters. If you feel rusty on any of these then please read up on them elsewhere. 2.1 Logarithm and exponentiation The following is inspired by Gelman and Nolan (2002). Suppose you have an amoeba that takes one hour to divide (Figure 2.1), and then the two amoebas each divide in one more hour, and so forth. What is the equation of the number of amoebas, \\(y\\), as a function of time, \\(t\\) (in hours)? Figure 2.1: Amoeba dividing. Source: http://www.gutenberg.org/files/18451/18451-h/images/illus002.jpg. The equation is: \\[\\begin{equation} y=2^t \\tag{2.1} \\end{equation}\\] This is an exponential function with base 2 and exponent \\(t\\). Figure 2.2 shows two plots of this function. t &lt;- seq(1, 6) y &lt;- 2^t plot(t, y, pch = 19, type = &#39;b&#39;) plot(t, log(y), pch = 19, type = &#39;b&#39;) Figure 2.2: Left: Plot of Equation 2.1. Right: Plot of Equation 2.1 on logarithmic scale. The inverse of the exponential function is the logarithmic function: \\[\\begin{equation} log(y)=log(2^t)=t \\cdot log(2) \\tag{2.2} \\end{equation}\\] Since the logarithm of \\(y\\) is a linear function of \\(t\\) (Equation (2.2)), the right-hand side of Figure 2.2 (\\(y\\) on logarithmic scale) displays a straight line. Common bases of the logarithmic function are: \\[\\begin{equation} log_2\\left(2^t\\right)=lb\\left(2^t\\right)=t \\tag{2.3} \\end{equation}\\] This is called the binary logarithm (lb). \\[\\begin{equation} log_{10}\\left(10^t\\right)=lg\\left(10^t\\right)=t \\tag{2.4} \\end{equation}\\] This is called the common logarithm (lg). \\[\\begin{equation} log_e\\left(e^t\\right)=ln\\left(e^t\\right)=t \\tag{2.5} \\end{equation}\\] This is called the natural logarithm (ln) with \\(e \\approx 2.7183\\) being Euler’s constant. Note, programming often uses a different notation, which is followed in this course: \\[\\begin{equation} ln()=log() \\tag{2.6} \\end{equation}\\] \\[\\begin{equation} e^t=\\exp(t) \\tag{2.7} \\end{equation}\\] Basic rules for exponentiation are: \\[\\begin{equation} a^m \\cdot a^n=a^{m+n} \\tag{2.8} \\end{equation}\\] \\[\\begin{equation} a^n \\cdot b^n=(a \\cdot b)^n \\tag{2.9} \\end{equation}\\] \\[\\begin{equation} \\frac{a^m}{a^n}=a^{m-n} \\tag{2.10} \\end{equation}\\] \\[\\begin{equation} \\frac{a^n}{b^n}=\\left(\\frac{a}{b}\\right)^n \\tag{2.11} \\end{equation}\\] \\[\\begin{equation} \\left(a^m\\right)^n=a^{m \\cdot n} \\tag{2.12} \\end{equation}\\] At this point it is also useful to remind ourselves of the meaning of the sum and product symbols: \\[\\begin{equation} \\sum_{i=1}^{n}x_i=x_1+x_2+\\ldots+x_n \\tag{2.13} \\end{equation}\\] This signifies the sum of all \\(x_i\\) for \\(i\\) taking integer values from 1 to \\(n\\). \\[\\begin{equation} \\prod_{i=1}^{n}x_i=x_1 \\cdot x_2 \\cdot \\ldots \\cdot x_n \\tag{2.14} \\end{equation}\\] This signifies the product of all \\(x_i\\) for \\(i\\) taking integer values from 1 to \\(n\\). The basic rules of logarithm are: \\[\\begin{equation} log(u \\cdot v)=log(u)+log(v) \\tag{2.15} \\end{equation}\\] \\[\\begin{equation} log\\left(\\frac{u}{v}\\right)=log(u)-log(v) \\tag{2.16} \\end{equation}\\] \\[\\begin{equation} log\\left(u^r\\right)=r \\cdot log(u) \\tag{2.17} \\end{equation}\\] 2.2 Centring and standardisation Centring and standardisation are used to transform different datasets onto the same scale. Note, for these transformations to be sensible the data \\(y\\) need to follow (approximately) a normal distribution. We will need this for the multivariate methods in sessions (Chapter 8). Centring means subtracting from every data point \\(y\\) the overall mean of the dataset \\(\\bar{y}\\): \\[\\begin{equation} y^*=y-\\bar{y} \\tag{2.18} \\end{equation}\\] This yields new data points \\(y^*\\) and new mean \\(\\bar{y^*}=0\\) while the standard deviation of the transformed data remains the same: \\(s_{y^*}=s_y\\). Centring thus shifts the data histogram to be centred on zero (Figure 2.3). # draw random sample of y~N(1,2) of size 1000 y &lt;- rnorm(1000, mean = 1, sd = 2) # mean ybar &lt;- mean(y) # standard deviation s_y &lt;- sd(y) # histogram, raw hist(y, freq = FALSE, xlim = c(-10,10), ylim = c(0, 0.4), main = &quot;&quot;, xlab = &quot;y&quot;, ylab = &quot;relative frequency&quot;) lines(c(ybar, ybar), c(0,0.4), lwd = 3) # histogram, centred hist(y-ybar, freq = FALSE, xlim = c(-10,10), ylim = c(0, 0.4), main = &quot;&quot;, xlab = &quot;y*&quot;, ylab = &quot;relative frequency&quot;) lines(c(0, 0), c(0,0.4), lwd = 3) Figure 2.3: Histogram of dataset \\(y\\) (left) and centred dataset \\(y^*\\) (right). The vertical line represents the mean. Standardisation means subtracting from every data point \\(y\\) the overall mean of the dataset \\(\\bar{y}\\) and dividing by the standard deviation \\(s_y\\): \\[\\begin{equation} y^*=\\frac{y-\\bar{y}}{s_y} \\tag{2.19} \\end{equation}\\] This yields new data points \\(y^*\\), new mean \\(\\bar{y^*}=0\\) and new standard deviation \\(s_{y^*}=1\\). Standardisation thus shifts the data histogram to be centred on zero and expands or contracts it to have unit standard deviation, i.e. transforming it to the scale of the standard normal distribution (Figure 2.4). # histogram, raw hist(y, freq = FALSE, xlim = c(-10,10), ylim = c(0, 0.4), main = &quot;&quot;, xlab = &quot;y&quot;, ylab = &quot;relative frequency&quot;) lines(c(ybar, ybar), c(0,0.4), lwd = 3) # histogram, standardised hist((y-ybar)/s_y, freq = FALSE, xlim = c(-10,10), ylim = c(0, 0.4), main = &quot;&quot;, xlab = &quot;y*&quot;, ylab = &quot;relative frequency&quot;) lines(c(0, 0), c(0,0.4), lwd = 3) Figure 2.4: Histogram of dataset \\(y\\) (left) and standardised dataset \\(y^*\\) (right). The vertical line represents the mean. 2.3 Derivatives The first derivative of a function \\(f(x)\\), \\(f&#39;(x)\\) or \\(\\frac{df(x)}{dx}\\), can be interpreted graphically as the slope of that function, i.e. the tangent line of a certain point of the function (Figure 2.5, left). Figure 2.5: Left: Tangent line of function \\(f(x)\\). Centre: Secant line of function \\(f(x)\\) between point \\(f\\left(x_0\\right)\\) and point \\(f\\left(x_0+h\\right)\\); the horizontal distance between these two points is \\(\\Delta x\\) and the vertical distance is \\(\\Delta f(x)\\). Right: Set of secant lines of function \\(f(x)\\) between point \\(f\\left(x_0\\right)\\) and point \\(f\\left(x_0+h\\right)\\) for progressively decreasing increments \\(h\\). Source: http://en.wikipedia.org/wiki/Derivative. Mathematically, the slope amounts to the limiting value of the ratio of the (vertical) increment of the function, \\(\\Delta f(x)\\), for an (horizontal) increment of \\(x\\), \\(\\Delta x\\), for \\(\\Delta x\\) approaching zero, \\(\\Delta x \\to 0\\): \\[\\begin{equation} f&#39;(x)=\\frac{df(x)}{dx}=\\lim_{\\Delta x \\to 0}\\frac{\\Delta f(x)}{\\Delta x} \\tag{2.20} \\end{equation}\\] This can be visualised as a secant line of the function between two points, \\(x_0\\) and \\(x_0+h\\) (Figure 2.5, centre), whose slope is: \\[\\begin{equation} \\frac{\\Delta f(x)}{\\Delta x}=\\frac{f\\left(x_0+h\\right)-f\\left(x_0\\right)}{\\left(x_0+h\\right)-\\left(x_0\\right)}=\\frac{f\\left(x_0+h\\right)-f\\left(x_0\\right)}{h} \\tag{2.21} \\end{equation}\\] As \\(h\\) approaches zero (Figure 2.5, right) we reach the limiting value of the slope at point \\(x_0\\), which is the first derivative: \\[\\begin{equation} \\frac{df(x)}{dx}=\\lim_{h \\to 0}\\frac{f\\left(x_0+h\\right)-f\\left(x_0\\right)}{h} \\tag{2.22} \\end{equation}\\] The first derivative is useful for finding minima, maxima and inflexion points of a function, because this is where the slope is zero, \\(\\frac{df(x)}{dx}=0\\) (Figure 2.6). The second derivative, measuring the curvature of the function, tells us whether these points are minima \\(\\left(\\frac{d^2f(x)}{dx^2}&gt;0\\right)\\), maxima \\(\\left(\\frac{d^2f(x)}{dx^2}&lt;0\\right)\\) or inflexion points \\(\\left(\\frac{d^2f(x)}{dx^2}=0\\right)\\), but often we already know that a function has only a single minimum or maximum and then we do not need the second derivative. Figure 2.6: Use of first and second derivative to determine minima, maxima and inflexion points of a function. Source: http://hyperphysics.phy-astr.gsu.edu/hbase/math/maxmin.html. The differentiation rules are: If \\(y=f(t)=t^a\\) then \\(\\frac{dy}{dt}=a \\cdot t^{a-1}\\), i.e. multiplying the function with the exponent and reducing the exponent by one. Constant factor rule: If \\(y=c \\cdot u(t)\\) then \\(\\frac{dy}{dt}=c \\cdot \\frac{du}{dt}\\). Sum rule: If \\(y=u(t) \\pm v(t)\\) then \\(\\frac{dy}{dt}=\\frac{du}{dt} \\pm \\frac{dv}{dt}\\). Product rule: If \\(y=u(t) \\cdot v(t)\\) then \\(\\frac{dy}{dt}=\\frac{du}{dt} \\cdot v+u \\cdot \\frac{dv}{dt}\\). Quotient rule: If \\(y=\\frac{u(t)}{v(t)}\\) then \\(\\frac{dy}{dt}=\\frac{\\left(\\frac{du}{dt} \\cdot v-u \\cdot \\frac{dv}{dt}\\right)}{v^2}\\). Chain rule: If \\(y=f[g(t)]\\) then \\(\\frac{dy}{dt}=\\frac{df[g]}{dg} \\cdot \\frac{dg}{dt}\\), i.e. “outer times inner derivative”. 2.4 Matrix algebra The following is based on Tabachnick and Fidell (2013). 2.4.1 Simple matrix operations Let \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) be matrices: \\[\\mathbf{A} = \\begin{pmatrix} a &amp; b &amp; c\\\\ d &amp; e &amp; f\\\\ g &amp; h &amp; i \\end{pmatrix} = \\begin{pmatrix} 3 &amp; 2 &amp; 4\\\\ 7 &amp; 5 &amp; 0\\\\ 1 &amp; 0 &amp; 8 \\end{pmatrix} \\] \\[\\mathbf{B} = \\begin{pmatrix} r &amp; s &amp; t\\\\ u &amp; v &amp; w\\\\ x &amp; y &amp; z \\end{pmatrix} = \\begin{pmatrix} 6 &amp; 1 &amp; 0\\\\ 2 &amp; 8 &amp; 7\\\\ 3 &amp; 4 &amp; 5 \\end{pmatrix} \\] Addition/subtraction of a constant to a matrix happens element-wise: \\[\\mathbf{A} + k = \\begin{pmatrix} a+k &amp; b+k &amp; c+k\\\\ d+k &amp; e+k &amp; f+k\\\\ g+k &amp; h+k &amp; i+k \\end{pmatrix} \\] \\[\\mathbf{A} - k = \\begin{pmatrix} a-k &amp; b-k &amp; c-k\\\\ d-k &amp; e-k &amp; f-k\\\\ g-k &amp; h-k &amp; i-k \\end{pmatrix} \\] Multiplication/division of a matrix by a constant also happens element-wise: \\[k \\cdot \\mathbf{A} = \\begin{pmatrix} k \\cdot a &amp; k \\cdot b &amp; k \\cdot c\\\\ k \\cdot d &amp; k \\cdot e &amp; k \\cdot f\\\\ k \\cdot g &amp; k \\cdot h &amp; k \\cdot i \\end{pmatrix} \\] \\[\\frac{1}{k} \\cdot \\mathbf{A} = \\begin{pmatrix} \\frac{1}{k} \\cdot a &amp; \\frac{1}{k} \\cdot b &amp; \\frac{1}{k} \\cdot c\\\\ \\frac{1}{k} \\cdot d &amp; \\frac{1}{k} \\cdot e &amp; \\frac{1}{k} \\cdot f\\\\ \\frac{1}{k} \\cdot g &amp; \\frac{1}{k} \\cdot h &amp; \\frac{1}{k} \\cdot i \\end{pmatrix} \\] Addition/subtraction of two matrices also happens element-wise: \\[\\mathbf{A} + \\mathbf{B} = \\begin{pmatrix} a+r &amp; b+s &amp; c+t\\\\ d+u &amp; e+v &amp; f+w\\\\ g+x &amp; h+y &amp; i+z \\end{pmatrix} \\] \\[\\mathbf{A} - \\mathbf{B} = \\begin{pmatrix} a-r &amp; b-s &amp; c-t\\\\ d-u &amp; e-v &amp; f-w\\\\ g-x &amp; h-y &amp; i-z \\end{pmatrix} \\] The transpose of a matrix is: \\[\\mathbf{A}&#39; = \\begin{pmatrix} a &amp; d &amp; g\\\\ b &amp; e &amp; h\\\\ c &amp; f &amp; i \\end{pmatrix} \\] 2.4.2 Matrix multiplication Now, the multiplication of two matrices is the only operation that is a bit complicated at first. It may be best to consider an example to work out the rules: \\[\\begin{eqnarray} \\mathbf{A} \\cdot \\mathbf{B}&amp;=&amp; \\begin{pmatrix} a &amp; b &amp; c\\\\ d &amp; e &amp; f\\\\ g &amp; h &amp; i \\end{pmatrix} \\cdot \\begin{pmatrix} r &amp; s &amp; t\\\\ u &amp; v &amp; w\\\\ x &amp; y &amp; z \\end{pmatrix}\\\\ &amp;=&amp;\\begin{pmatrix} a \\cdot r + b \\cdot u + c \\cdot x &amp; a \\cdot s + b \\cdot v + c \\cdot y &amp; a \\cdot t + b \\cdot w + c \\cdot z\\\\ d \\cdot r + e \\cdot u + f \\cdot x &amp; d \\cdot s + e \\cdot v + f \\cdot y &amp; d \\cdot t + e \\cdot w + f \\cdot z\\\\ g \\cdot r + h \\cdot u + i \\cdot x &amp; g \\cdot s + h \\cdot v + i \\cdot y &amp; g \\cdot t + h \\cdot w + i \\cdot z \\end{pmatrix}\\\\ &amp;=&amp;\\begin{pmatrix} 34 &amp; 35 &amp; 34\\\\ 52 &amp; 47 &amp; 35\\\\ 30 &amp; 33 &amp; 40 \\end{pmatrix} \\end{eqnarray}\\] The result of a matrix multiplication has as many rows as the 1st matrix and as many columns as the 2nd (in our example the matrices are square). For this to work, the number of columns of the 1st matrix must match the number of rows of the 2nd matrix. To construct each cell of the results matrix, one row of the 1st matrix is combined with one column of the 2nd matrix. For cell (1,1) (top-left), for example, we combine the 1st row of matrix 1 (here \\(\\mathbf{A}\\)) and the 1st column of matrix 2 (here \\(\\mathbf{B}\\)). Moving to the right, for cell (1,2) (top-middle), we combine the 1st row of matrix 1 and the 2nd column of matrix 2. For cell (2,1) (middle-left), we combine the 2nd row of matrix 1 and the 1st column of matrix 2. And so on and so forth. The combination of the two respective vectors is the sum of the products of the vector elements paired in order. So for cell (1,1) in our example this is \\(a \\cdot r + b \\cdot u + c \\cdot x\\). Try and recreate the following example to get a feeling for the matrix multiplication rules. \\[\\begin{eqnarray} \\mathbf{B} \\cdot \\mathbf{A}&amp;=&amp; \\begin{pmatrix} r &amp; s &amp; t\\\\ u &amp; v &amp; w\\\\ x &amp; y &amp; z \\end{pmatrix} \\cdot \\begin{pmatrix} a &amp; b &amp; c\\\\ d &amp; e &amp; f\\\\ g &amp; h &amp; i \\end{pmatrix}\\\\ &amp;=&amp;\\begin{pmatrix} r \\cdot a + s \\cdot d + t \\cdot g &amp; r \\cdot b + s \\cdot e + t \\cdot h &amp; r \\cdot c + s \\cdot f + t \\cdot i\\\\ u \\cdot a + v \\cdot d + w \\cdot g &amp; u \\cdot b + v \\cdot e + w \\cdot h &amp; u \\cdot c + v \\cdot f + w \\cdot i\\\\ x \\cdot a + y \\cdot d + z \\cdot g &amp; x \\cdot b + y \\cdot e + z \\cdot h &amp; x \\cdot c + y \\cdot f + z \\cdot i \\end{pmatrix}\\\\ &amp;=&amp;\\begin{pmatrix} 25 &amp; 17 &amp; 24\\\\ 69 &amp; 44 &amp; 64\\\\ 42 &amp; 26 &amp; 52 \\end{pmatrix} \\end{eqnarray}\\] The point with this example is that \\(\\mathbf{A} \\cdot \\mathbf{B}\\) is not the same as \\(\\mathbf{B} \\cdot \\mathbf{A}\\). The order matters when multiplying matrices! Let’s consider two more example: \\[\\begin{eqnarray} \\mathbf{A} \\cdot \\mathbf{A}&amp;=&amp; \\begin{pmatrix} a &amp; b &amp; c\\\\ d &amp; e &amp; f\\\\ g &amp; h &amp; i \\end{pmatrix} \\cdot \\begin{pmatrix} a &amp; b &amp; c\\\\ d &amp; e &amp; f\\\\ g &amp; h &amp; i \\end{pmatrix}\\\\ &amp;=&amp;\\begin{pmatrix} a^2 + b \\cdot d + c \\cdot g &amp; a \\cdot b + b \\cdot e + c \\cdot h &amp; a \\cdot c + b \\cdot f + c \\cdot i\\\\ d \\cdot a + e \\cdot d + f \\cdot g &amp; d \\cdot b + e^2 + f \\cdot h &amp; d \\cdot c + e \\cdot f + f \\cdot i\\\\ g \\cdot a + h \\cdot d + i \\cdot g &amp; g \\cdot b + h \\cdot e + i \\cdot h &amp; g \\cdot c + h \\cdot f + i^2 \\end{pmatrix}\\\\ &amp;=&amp;\\begin{pmatrix} 27 &amp; 16 &amp; 44\\\\ 56 &amp; 39 &amp; 28\\\\ 11 &amp; 2 &amp; 68 \\end{pmatrix} \\end{eqnarray}\\] This matrix multiplied with itself, \\(\\mathbf{A} \\cdot \\mathbf{A}\\), is different to the same matrix multiplied with its inverse \\(\\mathbf{A} \\cdot \\mathbf{A}&#39;\\): \\[\\begin{eqnarray} \\mathbf{A} \\cdot \\mathbf{A}&#39;&amp;=&amp; \\begin{pmatrix} a &amp; b &amp; c\\\\ d &amp; e &amp; f\\\\ g &amp; h &amp; i \\end{pmatrix} \\cdot \\begin{pmatrix} a &amp; d &amp; g\\\\ b &amp; e &amp; h\\\\ c &amp; f &amp; i \\end{pmatrix}\\\\ &amp;=&amp;\\begin{pmatrix} a^2 + b^2 + c^2 &amp; a \\cdot d + b \\cdot e + c \\cdot f &amp; a \\cdot g + b \\cdot h + c \\cdot i\\\\ d \\cdot a + e \\cdot b + f \\cdot c &amp; d^2 + e^2 + f^2 &amp; d \\cdot g + e \\cdot h + f \\cdot i\\\\ g \\cdot a + h \\cdot b + i \\cdot c &amp; g \\cdot d + h \\cdot e + i \\cdot f &amp; g^2 + h^2 + i^2 \\end{pmatrix}\\\\ &amp;=&amp;\\begin{pmatrix} 29 &amp; 31 &amp; 35\\\\ 31 &amp; 74 &amp; 7\\\\ 35 &amp; 7 &amp; 65 \\end{pmatrix} \\end{eqnarray}\\] This matrix is symmetrical, i.e. mirrored along its diagonal. Diagonal elements are so called sums of squares, off-diagonal elements are so called cross-products. This will be useful later on when working with variance-covariance matrices in Chapter 8. 2.4.3 Matrix division, inverse of a matrix, identity matrix Division of two matrices means multiplication of one matrix with the inverse of the other: \\[\\frac{\\mathbf{A}}{\\mathbf{B}} = \\mathbf{A} \\cdot \\mathbf{B}^{-1}\\] The inverse of matrix is found so that the following equation holds: \\[\\mathbf{A} \\cdot \\mathbf{A}^{-1} = \\mathbf{A}^{-1} \\cdot \\mathbf{A} = \\mathbf{I}\\] With \\(\\mathbf{I}\\) being the identity matrix, where diagonal elements are 1 and off-diagnal elements are 0: \\[\\mathbf{I} = \\begin{pmatrix} 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix} \\] Only matrices with determinants larger zero can be inverted! 2.5 Exercises Exercise 1 Apply the rules in the script to the following equation to get rid of the product operator, and then simplify the resultant equation as much as you can: \\[\\begin{equation} \\prod_{i=1}^{n}\\frac{1}{\\sigma \\cdot \\sqrt{2 \\cdot \\pi}} \\cdot \\exp\\left(\\frac{\\left(y_i - \\beta_0 - \\beta_1 \\cdot x_i\\right)^2}{-2 \\cdot \\sigma^2}\\right)= \\tag{2.23} \\end{equation}\\] Exercise 2 Apply the differentiation rules in the script to take the derivative of the following equation: \\[\\begin{equation} \\frac{d\\sum_{i=1}^{n}\\left(y_i - \\beta_0 - \\beta_1 \\cdot x_i\\right)^2}{d\\beta_0}= \\tag{2.24} \\end{equation}\\] Exercise 3 Consider the following vectors and matrix: \\[y = \\begin{pmatrix} y_1\\\\ y_2\\\\ y_3 \\end{pmatrix} \\] \\[\\beta = \\begin{pmatrix} \\beta_0\\\\ \\beta_1\\\\ \\beta_2\\\\ \\beta_3 \\end{pmatrix} \\] \\[\\epsilon = \\begin{pmatrix} \\epsilon_1\\\\ \\epsilon_2\\\\ \\epsilon_3 \\end{pmatrix} \\] \\[\\mathbf{X} = \\begin{pmatrix} 1 &amp; x_{11} &amp; x_{12} &amp; x_{13}\\\\ 1 &amp; x_{21} &amp; x_{22} &amp; x_{23}\\\\ 1 &amp; x_{31} &amp; x_{32} &amp; x_{33} \\end{pmatrix} \\] Now solve the following equation using matrix algebra: \\[y = \\mathbf{X} \\cdot \\beta + \\epsilon = \\] References "],
["lin-reg.html", "Chapter 3 Linear regression 3.1 Motivation 3.2 The linear model 3.3 Description versus prediction 3.4 Linear Regression 3.5 Significance of regression 3.6 Confidence in parameter estimates 3.7 Goodness of fit", " Chapter 3 Linear regression 3.1 Motivation The questions we wish to answer with linear regression are of the kind depicted in Figure 1.1: What drives spatial variation in annual average precipitation and annual average temperature? In the case of precipitation the drivers seem to be continentality and elevation, while temperature seems to be dominantly controlled by elevation only. Linear regression puts this question as a problem of modelling a response variable with one or more predictor variables, while the relationship between the two is linear in its parameters. 3.2 The linear model A linear model is generally of the form: \\[\\begin{equation} y = \\beta_0 + \\sum_{j+1}^{p}\\beta_j \\cdot x_j + \\epsilon \\tag{3.1} \\end{equation}\\] In this equation, \\(y\\) is the response variable (also called dependent or output variable), \\(x_j\\) are the predictor variables (also called independent, explanatory, input variables or covariates), \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) are the parameters and \\(\\epsilon\\) is the residual, i.e. that part of the response which remains unexplained by the predictors. In the case of one predictor, which has come to be known as linear regression, the linear model is: \\[\\begin{equation} y = \\beta_0 + \\beta_1 \\cdot x + \\epsilon \\tag{3.2} \\end{equation}\\] It can be visualised as a line, with \\(\\beta_0\\) being the intercept, where the line intersects the vertical axis (\\(x=0\\)), and \\(\\beta_1\\) being the slope of the line (Figure 3.1). Note, the point \\(\\left(\\bar{x},\\bar{y}\\right)\\), the centroid of the data, lies always on the line. Figure 3.1: Linear model with one predictor variable (linear regression) Linear means linear in the model parameters, not (necessarily) in the predictor variables. With this in mind, consider the following five models. Which are linear models, which are non-linear models? (Q1)1 \\[\\begin{equation} y = \\beta_0 + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 + \\epsilon \\tag{3.3} \\end{equation}\\] \\[\\begin{equation} y = \\beta_0 + \\beta_1 \\cdot x_1^{\\beta_2} + \\epsilon \\tag{3.4} \\end{equation}\\] \\[\\begin{equation} y = \\beta_0 + \\beta_1 \\cdot x_1^3 + \\beta_2 \\cdot x_1 \\cdot x_2 + \\epsilon \\tag{3.5} \\end{equation}\\] \\[\\begin{equation} y = \\beta_0 + \\exp(\\beta_1 \\cdot x_1) + \\beta_2 \\cdot x_2 + \\epsilon \\tag{3.6} \\end{equation}\\] \\[\\begin{equation} y = \\beta_0 + \\beta_1 \\cdot \\log x_1 + \\beta_2 \\cdot x_2 + \\epsilon \\tag{3.7} \\end{equation}\\] We can also write the linear model equation with the data points index by \\(i\\) for \\(i=1,\\ldots,n\\): \\[\\begin{equation} y_i = \\beta_0 + \\sum_{j=1}^{p}\\beta_j \\cdot x_{ij} + \\epsilon_i \\tag{3.8} \\end{equation}\\] These data points could be repeat measurements in time or in space. We can also write the model more compactly in matrix formulation: \\[\\begin{equation} y = \\mathbf{X} \\cdot \\beta + \\epsilon \\tag{3.9} \\end{equation}\\] With \\(y = \\begin{pmatrix} y_1\\\\ y_2\\\\ y_3 \\end{pmatrix}\\), \\(\\beta = \\begin{pmatrix} \\beta_0\\\\ \\beta_1\\\\ \\beta_2\\\\ \\beta_3 \\end{pmatrix}\\), \\(\\epsilon = \\begin{pmatrix} \\epsilon_1\\\\ \\epsilon_2\\\\ \\epsilon_3 \\end{pmatrix}\\) and \\(\\mathbf{X} = \\begin{pmatrix} 1 &amp; x_{11} &amp; x_{12} &amp; x_{13}\\\\ 1 &amp; x_{21} &amp; x_{22} &amp; x_{23}\\\\ 1 &amp; x_{31} &amp; x_{32} &amp; x_{33} \\end{pmatrix}\\), the latter being the design matrix which summarises the predictor data. When we talk about the linear model, the response variable is always continuous, while the predictor variables can be continuous, categorical or mixed. In principle, each of these variants can be treated mathematically in the same way, e.g. all can be analysed using the lm function in R. However, historically different names have been established for these variants, which are worth mentioning here to avoid confusion (Tables 3.1 and 3.2). Table 3.1: Historical names for the variants of the linear model, depending on whether the predictors are continuous, categorical or mixed. The response is always continuous. Continuous Categorical Mixed Regression Analysis of variance(ANOVA) Analysis of covariance(ANCOVA) Table 3.2: For the case of regression, different variants with different historical names can be distinguished again, depending on whether we have one or more predictors and one or more responses. 1 predictor variable &gt;1 predictor variables 1 response variable Regression Multiple regression &gt;1 response variables Multivariate regression Multivariate regression 3.3 Description versus prediction […] 3.4 Linear Regression 3.5 Significance of regression 3.6 Confidence in parameter estimates 3.7 Goodness of fit The answer can be found at the end of this script.↩︎ "],
["categorical-vars.html", "Chapter 4 Categorical variables 4.1 Example one 4.2 Example two", " Chapter 4 Categorical variables Some significant applications are demonstrated in this chapter. 4.1 Example one 4.2 Example two "],
["multiple-lin-reg.html", "Chapter 5 Multiple linear regression", " Chapter 5 Multiple linear regression We have finished a nice book. "],
["ml-bayes.html", "Chapter 6 Probabilistic underpinnings", " Chapter 6 Probabilistic underpinnings Text. "],
["glms.html", "Chapter 7 Generalised Linear Models (GLMs)", " Chapter 7 Generalised Linear Models (GLMs) Text. "],
["multivariate.html", "Chapter 8 Multivariate methods 8.1 Cluster analysis 8.2 Principal Component Analysis (PCA) 8.3 Multivariate ANOVA (MANOVA) 8.4 Discriminant Function Analysis (DFA)", " Chapter 8 Multivariate methods 8.1 Cluster analysis 8.2 Principal Component Analysis (PCA) 8.3 Multivariate ANOVA (MANOVA) 8.4 Discriminant Function Analysis (DFA) "],
["references.html", "References", " References "]
]
