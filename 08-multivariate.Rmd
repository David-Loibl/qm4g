# Multivariate methods {#multivariate}

**Multivariate regression** extends the case of multiple regression (one response variable/ multiple predictor variables) to the case of _multiple response variables_/ multiple predictor variables. _Predictors are continuous variables_, though there is the same correspondence as between linear regression and ANOVA.

**Multivariate analysis of variance (MANOVA)** (Chapter \@ref(manova)) extends the case of ANOVA (one response variable/ multiple predictor variables) to the case of _multiple response variables_/ multiple predictor variables. _Predictors are categorical variables_, though MANOVA relates to multivariate regression just like ANOVA does to linear regression.

**Discriminant Function Analysis (DFA)** (Chapter \@ref(dfa)) is a _classification_ method that tests how well multi-response observations discriminate between pre-determined groups, and can also be used to classify new observations into one of the groups.

**Principle Component Analysis (PCA)** (Chapter \@ref(pca)), **Factor Analysis (FA)** and related methods aim at finding _structure_ in a multivariate dataset, not deciding on response/predictor variables just yet. They extract a reduced set of components that explain much of the _variability_ or _correlation_ among the original variables. PCA and FA are typically employed to pre-structure and _simplify_ a problem by reducing its data dimensions, e.g. to reduce collinearity (compare Chapter \@ref(multiplelinreg)).

**Cluster Analysis** (Chapter \@ref(clusteranalysis)) looks for groups in a multivariate dataset. Data points belonging to the same group "resemble" each other - we will see what this means. Data points belonging to different groups are "dissimilar".

## Cluster analysis {#clusteranalysis}

This section is based on material by Cornelius Senf.^[https://corneliussenf.com/] Cluster analysis looks for groups (clusters) in a multivariate dataset. Objects (data points) belonging to the same group "resemble" each other. Objects belonging to different groups are "dissimilar". There are three groups of methods:

1. **Partitioning** the dataset into a number of clusters specified by the user, e.g. the _kmeans_ algorithm
2. **Hierarchical**, starting with each object (data point) as a separate cluster and then aggregating these step by step, ending up with a single cluster
3. **Divisive**, starting with a single cluster of all objects (data points) and then splitting this step by step until all objects are in different clusters

Let's illustrate the principles of these methods with the Iris dataset that is available from _R_, on which cluster analysis can be used to separate taxonomic groups. The dataset consists of a sample of Iris flowers for which the lengths and widths of their sepals and petals were measured. Sepals and petals are two different kinds of leaves in the flower. The question is: _Can we separate clusters of flowers that are sufficiently different with respect to these four features?_ This then could form the basis of deriving taxonomic groups; indeed this is a typical approach in botany. To get a sense of the dataset let's first plot a scatterplot matrix:

```{r echo=TRUE, out.width='80%'}
# load Iris dataset
data(iris)
# scatterplot matrix
plot(iris[,1:4])
```

We can already see at least two clusters. In some dimensions (petal length and width) they are more apart than in others (sepal length and width). Let's formalise this analysis using the _kmeans_ algorithm and afterwards look briefly at what hierarchical methods do.

### The _kmeans_ algorithm

The purpose of _kmeans_ is to build clusters such that the distance of cluster objects (data points) to cluster _centroids_ (vectors of means) is minimised. The algorithm proceeds though the following steps:

1. Choose $k$ random cluster centroids in the multivariate space
2. Allocate each object to a cluster so that _total intra-cluster sum of squares_ (Equation \@ref(eq:intraclusterss)) is minimised

$$\begin{equation}
\sum_{j=1}^{k}\sum_{i=1}^{n_j}\lVert \mathbf{y}_{ij}-\boldsymbol{\mu}_j\rVert^2
(\#eq:intraclusterss)
\end{equation}$$

$\boldsymbol{\mu}_j$ is the centroid of cluster $j=1,2,\ldots,k$, i.e. the vector of means across the data dimensions (here four). $\mathbf{y}_{ij}$ is data point $i=1,2,\ldots,n_j$ of cluster $j$, i.e. a multivariate vector too. $\lVert\cdot\rVert$ symbolises the _Euclidean distance_.

3. Re-calculate cluster centroids
4. Repeat steps 2-3 until cluster centroids are not changing much anymore (by some chosen criterion)

Often the Euclidean distance is used as a measure of (dis)similarity but others can be specified as well.

We have to tell the algorithm how many clusters we want. Let's use two to begin with (because that was our intuition earlier):

```{r echo=TRUE, out.width='80%'}
# run kmeans algorithm on Iris data asking for 2 clusters
iris_fit2 <- kmeans(iris[,1:4], centers=2)
# scatterplot matrix
plot(iris[,1:4], col=iris_fit2$cluster)
```

Two clusters didn't seem enough to reproduce the separation we see visually. Let's increase the number of clusters to three:

```{r echo=TRUE, out.width='80%'}
# run kmeans algorithm on Iris data asking for 3 clusters
iris_fit3 <- kmeans(iris[,1:4], centers=3)
# scatterplot matrix
plot(iris[,1:4], col=iris_fit3$cluster)
```

I think we would be happy with this visually. But are there perhaps even more clusters? When to stop? A useful stopping criterion is to look at the inflexion point where the total intra-cluster sum of squares (Equation \@ref(eq:intraclusterss)) does not change much anymore with increasing $k$ in a so called **screeplot**^[It's called screeplot because it looks like the scree at the foot of a mountain.]:

```{r echo=TRUE, out.width='80%'}
# specify vector of clusters
clusters <- 1:10
# initialise corresponding vector of total intra-cluster sum of squares
ticss <- rep(NA, 10)
# loop through vector of clusters
for(i in clusters){
  # run kmeans
  iris_fit <- kmeans(iris[,1:4], centers=i)
  # collect total intra-cluster sum of squares
  ticss[i] <- iris_fit$tot.withinss
}
# plot total intra-cluster sum of squares against clusters
plot(clusters, ticss, pch = 19, type = 'b', xlab = "Number of clusters",
     ylab = "Total intra-cluster sum of squares")
```

We can see that beyond three clusters the improvement in separation is minimal, so we would leave it at three. This, it turns out, matches almost perfectly the official taxonomic separation of the Iris dataset:

```{r echo=TRUE}
# contingency table of cluster:species matches
table(iris_fit3$cluster, iris$Species)
```

The separation of Iris versicolor and Iris virginica, however, is not perfect.

### Hierarchical methods

The principle of hierarchical methods is to start with each object as a separate cluster and then aggregate these step by step, ending up with a single cluster. Note, devisive methods are not covered here, but they work exactly the other way round. An example algorithm works as follows:

1. Build _dissimilarity matrix_, i.e. a matrix of the distances of every object to every other object in the multivariate space, e.g. by Euclidean distance $\lVert\mathbf{y}_i-\mathbf{y}_{i^*}\rVert$
2. Start with each object as a separate cluster
3. Join the two most similar clusters, e.g. those that lead to minimum increase in total intra-cluster sum of squares after merging (Equation \@ref(eq:intraclusterss)); this is called _Ward’s method_
4. Repeat step 3 until a single cluster is build

The result is a **dendrogram**, whose "height" is the distance between clusters, e.g. in Ward’s method the increase in the intra-cluster sum of squares of the clusters being merged:

```{r echo=TRUE, out.width='80%'}
# construct dissimilarity matrix for Iris data
iris_dist <- dist(iris[,1:4])
# run hierarchical clustering with Ward's method
iris_fith <- hclust(iris_dist, method="ward.D2")
# dendrogram
plot(iris_fith)
```

Again we can see the three clearly separate clusters, beyond which any further separation is ambiguous.

## Principal Component Analysis (PCA) {#pca}

To illustrate PCA I will use a dataset from @lovett2000, cited in @quinn2002, that consists of stream chemistry measurements from 38 forested catchments in the Catskill Mountains, New York:

```{r echo=TRUE}
# load stream chemistry data
streams <- read.table("data/streams.txt",header=T)
head(streams)
```

The dataset first lists catchment characteristic: catchment name, maximum elevation in catchment, elevation where stream water sample was taken, stream length in catchment, catchment area. The stream chemistry variables are concentrations of: nitrate, total organic nitrogen, total nitrogen, ammonium, dissolved organic carbon, sulfate, chloride, calcium, magnesium, hydrogen.

### From univariate normal to multivariate normal

Like univariate methods, multivariate methods, too, rely on the normality assumption; the univariate normal distribution (Figure \@ref(fig:camg), left) is generalised to the **multivariate normal distribution** (Figure \@ref(fig:camg), right). Taking calcium concentration in the stream chemistry dataset as the example, the univariate normal model would be $N(\mu,\sigma)$ with mean $\mu=65.13$ and variance $\sigma^2=194.74$ (Figure \@ref(fig:camg), left). Looking at calcium and magnesium concentration together, the multivariate normal model would be $MVN(\boldsymbol\mu,\boldsymbol\Sigma)$  with **centroid** (vector of means) $\boldsymbol\mu=\begin{pmatrix}65.13&22.86\end{pmatrix}^T$ and **variance-covariance matrix** $\boldsymbol\Sigma=\begin{pmatrix}194.74&27.03\\27.03&194.74\end{pmatrix}$ (Figure \@ref(fig:camg), right).

```{r camg, echo=FALSE, fig.align='center', fig.cap='Left: Histogram of calcium concentration measurements from stream chemistry dataset, with fitted normal distribution. The vertical line marks the mean. Right: Scatterplot of calcium against magnesium concentration measurements, with coloured countours of fitted multivariate normal distribution. The vertical and horizontal lines are the individual means, which intersect at the centroid. Data from: @lovett2000, cited in @quinn2002.', fig.show='hold', out.width='50%'}
knitr::include_graphics(c('figs/ca_pdf.jpg','figs/ca_mg_jointpdf.jpg'))
```

The **variance-covariance matrix** (or just covariance matrix) is a matrix of associations between variables. On its diagonal are the variances of the individual variables,^[The variance of a variable is of course the covariance between that variable and itself, e.g. $var\left(y_1\right)=cov\left(y_1,y_1\right)$.] on the off-diagonals are the covariances between two variables:

$$\begin{equation}
\mathbf{C}=\begin{pmatrix}
\frac{\sum_{i=1}^{n}\left(y_{i1}-\bar y_1\right)^2}{n-1} & \frac{\sum_{i=1}^{n}\left(y_{i2}-\bar y_2\right)\cdot\left(y_{i1}-\bar y_1\right)}{n-1} & \cdots & \frac{\sum_{i=1}^{n}\left(y_{ip}-\bar y_p\right)\cdot\left(y_{i1}-\bar y_1\right)}{n-1} \\
\frac{\sum_{i=1}^{n}\left(y_{i1}-\bar y_1\right)\cdot\left(y_{i2}-\bar y_2\right)}{n-1} & \frac{\sum_{i=1}^{n}\left(y_{i2}-\bar y_2\right)^2}{n-1} & \cdots & \frac{\sum_{i=1}^{n}\left(y_{ip}-\bar y_p\right)\cdot\left(y_{i2}-\bar y_2\right)}{n-1} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\sum_{i=1}^{n}\left(y_{i1}-\bar y_1\right)\cdot\left(y_{ip}-\bar y_p\right)}{n-1} & \frac{\sum_{i=1}^{n}\left(y_{i2}-\bar y_2\right)\cdot\left(y_{ip}-\bar y_p\right)}{n-1} & \cdots & \frac{\sum_{i=1}^{n}\left(y_{ip}-\bar y_p\right)^2}{n-1}
\end{pmatrix}
(\#eq:covmatrix)
\end{equation}$$

Normalising the variance-covariance terms in Equation \@ref(eq:covmatrix) by the variances of the respective variables, e.g. $corr\left(y_1,y_2\right)=\frac{cov\left(y_1,y_2\right)}{\sqrt{\sigma_{y_1}^2\cdot\sigma_{y_2}^2}}=\frac{cov\left(y_1,y_2\right)}{\sigma_{y_1}\cdot\sigma_{y_2}}$, yields the **correlation matrix**:

$$\begin{equation}
\mathbf{R}=\begin{pmatrix}
\frac{\sum_{i=1}^{n}\left(y_{i1}-\bar y_1\right)^2}{\sqrt{\sum_{i=1}^{n}\left(y_{i1}-\bar y_1\right)^2\cdot\sum_{i=1}^{n}\left(y_{i1}-\bar y_1\right)^2}} & \frac{\sum_{i=1}^{n}\left(y_{i2}-\bar y_2\right)\cdot\left(y_{i1}-\bar y_1\right)}{\sqrt{\sum_{i=1}^{n}\left(y_{i2}-\bar y_2\right)^2\cdot\sum_{i=1}^{n}\left(y_{i1}-\bar y_1\right)^2}} & \cdots & \frac{\sum_{i=1}^{n}\left(y_{ip}-\bar y_p\right)\cdot\left(y_{i1}-\bar y_1\right)}{\sqrt{\sum_{i=1}^{n}\left(y_{ip}-\bar y_p\right)^2\cdot\sum_{i=1}^{n}\left(y_{i1}-\bar y_1\right)^2}} \\
\frac{\sum_{i=1}^{n}\left(y_{i1}-\bar y_1\right)\cdot\left(y_{i2}-\bar y_2\right)}{\sqrt{\sum_{i=1}^{n}\left(y_{i1}-\bar y_1\right)^2\cdot\sum_{i=1}^{n}\left(y_{i2}-\bar y_2\right)^2}} & \frac{\sum_{i=1}^{n}\left(y_{i2}-\bar y_2\right)^2}{\sqrt{\sum_{i=1}^{n}\left(y_{i2}-\bar y_2\right)^2\cdot\sum_{i=1}^{n}\left(y_{i2}-\bar y_2\right)^2}} & \cdots & \frac{\sum_{i=1}^{n}\left(y_{ip}-\bar y_p\right)\cdot\left(y_{i2}-\bar y_2\right)}{\sqrt{\sum_{i=1}^{n}\left(y_{ip}-\bar y_p\right)^2\cdot\sum_{i=1}^{n}\left(y_{i2}-\bar y_2\right)^2}} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\sum_{i=1}^{n}\left(y_{i1}-\bar y_1\right)\cdot\left(y_{ip}-\bar y_p\right)}{\sqrt{\sum_{i=1}^{n}\left(y_{i1}-\bar y_1\right)^2\cdot\sum_{i=1}^{n}\left(y_{ip}-\bar y_p\right)^2}} & \frac{\sum_{i=1}^{n}\left(y_{i2}-\bar y_2\right)\cdot\left(y_{ip}-\bar y_p\right)}{\sqrt{\sum_{i=1}^{n}\left(y_{i2}-\bar y_2\right)^2\cdot\sum_{i=1}^{n}\left(y_{ip}-\bar y_p\right)^2}} & \cdots & \frac{\sum_{i=1}^{n}\left(y_{ip}-\bar y_p\right)^2}{\sqrt{\sum_{i=1}^{n}\left(y_{ip}-\bar y_p\right)^2\cdot\sum_{i=1}^{n}\left(y_{ip}-\bar y_p\right)^2}}
\end{pmatrix}
(\#eq:covmatrix)
\end{equation}$$

Let's look at the associations between the variables in the stream chemistry dataset:

```{r include=FALSE}
example(pairs)
```
```{r echo=TRUE}
pairs(streams[,6:15], diag.panel = panel.hist)
```

First of all we notice the non-normality of DOC, Cl and H (at least; in principle this diagnosis is ambiguous). Hence we log-transform these three variables to make them adhere better to normality:

```{r echo=TRUE}
streams_log <- streams
streams_log$DOC <- log(streams$DOC)
streams_log$CL <- log(streams$CL)
streams_log$H <- log(streams$H)
pairs(streams_log[,6:15], diag.panel = panel.hist)
```

We then notice a positive correlation between NO3 and TN; most of the total nitrogen in streams actually comes in the form of nitrate. We notice a positive correlation between Ca and Mg; both are characteristic of alkaline waters and often appear together. We notice a positive correlation between Mg (and Ca) and SO4; magnesium (and calcium) often appear as Mg(Ca)-sulphate. And we notice a negative correlation between Ca and H; calcium is characteristic of alkaline waters, i.e. waters that have a high pH, hence a low hydrogen concentration. These associations are reflected in the covariance matrix:

```{r echo=TRUE}
cov(streams_log[,6:15])
```

In particular, $cov(NO3,TN)=68.57$, $cov(SO4,MG)=19.83$ and $cov(CA,H)=-7.64$. However, at the scale of covariances we cannot judge the strengths of the associations, because the magnitude of the covariance is determined by the diagonal variance terms of the respective variables. We need to normalise by those variances, i.e. look at the correlation matrix:

```{r echo=TRUE}
cor(streams_log[,6:15])
```

Now the magnitude of the association is easier to see: $corr(NO3,TN)=0.98$, $corr(SO4,MG)=0.74$ and $corr(CA,H)=-0.81$.

### Linear combination of variables

A first step in multivariate analyses (PCA, DFA, FA etc.) is usually to centre the variables to zero mean: $y^*=y-\mu$ so that $\mu^*=0$ and $\sigma^*=\sigma$. Compare Chapter \@ref(math).

Then, the fundamental concept underlying multivariate analyses is to derive new linear combinations of the variables that summarise the variation in the original data set:

$$\begin{equation}
z_{ik}=u_{1k}\cdot y_{i1}+u_{2k}\cdot y_{i2}+\ldots+u_{pk}\cdot y_{ip}
(\#eq:lincomb)
\end{equation}$$

$z_{ik}$ is the value of the _new_ variable $k$ for object $i$. The object can be a timestep or a spatial location or else. $u_{1k},\ldots,u_{pk}$ are the _coefficients_ indicating how much each original variable contributes to the linear combination. This is the _Eigenvector_ of the covariance matrix (more on this later). $y_{i1},\ldots,y_{ip}$ are the values of _original_ variables $1,\ldots,p$ for object $i$.

The new variables are called (depending on the method) **principal components**, factors or discriminant functions. They are extracted so that the first explains most of the variance in the original variables, the second explains most of the remaining variance after the first has been extracted but is _independent_ of (uncorrelated with) the first ... and so on. The number of new variables $k$ is the same as the number of original variables $p$, although the variance is usually consolidated in the first few new variables. The unknown coefficients $u_{1k},\ldots,u_{pk}$ are determined via so called **Eigenanalysis** (see below).

A graphical explanation of what PCA does is **axis rotation**. We illustrate this with an example in 2D from @green1997, cited in @quinn2002; total biomass of red land crabs against number of burrows at ten forested sites on Christmas Island (Figure \@ref(fig:crabs1)). When the data are centred then the origin is where the individual means intersect (the centroid); the original axes are the black lines in the plots. Through PCA, the original axes are now rotated such that the 1st new one (PC1 in red) goes in the direction of the greatest spread of points and the 2nd new axis (PC2 in red), still perpendicular, covers the secondary variation. This is also the principle with more dimensions, though we cannot visualise it anymore.

```{r crabs1, echo=FALSE, fig.align='center', fig.cap='Total biomass of red land crabs against number of burrows at ten forested sites on Christmas Island. When the data are centred then the origin is where the individual means intersect (the centroid); the original axes are the black lines in the plots. Through PCA, the original axes are now rotated such that the 1st new one (PC1 in red) goes in the direction of the greatest spread of points and the 2nd new axis (PC2 in red), still perpendicular, covers the secondary variation. For one data point, marked by the cross, the new coordinates in PC1 and PC2 direction are marked in blue and green, respectively. After: @quinn2002, based on data by @green1997.', out.width='100%'}
# load crabs data
crabs <- read.table("data/crabs.txt",header=T)
# PCA
crabs_pca <- prcomp(crabs, center=TRUE)
# square
par(pty="s")
# data
plot(crabs$burrows, crabs$biomass, xlim=c(10,50), ylim=c(2,5), xlab="Number of burrows", ylab="Total biomass")
# original axes
lines(c(1,1)*mean(crabs$burrows), c(2,5))
lines(c(10,50), c(1,1)*mean(crabs$biomass))
# PCs
x <- c(10,50)
y1 <- mean(crabs$biomass)+(x-mean(crabs$burrows))*crabs_pca$rotation[1,1]/crabs_pca$rotation[2,1]
lines(x, y1, col="red")
y2 <- mean(crabs$biomass)+(x-mean(crabs$burrows))*crabs_pca$rotation[2,2]/crabs_pca$rotation[1,2]
lines(x, y2, col="red")
# example point
points(crabs$burrows[8], crabs$biomass[8], pch=13)
xnew <- as.numeric(crabs_pca$x[8,2])
xold <- sqrt(xnew^2/(1+crabs_pca$rotation[1,1]^2/crabs_pca$rotation[2,1]^2))
yold <- crabs_pca$rotation[1,1]/crabs_pca$rotation[2,1]*xold
lines(c(0,xold)+mean(crabs$burrows), c(0,yold)+mean(crabs$biomass), lwd=3, col="blue")
lines(c(xold+mean(crabs$burrows),crabs$burrows[8]), c(yold+mean(crabs$biomass),crabs$biomass[8]), lwd=3, col="green")
# annotation
text(45, 4.5, "PC1", col="red")
text(20, 4.5, "PC2", col="red")
```

Let's look at one data point, marked by the cross in Figure \@ref(fig:crabs1). The new coordinate in PC1 direction, marked in blue in the plot, is calculated after Equation \@ref(eq:lincomb) as:
$$\begin{equation}
\color{blue}{z_{i1}=u_{11}\cdot y_{i1}+u_{21}\cdot y_{i2}}
(\#eq:zi1)
\end{equation}$$

The new coordinate in PC2 direction, marked in green in the plot, is calculated after Equation \@ref(eq:lincomb) as:
$$\begin{equation}
\color{green}{z_{i2}=u_{12}\cdot y_{i1}+u_{22}\cdot y_{i2}}
(\#eq:zi2)
\end{equation}$$

To see how this is a rotation consider Figure \@ref(fig:rotation): After rotating the $x$ and $y$ axes by angle $\alpha$, the new coordinates $x'$ and $y'$ of point $P$ are:
$$\begin{equation}
x'=\cos\alpha\cdot x+\sin\alpha\cdot y
(\#eq:xtick)
\end{equation}$$
$$\begin{equation}
y'=-\sin\alpha\cdot x+\cos\alpha\cdot y
(\#eq:ytick)
\end{equation}$$

```{r rotation, echo=FALSE, fig.align='center', fig.cap='Geometric derivation of the new coordinates of a point $P$ after rotating the $x$ and $y$ axes by an angle $\\alpha$. Note that $\\sin\\alpha=\\frac{opposite}{hypotenuse}$ and $\\cos\\alpha=\\frac{adjacent}{hypotenuse}$. <br><small>By Jochen Burghardt - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=71157895.</small>', out.width='80%'}
knitr::include_graphics('figs/Coordinate_system_rotation_svg.svg')
```

### Eigenanalysis

The vector of coefficients $\begin{pmatrix}u_{1k}&\ldots&u_{pk}\end{pmatrix}^T$ in Equation \@ref(eq:lincomb) is called the $k$th **Eigenvector**. The Eigenvectors span the new coordinate system after rotation as illustrated in Figure \@ref(fig:crabs2): The first Eigenvector is $\color{blue}{\begin{pmatrix}u_{11}&u_{21}\end{pmatrix}^T}$ and the second Eigenvector is $\color{green}{\begin{pmatrix}u_{12}&u_{22}\end{pmatrix}^T}$.

```{r crabs2, echo=FALSE, fig.align='center', fig.cap='PCA of the crabs dataset as axis rotation. The two **Eigenvectors** spanning the new coordinate system are marked in blue and green. After: @quinn2002, based on data by @green1997.', out.width='100%'}
# square
par(pty="s")
# data
plot(crabs$burrows, crabs$biomass, xlim=c(10,50), ylim=c(2,5), xlab="Number of burrows", ylab="Total biomass")
# original axes
lines(c(1,1)*mean(crabs$burrows), c(2,5))
lines(c(10,50), c(1,1)*mean(crabs$biomass))
# PCs
x <- c(10,50)
y1 <- mean(crabs$biomass)+(x-mean(crabs$burrows))*crabs_pca$rotation[1,1]/crabs_pca$rotation[2,1]
lines(x, y1, col="red")
y2 <- mean(crabs$biomass)+(x-mean(crabs$burrows))*crabs_pca$rotation[2,2]/crabs_pca$rotation[1,2]
lines(x, y2, col="red")
# Eigenvectors
arrows(x0=mean(crabs$burrows), y0=mean(crabs$biomass), x1=mean(crabs$burrows)+5, y1=mean(crabs$biomass)+5*crabs_pca$rotation[1,1]/crabs_pca$rotation[2,1], lwd=3, length=0.1, angle=15, col="blue")
arrows(x0=mean(crabs$burrows), y0=mean(crabs$biomass), x1=mean(crabs$burrows)-5, y1=mean(crabs$biomass)-5*crabs_pca$rotation[2,2]/crabs_pca$rotation[1,2], lwd=3, length=0.1, angle=15, col="green")
# annotation
text(45, 4.5, "PC1", col="red")
text(20, 4.5, "PC2", col="red")
```

The Eigenvectors are found so that the following equation holds:^[An example - though one that doesn't involve a covariance matrix! - is: $\begin{pmatrix}2&3\\2&1\end{pmatrix}\cdot\begin{pmatrix}3\\2\end{pmatrix}=\begin{pmatrix}12\\8\end{pmatrix}=4\cdot\begin{pmatrix}3\\2\end{pmatrix}$.]

$$\begin{equation}
\mathbf{C}\cdot\begin{pmatrix}
u_{1k}\\u_{2k}\\\vdots\\u_{pk}
\end{pmatrix}=\lambda_k\cdot\begin{pmatrix}
u_{1k}\\u_{2k}\\\vdots\\u_{pk}
\end{pmatrix}
(\#eq:eigen)
\end{equation}$$

$\mathbf{C}$ is the covariance matrix and $\lambda_1,\ldots,\lambda_k$ are the so called **Eigenvalues**, which equal the amount of variance explained by each new variable. The sum of variances (Eigenvalues) of the new variables equals the sum of variances of the original variables.

The Eigenanalysis can be carried out, i.e. the Eigenvectors and Eigenvalues determined, on the covariance matrix $\mathbf{C}$ or the correlation matrix $\mathbf{R}$ using _Spectral Decomposition (Eigendecomposition)_ or on the data matrix (raw, centred or standardised) using _Singular Value Decomposition_, which is the more general method. We don't go into the details of these techniques here.

If the Eigenanalysis is carried out on the covariance matrix $\mathbf{C}$ then $\sum_{j=1}^{k}\lambda_j=Tr(\mathbf{C})$, i.e. the sum of the Eigenvalues is the trace of $\mathbf{C}$. The **trace** is defined as the sum of the diagonal elements of a matrix, i.e. here the sum of the variances of the original _centred_ variables. This isappropriate when the variables are measured in comparable units and differences in variance make an important contribution to interpretation.

If the Eigenanalysis is carried out on the correlation matrix $\mathbf{R}$ then $\sum_{j=1}^{k}\lambda_j=Tr(\mathbf{R})$, i.e. the sum of the variances of the original _standardised_ variables: $y^*=\frac{y-\mu}{\sigma}$ so that $\mu^*=0$ and $\sigma^*=1$. Compare Chapter \@ref(math). This is necessary when variables are measured in very different units or scales, otherwise variables with large values/variances may dominate the results.

In the stream chemistry example we best work with standardised data because variances are orders of magnitude different.^[You could try this yourself either way to see what difference it makes.] We use the `prcomp()` function on the data matrix, and tell it to standardise the variables by `scale=TRUE`. To only centre the variables we'd use `center=TRUE` and `scale=FALSE` (which is the default).

```{r echo=TRUE}
streams_pca <- prcomp(streams_log[,6:15], scale=TRUE)
streams_pca
```

In the output, what's called "standard deviations" are the square roots of the Eigenvalues of the principal components (PCs). So squaring them yields the Eigenvalues:
```{r echo=TRUE}
ev <- streams_pca$sdev^2
ev
```

We can verify that in this example the Eigenvalues sum to 10, which is the trace of the correlation matrix:
```{r echo=TRUE}
sum(ev)
```

By normalising the Eigenvalues by 10 we get the fraction of original variance explained by each PC:
```{r echo=TRUE}
ev/sum(ev)*100
```

Plotting the Eigenvalues in a **screeplot** shows that the bulk of the original variance is consolidated in the first three PCs; together they explain 71% of the original variance:
```{r echo=TRUE, out.width='80%'}
plot(seq(1,10,1), ev, pch = 19, type = 'b', ylim=c(0,4),
     xlab="Principal Component", ylab="Eigenvalue")
```

What's called "rotation" in the output are the Eigenvectors, measuring how much much each original variable contributes to the PCs. We look at the Eigenvectors of the first three PCs only here:
```{r echo=TRUE}
streams_pca$rotation[,1:3]
```

We see that SO4, log(Cl) and Mg load strongly positively on PC1 and log(H) loads strongly negatively. It seems PC1 distinguishes alkaline from acidic sites. NO3, total nitrogen and Ca load strongly positively on PC2. The correlation between total nitrogen and NO3 is again apparent. My interpretation is that PC2 distinguishes subsurface (high Ca, high nitrogen) from surface water sites. Total organic nitrogen and log(dissolved organic carbon) load strongly positively on PC3 and NH4 loads strongly negatively. PC3 might distinguish organic from mineralised (surface) water sites. We can also visualise this in so called **biplots**:
```{r echo=TRUE, fig.show='hold', out.width='33%'}
biplot(streams_pca, choices=c(1,2))
biplot(streams_pca, choices=c(1,3))
biplot(streams_pca, choices=c(2,3))
```

In these plots we see the original data points (black numbers) in PC space. The red vectors show the contributions of the original variables to the PCs. Vectors close together in all three dimensions indicate correlation; we can see this best here for NO3 and total nitrogen.

Lastly, in order to shows what we can do with the PCs, we extract the values of the first three PCs and plot them against the other catchment characteristics in the dataset:
```{r echo=TRUE, fig.show='hold', warning=FALSE, out.width='25%'}
library(latex2exp)
for(i in 1:3){
  plot(streams_pca$x[,i],streams_log$max_elev, xlab=paste("PC",i), ylab="Max. elevation",
       main=TeX(paste("$\\rho$=",round(cor(streams_pca$x[,i],streams_log$sample_elev),digits=3))))
  plot(streams_pca$x[,i],streams_log$sample_elev, xlab=paste("PC",i), ylab="Sample elevation",
       main=TeX(paste("$\\rho$=",round(cor(streams_pca$x[,i],streams_log$stream_length),digits=3))))
  plot(streams_pca$x[,i],streams_log$stream_length, xlab=paste("PC",i), ylab="Stream length",
       main=TeX(paste("$\\rho$=",round(cor(streams_pca$x[,i],streams_log$catchm_area),digits=3))))
  plot(streams_pca$x[,i],streams_log$catchm_area, xlab=paste("PC",i), ylab="Catchment area",
       main=TeX(paste("$\\rho$=",round(cor(streams_pca$x[,i],streams_log$max_elev),digits=3))))
}
```

On top of these plots are the linear correlation coefficients $\rho$ as a first indication of associations between PCs and other catchment characteristics. The strongest correlation is that between PC1 and max. elevation. Given our interpretation of PC1 from above we can conclude that waters tend to get more acidic (less alkaline) with elevation, perhaps because these sites are less influenced by human activity that tends to introduce alkalinity through agricultural or urban runoff and discharges.

We could also use the PCs as predictors in a regression model. This is especially useful for predictors that are highly correlated; these we can consolidate in a few PCs that then serve as independent predictors. Interpretation, however - as the stream chemistry example demonstrates - may be challenging.

## Multivariate ANOVA (MANOVA) {#manova}

## Discriminant Function Analysis (DFA) {#dfa}
